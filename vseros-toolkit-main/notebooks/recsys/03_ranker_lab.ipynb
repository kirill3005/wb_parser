{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "975af5b4",
   "metadata": {},
   "source": [
    "# 03 ¬∑ Ranker Lab ‚Äî —Ñ–∏—á–∏, —Ä–∞–Ω–∫–µ—Ä, OOF –∏ —Ç–µ—Å—Ç–æ–≤—ã–π —Å–∫–æ—Ä üîª\n",
    "\n",
    "–¶–µ–ª—å: –∏–∑ –ø—É–ª–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å–æ–±—Ä–∞—Ç—å —Ñ–∏—á–∏ **time-safe**, –æ–±—É—á–∏—Ç—å 1‚Äì2 —Ä–∞–Ω–∫–µ—Ä–∞ (LightGBM LambdaRank / CatBoost Ranking), –ø–æ–ª—É—á–∏—Ç—å **OOF** –∏ **test** –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ä–µ–∑—ã –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã.\n",
    "\n",
    "–í—ã—Ö–æ–¥—ã:\n",
    "`artifacts/recsys/ranker/<id>/<profile>/<run_id>/{X_rank_val.parquet, X_rank_test.parquet, oof.parquet, ranked.parquet, model.*, fi.csv, ranker_report.json}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076c29e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "/* ipywidgets v8 (JupyterLab 4) */\n",
    ".jp-OutputArea .widget-button .widget-label { \n",
    "  white-space: normal !important; \n",
    "  overflow: visible !important; \n",
    "  text-overflow: clip !important;\n",
    "  line-height: 1.2 !important;\n",
    "}\n",
    "/* fallback –¥–ª—è ipywidgets v7 */\n",
    ".jupyter-widgets.widget-button .widget-label {\n",
    "  white-space: normal !important; \n",
    "  overflow: visible !important; \n",
    "  text-overflow: clip !important;\n",
    "  line-height: 1.2 !important;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a971e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, time, math, warnings, shlex, subprocess\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import ipywidgets as W\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_colwidth\", 140)\n",
    "\n",
    "# –õ–µ–≥–∫–∏–π –∏–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ —Ä–∞–Ω–∫–∏–Ω–≥–∞\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception as e:\n",
    "    lgb = None\n",
    "try:\n",
    "    from catboost import CatBoostRanker, Pool\n",
    "except Exception:\n",
    "    CatBoostRanker = None\n",
    "    Pool = None\n",
    "\n",
    "REPO = Path.cwd()\n",
    "\n",
    "def mem_gb(obj=None):\n",
    "    try:\n",
    "        import psutil\n",
    "        if obj is None: return psutil.Process().memory_info().rss/(1024**3)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if hasattr(obj, \"memory_usage\"):\n",
    "            return float(obj.memory_usage(deep=True).sum())/(1024**3)\n",
    "        return float(np.array(obj).nbytes)/(1024**3)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def save_json(path: Path, obj: dict):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(obj, ensure_ascii=False, indent=2))\n",
    "\n",
    "def now_tag(): return datetime.now().strftime(\"%m%d_%H%M\")\n",
    "\n",
    "def art_paths(dataset_id, profile_path):\n",
    "    prof = Path(profile_path).stem\n",
    "    root = Path(\"artifacts\")\n",
    "    return dict(\n",
    "        dataio     = root/\"recsys\"/\"dataio\"/dataset_id,\n",
    "        profile    = root/\"recsys\"/\"dataio\"/dataset_id/prof,\n",
    "        candidates = root/\"recsys\"/\"candidates\"/dataset_id/prof,\n",
    "        ranker     = root/\"recsys\"/\"ranker\"/dataset_id/prof,\n",
    "    )\n",
    "\n",
    "def exists(p): \n",
    "    try: return Path(p).exists()\n",
    "    except: return False\n",
    "\n",
    "def floor_day(ts): \n",
    "    s = pd.to_datetime(ts, utc=True)\n",
    "    return s.dt.floor(\"D\")\n",
    "\n",
    "def merge_asof_by(df_left, df_right, on, by, direction=\"backward\"):\n",
    "    \"\"\"–£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è merge_asof —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π –ø–æ 'by'.\"\"\"\n",
    "    l = df_left.sort_values([by, on])\n",
    "    r = df_right.sort_values([by, on])\n",
    "    return pd.merge_asof(l, r, on=on, by=by, direction=direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd9c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "BTN_LAYOUT = W.Layout(min_width=\"220px\", width=\"auto\", height=\"36px\", flex=\"0 0 auto\")\n",
    "ROW_LAYOUT = W.Layout(flex_flow=\"row wrap\", grid_gap=\"8px\")\n",
    "GRID_LAYOUT = W.Layout(grid_template_columns=\"repeat(3, minmax(220px, 1fr))\", grid_gap=\"8px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6869423",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE = dict(\n",
    "    DATASET_ID   = \"s5e11\",\n",
    "    PROFILE_PATH = \"configs/recsys/profiles/gate.yaml\",\n",
    "    USE_CAND_LOCAL_QUOTA = False,\n",
    "    K_IN   = 300,   # —Å–∫–æ–ª—å–∫–æ –±–µ—Ä—ë–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ —Ä–∞–Ω–∫–∏–Ω–≥ –Ω–∞ query\n",
    "    K_OUT  = 100,   # —Å–∫–æ–ª—å–∫–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º ranked.parquet\n",
    "    RANDOM_SEED = 42,\n",
    "    JOBS   = -1,\n",
    ")\n",
    "\n",
    "CV = dict(\n",
    "    CV_KIND   = \"kfold_queries\",   # kfold_queries | group_user | time\n",
    "    N_SPLITS  = 5,\n",
    "    EMBARGO   = \"0D\"\n",
    ")\n",
    "\n",
    "MODEL = dict(\n",
    "    LIB           = \"lightgbm\",    # lightgbm | catboost\n",
    "    OBJECTIVE     = \"lambdarank\",  # –¥–ª—è LGBM: lambdarank; –¥–ª—è CatBoost: YetiRankPairwise/PairLogitPairwise\n",
    "    N_EST         = 400,\n",
    "    LR            = 0.05,\n",
    "    MAX_DEPTH     = 6,\n",
    "    NUM_LEAVES    = 63,\n",
    "    L2            = 1.0,\n",
    "    MIN_DATA_LEAF = 20,\n",
    "    FEATURE_FRACT = 0.9,\n",
    "    BAGGING_FRACT = 0.8,\n",
    "    EARLY_ROUNDS  = 50\n",
    ")\n",
    "\n",
    "FEATS = dict(\n",
    "    use_source_feats    = True,\n",
    "    use_pop_recency     = True,\n",
    "    use_user_hist       = True,\n",
    "    use_user_item_match = True,\n",
    "    use_item_meta       = True,\n",
    "    use_context         = True,\n",
    "    use_cold_flags      = True,\n",
    "\n",
    "    # –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ–∏—á\n",
    "    windows_days = [7, 30],\n",
    "    user_last_k  = 5,\n",
    ")\n",
    "\n",
    "# --- –í–∏–¥–∂–µ—Ç—ã\n",
    "w_ds    = W.Text(STATE[\"DATASET_ID\"], description=\"dataset_id:\", layout=W.Layout(width=\"240px\"))\n",
    "w_prof  = W.Text(STATE[\"PROFILE_PATH\"], description=\"profile:\", layout=W.Layout(width=\"520px\"))\n",
    "w_k_in  = W.IntText(STATE[\"K_IN\"], description=\"K_IN:\", layout=W.Layout(width=\"150px\"))\n",
    "w_k_out = W.IntText(STATE[\"K_OUT\"], description=\"K_OUT:\", layout=W.Layout(width=\"150px\"))\n",
    "w_seed  = W.IntText(STATE[\"RANDOM_SEED\"], description=\"seed:\", layout=W.Layout(width=\"150px\"))\n",
    "w_jobs  = W.IntText(STATE[\"JOBS\"], description=\"jobs:\", layout=W.Layout(width=\"150px\"))\n",
    "w_cand_local = W.Checkbox(STATE[\"USE_CAND_LOCAL_QUOTA\"], description=\"use candidates_local_quota\")\n",
    "\n",
    "w_cv_kind = W.Dropdown(options=[\"kfold_queries\",\"group_user\",\"time\"], value=CV[\"CV_KIND\"], description=\"CV_KIND:\")\n",
    "w_cv_ns   = W.IntText(CV[\"N_SPLITS\"], description=\"N_SPLITS:\")\n",
    "w_cv_emb  = W.Text(CV[\"EMBARGO\"], description=\"EMBARGO:\")\n",
    "\n",
    "w_lib   = W.Dropdown(options=[\"lightgbm\",\"catboost\"], value=MODEL[\"LIB\"], description=\"LIB:\")\n",
    "w_obj   = W.Dropdown(options=[\"lambdarank\",\"pairwise\",\"yeti\"], value=MODEL[\"OBJECTIVE\"], description=\"objective:\")\n",
    "w_nest  = W.IntText(MODEL[\"N_EST\"], description=\"n_estimators:\")\n",
    "w_lr    = W.FloatText(MODEL[\"LR\"], description=\"learning_rate:\")\n",
    "w_maxd  = W.IntText(MODEL[\"MAX_DEPTH\"], description=\"max_depth:\")\n",
    "w_leaves= W.IntText(MODEL[\"NUM_LEAVES\"], description=\"num_leaves:\")\n",
    "w_l2    = W.FloatText(MODEL[\"L2\"], description=\"l2:\")\n",
    "w_minlf = W.IntText(MODEL[\"MIN_DATA_LEAF\"], description=\"min_data_leaf:\")\n",
    "w_ff    = W.FloatText(MODEL[\"FEATURE_FRACT\"], description=\"feature_fraction:\")\n",
    "w_bf    = W.FloatText(MODEL[\"BAGGING_FRACT\"], description=\"bagging_fraction:\")\n",
    "w_es    = W.IntText(MODEL[\"EARLY_ROUNDS\"], description=\"early_rounds:\")\n",
    "\n",
    "tog = dict(\n",
    "    src   = W.Checkbox(FEATS[\"use_source_feats\"], description=\"source\"),\n",
    "    pop   = W.Checkbox(FEATS[\"use_pop_recency\"], description=\"pop/recency\"),\n",
    "    uhist = W.Checkbox(FEATS[\"use_user_hist\"], description=\"user_hist\"),\n",
    "    uim   = W.Checkbox(FEATS[\"use_user_item_match\"], description=\"user_item_match\"),\n",
    "    imeta = W.Checkbox(FEATS[\"use_item_meta\"], description=\"item_meta\"),\n",
    "    ctx   = W.Checkbox(FEATS[\"use_context\"], description=\"context\"),\n",
    "    cs    = W.Checkbox(FEATS[\"use_cold_flags\"], description=\"cold_flags\"),\n",
    ")\n",
    "\n",
    "w_windows = W.Text(\",\".join(map(str, FEATS[\"windows_days\"])), description=\"windows_days:\")\n",
    "w_lastk   = W.IntText(FEATS[\"user_last_k\"], description=\"user_last_k:\")\n",
    "\n",
    "btn_apply = W.Button(description=\"–ü—Ä–∏–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\", button_style=\"success\", layout=BTN_LAYOUT)\n",
    "out_apply = W.Output()\n",
    "\n",
    "def on_apply(_):\n",
    "    with out_apply:\n",
    "        clear_output()\n",
    "        STATE.update(\n",
    "            DATASET_ID=w_ds.value.strip(),\n",
    "            PROFILE_PATH=w_prof.value.strip(),\n",
    "            K_IN=int(w_k_in.value),\n",
    "            K_OUT=int(w_k_out.value),\n",
    "            RANDOM_SEED=int(w_seed.value),\n",
    "            JOBS=int(w_jobs.value),\n",
    "            USE_CAND_LOCAL_QUOTA=bool(w_cand_local.value)\n",
    "        )\n",
    "        CV.update(\n",
    "            CV_KIND=w_cv_kind.value,\n",
    "            N_SPLITS=int(w_cv_ns.value),\n",
    "            EMBARGO=w_cv_emb.value.strip()\n",
    "        )\n",
    "        MODEL.update(\n",
    "            LIB=w_lib.value,\n",
    "            OBJECTIVE=w_obj.value,\n",
    "            N_EST=int(w_nest.value),\n",
    "            LR=float(w_lr.value),\n",
    "            MAX_DEPTH=int(w_maxd.value),\n",
    "            NUM_LEAVES=int(w_leaves.value),\n",
    "            L2=float(w_l2.value),\n",
    "            MIN_DATA_LEAF=int(w_minlf.value),\n",
    "            FEATURE_FRACT=float(w_ff.value),\n",
    "            BAGGING_FRACT=float(w_bf.value),\n",
    "            EARLY_ROUNDS=int(w_es.value)\n",
    "        )\n",
    "        FEATS.update(\n",
    "            use_source_feats=bool(tog[\"src\"].value),\n",
    "            use_pop_recency=bool(tog[\"pop\"].value),\n",
    "            use_user_hist=bool(tog[\"uhist\"].value),\n",
    "            use_user_item_match=bool(tog[\"uim\"].value),\n",
    "            use_item_meta=bool(tog[\"imeta\"].value),\n",
    "            use_context=bool(tog[\"ctx\"].value),\n",
    "            use_cold_flags=bool(tog[\"cs\"].value),\n",
    "            windows_days=[int(x) for x in w_windows.value.split(\",\") if x.strip()],\n",
    "            user_last_k=int(w_lastk.value),\n",
    "        )\n",
    "        print(\"STATE:\", json.dumps(STATE, indent=2, ensure_ascii=False))\n",
    "        print(\"CV:\", json.dumps(CV, indent=2, ensure_ascii=False))\n",
    "        print(\"MODEL:\", json.dumps(MODEL, indent=2, ensure_ascii=False))\n",
    "        print(\"FEATS:\", json.dumps(FEATS, indent=2, ensure_ascii=False))\n",
    "\n",
    "W.VBox([\n",
    "    W.HTML(\"<h3>–ü–∞—Ä–∞–º–µ—Ç—Ä—ã</h3>\"),\n",
    "    W.HBox([w_ds, w_seed, w_jobs, w_k_in, w_k_out, w_cand_local], layout=ROW_LAYOUT),\n",
    "    w_prof,\n",
    "    W.HTML(\"<b>CV</b>\"),\n",
    "    W.HBox([w_cv_kind, w_cv_ns, w_cv_emb], layout=ROW_LAYOUT),\n",
    "    W.HTML(\"<b>–ú–æ–¥–µ–ª—å</b>\"),\n",
    "    W.HBox([w_lib, w_obj, w_nest, w_lr, w_maxd, w_leaves], layout=ROW_LAYOUT),\n",
    "    W.HBox([w_l2, w_minlf, w_ff, w_bf, w_es], layout=ROW_LAYOUT),\n",
    "    W.HTML(\"<b>–§–∏—á–∏</b>\"),\n",
    "    W.HBox([tog[\"src\"], tog[\"pop\"], tog[\"uhist\"], tog[\"uim\"], tog[\"imeta\"], tog[\"ctx\"], tog[\"cs\"]], layout=ROW_LAYOUT),\n",
    "    W.HBox([w_windows, w_lastk], layout=ROW_LAYOUT),\n",
    "    btn_apply, out_apply\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099fa488",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_check = W.Button(description=\"–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Ö–æ–¥—ã –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–∫—Ä—ã—Ç–∏–µ\", button_style=\"primary\", layout=BTN_LAYOUT)\n",
    "out_check = W.Output()\n",
    "\n",
    "CACHE = dict()  # inter/items/queries/pairs/cands\n",
    "\n",
    "def on_check(_):\n",
    "    with out_check:\n",
    "        clear_output()\n",
    "        A = art_paths(STATE[\"DATASET_ID\"], STATE[\"PROFILE_PATH\"])\n",
    "        p_inter = A[\"dataio\"]/ \"interactions_norm.parquet\"\n",
    "        p_items = A[\"dataio\"]/ \"items_norm.parquet\"\n",
    "        p_splits= A[\"dataio\"]/ \"splits.json\"\n",
    "        p_qv    = A[\"profile\"]/ \"queries_val.parquet\"\n",
    "        p_pv    = A[\"profile\"]/ \"pairs_val.parquet\"\n",
    "        p_qt    = A[\"profile\"]/ \"queries_test.parquet\"\n",
    "        cdir    = A[\"candidates\"]\n",
    "        p_cand  = cdir/\"candidates_local_quota.parquet\" if STATE[\"USE_CAND_LOCAL_QUOTA\"] and exists(cdir/\"candidates_local_quota.parquet\") else cdir/\"candidates.parquet\"\n",
    "        need = [p_inter,p_items,p_splits,p_qv,p_pv,p_qt,p_cand]\n",
    "        ok=True\n",
    "        for p in need:\n",
    "            tag = \"OK \" if exists(p) else \"MISS\"\n",
    "            print(tag, p)\n",
    "            ok &= exists(p)\n",
    "        if not ok:\n",
    "            print(\"\\n‚ö†Ô∏è –ù–µ—Ç –Ω—É–∂–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ ‚Äî –≤–µ—Ä–Ω–∏—Å—å –≤ 01/02.\")\n",
    "            return\n",
    "        inter = pd.read_parquet(p_inter)\n",
    "        items = pd.read_parquet(p_items)\n",
    "        splits = json.loads(Path(p_splits).read_text())\n",
    "        qv = pd.read_parquet(p_qv)\n",
    "        pv = pd.read_parquet(p_pv)\n",
    "        qt = pd.read_parquet(p_qt)\n",
    "        cands = pd.read_parquet(p_cand)\n",
    "\n",
    "        # –æ–≥—Ä–∞–Ω–∏—á–∏–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–æ K_IN\n",
    "        if \"score\" in cands.columns:\n",
    "            cands = cands.sort_values([\"query_id\",\"score\"], ascending=[True,False])\n",
    "        cands = cands.groupby(\"query_id\").head(STATE[\"K_IN\"]).reset_index(drop=True)\n",
    "\n",
    "        print(\"\\ninter:\", inter.shape, \"| items:\", items.shape, \"| cands:\", cands.shape)\n",
    "        print(\"queries_val:\", qv.shape, \"pairs_val:\", pv.shape, \"queries_test:\", qt.shape)\n",
    "\n",
    "        # –ø–æ–∫—Ä—ã—Ç–∏–µ\n",
    "        cov = cands[\"query_id\"].nunique()/ max(1, qv[\"query_id\"].nunique())\n",
    "        print(f\"coverage (queries_val with >=1 cand): {cov:.2%}\")\n",
    "        if cov < 0.97:\n",
    "            print(\"‚ö†Ô∏è –Ω–∏–∑–∫–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ ‚Äî –≤–µ—Ä–Ω–∏—Å—å –≤ 02, –≤–∫–ª—é—á–∏ fallbacks.\")\n",
    "\n",
    "        # –ö—ç—à\n",
    "        CACHE.update(inter=inter, items=items, splits=splits, qv=qv, pv=pv, qt=qt, cands=cands)\n",
    "\n",
    "btn_check.on_click(on_check)\n",
    "W.VBox([btn_check, out_check])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fa204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_preagg = W.Button(description=\"–ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Ç–∞–π–º-safe –∞–≥—Ä–µ–≥–∞—Ç—ã (item/user –ø–æ –¥–Ω—è–º)\", button_style=\"primary\", layout=BTN_LAYOUT)\n",
    "out_preagg = W.Output()\n",
    "\n",
    "AGG = dict(item_day=None, item_day_cum=None, user_day=None, user_day_cum=None, user_events=None, ui_timeline=None)\n",
    "\n",
    "def build_daily_cum(df, key_col, ts_col=\"ts\"):\n",
    "    tmp = df[[key_col, ts_col]].copy()\n",
    "    tmp[\"day\"] = floor_day(tmp[ts_col])\n",
    "    g = tmp.groupby([key_col, \"day\"]).size().rename(\"cnt\").reset_index()\n",
    "    g = g.sort_values([key_col, \"day\"])\n",
    "    g[\"cum_cnt\"] = g.groupby(key_col)[\"cnt\"].cumsum()\n",
    "    return g  # –∫–æ–ª–æ–Ω–∫–∏: key_col, day, cnt, cum_cnt\n",
    "\n",
    "def build_user_events(df):\n",
    "    # –¥–ª—è merge_asof –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É —Å–æ–±—ã—Ç–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "    ev = df[[\"user_id\",\"ts\"]].copy().sort_values([\"user_id\",\"ts\"]).reset_index(drop=True)\n",
    "    ev[\"u_cum_cnt\"] = ev.groupby(\"user_id\").cumcount()+1\n",
    "    return ev\n",
    "\n",
    "def build_user_item_timeline(df):\n",
    "    # –¥–ª—è seen_before –∏ times_seen_before –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É (user,item)\n",
    "    tmp = df[[\"user_id\",\"item_id\",\"ts\"]].copy().sort_values([\"user_id\",\"item_id\",\"ts\"])\n",
    "    tmp[\"uid_item\"] = (tmp[\"user_id\"].astype(\"int64\")<<32) + tmp[\"item_id\"].astype(\"int64\")\n",
    "    tmp[\"ui_cum_cnt\"] = tmp.groupby([\"user_id\",\"item_id\"]).cumcount()+1\n",
    "    return tmp[[\"uid_item\",\"ts\",\"ui_cum_cnt\"]]\n",
    "\n",
    "def on_preagg(_):\n",
    "    with out_preagg:\n",
    "        clear_output()\n",
    "        inter = CACHE.get(\"inter\")\n",
    "        if inter is None:\n",
    "            print(\"–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –≤—Ö–æ–¥—ã.\")\n",
    "            return\n",
    "        t0=time.time()\n",
    "        AGG[\"item_day\"]      = build_daily_cum(inter.rename(columns={\"item_id\":\"KEY\"}), \"KEY\").rename(columns={\"KEY\":\"item_id\"})\n",
    "        AGG[\"item_day_cum\"]  = AGG[\"item_day\"][ [\"item_id\",\"day\",\"cum_cnt\"] ].copy()\n",
    "        AGG[\"user_day\"]      = build_daily_cum(inter.rename(columns={\"user_id\":\"KEY\"}), \"KEY\").rename(columns={\"KEY\":\"user_id\"})\n",
    "        AGG[\"user_day_cum\"]  = AGG[\"user_day\"][ [\"user_id\",\"day\",\"cum_cnt\"] ].copy()\n",
    "        AGG[\"user_events\"]   = build_user_events(inter)\n",
    "        AGG[\"ui_timeline\"]   = build_user_item_timeline(inter)\n",
    "        dt=time.time()-t0\n",
    "        print(\"‚úì –≥–æ—Ç–æ–≤–æ –∑–∞\", f\"{dt:.1f}s\")\n",
    "        for k,v in AGG.items():\n",
    "            if isinstance(v, pd.DataFrame):\n",
    "                print(k, v.shape, \"| mem:\", f\"{mem_gb(v):.3f} GB\")\n",
    "btn_preagg.on_click(on_preagg)\n",
    "W.VBox([btn_preagg, out_preagg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ea1a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows_to_list(ws): \n",
    "    return [int(x) for x in ws] if isinstance(ws, (list,tuple)) else [int(y) for y in str(ws).split(\",\")]\n",
    "\n",
    "def item_pop_at_queries(cum_df, df_q, windows=(7,30), item_col=\"item_id\", q_ts_col=\"query_ts\"):\n",
    "    \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ pop@W –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–∫–Ω–∞ W. time-safe: cum_cnt(item, day-1) - cum_cnt(item, day-W-1).\"\"\"\n",
    "    windows = windows_to_list(windows)\n",
    "    q = df_q[[item_col, q_ts_col]].copy()\n",
    "    q[\"day\"] = floor_day(q[q_ts_col])\n",
    "    out = q[[item_col, \"day\"]].copy()\n",
    "    base = cum_df.copy()\n",
    "    base = base.sort_values([item_col, \"day\"])\n",
    "    for W in windows:\n",
    "        right = out.rename(columns={\"day\":\"day_right\"})\n",
    "        right[\"day_right\"] = right[\"day_right\"] - pd.Timedelta(days=1)\n",
    "        right = pd.merge_asof(right.sort_values(\"day_right\"), base, left_on=\"day_right\", right_on=\"day\", by=item_col, direction=\"backward\")\n",
    "        right = right.rename(columns={\"cum_cnt\":f\"cum_r_{W}\"}).drop(columns=[\"day\"])\n",
    "        left = out.rename(columns={\"day\":\"day_left\"})\n",
    "        left[\"day_left\"] = left[\"day_left\"] - pd.Timedelta(days=W+1)\n",
    "        left = pd.merge_asof(left.sort_values(\"day_left\"), base, left_on=\"day_left\", right_on=\"day\", by=item_col, direction=\"backward\")\n",
    "        left = left.rename(columns={\"cum_cnt\":f\"cum_l_{W}\"}).drop(columns=[\"day\"])\n",
    "        m = pd.concat([out, right[[f\"cum_r_{W}\"]], left[[f\"cum_l_{W}\"]]], axis=1)\n",
    "        m[f\"item_pop@{W}\"] = (m[f\"cum_r_{W}\"].fillna(0) - m[f\"cum_l_{W}\"].fillna(0)).astype(\"float32\")\n",
    "        out = pd.concat([out, m[[f\"item_pop@{W}\"]]], axis=1)\n",
    "    return out.drop(columns=[\"day\"])\n",
    "\n",
    "def user_activity_at_queries(cum_df, df_q, windows=(7,30), user_col=\"user_id\", q_ts_col=\"query_ts\"):\n",
    "    \"\"\"User events count –∑–∞ –æ–∫–Ω–∞ W –¥–æ query_ts.\"\"\"\n",
    "    windows = windows_to_list(windows)\n",
    "    q = df_q[[user_col, q_ts_col]].copy()\n",
    "    q[\"day\"] = floor_day(q[q_ts_col])\n",
    "    out = q[[user_col, \"day\"]].copy()\n",
    "    base = cum_df.copy().sort_values([user_col,\"day\"])\n",
    "    for W in windows:\n",
    "        right = out.rename(columns={\"day\":\"day_right\"})\n",
    "        right[\"day_right\"] = right[\"day_right\"] - pd.Timedelta(days=1)\n",
    "        right = pd.merge_asof(right.sort_values(\"day_right\"), base, left_on=\"day_right\", right_on=\"day\", by=user_col, direction=\"backward\")\n",
    "        right = right.rename(columns={\"cum_cnt\":f\"u_cum_r_{W}\"}).drop(columns=[\"day\"])\n",
    "        left = out.rename(columns={\"day\":\"day_left\"})\n",
    "        left[\"day_left\"] = left[\"day_left\"] - pd.Timedelta(days=W+1)\n",
    "        left = pd.merge_asof(left.sort_values(\"day_left\"), base, left_on=\"day_left\", right_on=\"day\", by=user_col, direction=\"backward\")\n",
    "        left = left.rename(columns={\"cum_cnt\":f\"u_cum_l_{W}\"}).drop(columns=[\"day\"])\n",
    "        m = pd.concat([out, right[[f\"u_cum_r_{W}\"]], left[[f\"u_cum_l_{W}\"]]], axis=1)\n",
    "        out[f\"user_events@{W}\"] = (m[f\"u_cum_r_{W}\"].fillna(0) - m[f\"u_cum_l_{W}\"].fillna(0)).astype(\"float32\")\n",
    "    return out.drop(columns=[\"day\"])\n",
    "\n",
    "def user_last_event_gap(user_events, df_q, user_col=\"user_id\", q_ts_col=\"query_ts\"):\n",
    "    \"\"\"–í—Ä–µ–º—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ —Å–æ–±—ã—Ç–∏—è –¥–æ –∑–∞–ø—Ä–æ—Å–∞ (–¥–Ω–∏).\"\"\"\n",
    "    ue = user_events.copy()\n",
    "    # merge_asof –ø–æ user_id: –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–±—ã—Ç–∏–µ ‚â§ query_ts\n",
    "    m = merge_asof_by(df_q[[user_col, q_ts_col]].rename(columns={q_ts_col:\"ts\"}), ue.rename(columns={\"ts\":\"ue_ts\"}), on=\"ts\", by=user_col, direction=\"backward\")\n",
    "    gap_days = ((m[\"ts\"] - m[\"ue_ts\"]).dt.total_seconds()/86400.0).fillna(1e6).astype(\"float32\")\n",
    "    return pd.DataFrame({user_col: df_q[user_col].values, q_ts_col: df_q[q_ts_col].values, \"u_gap_days\": gap_days})\n",
    "\n",
    "def user_item_seen_features(ui_timeline, df_q, user_col=\"user_id\", item_col=\"item_id\", q_ts_col=\"query_ts\"):\n",
    "    \"\"\"–ü—Ä–∏–∑–Ω–∞–∫–∏: –≤–∏–¥–µ–ª –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å item –¥–æ –∑–∞–ø—Ä–æ—Å–∞, —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑, –¥–∞–≤–Ω–æ—Å—Ç—å (–¥–Ω–∏).\"\"\"\n",
    "    t = ui_timeline.copy()\n",
    "    q = df_q[[user_col,item_col,q_ts_col]].copy()\n",
    "    q[\"uid_item\"] = (q[user_col].astype(\"int64\")<<32)+q[item_col].astype(\"int64\")\n",
    "    # merge_asof –ø–æ (uid_item, ts)\n",
    "    left = q.rename(columns={q_ts_col:\"ts\"})\n",
    "    m = merge_asof_by(left[[\"uid_item\",\"ts\"]], t.rename(columns={\"ts\":\"ui_ts\"}), on=\"ts\", by=\"uid_item\", direction=\"backward\")\n",
    "    seen_before = m[\"ui_cum_cnt\"].fillna(0).astype(\"int32\")\n",
    "    # –¥–∞–≤–Ω–æ—Å—Ç—å\n",
    "    rec_days = ((left[\"ts\"] - m[\"ui_ts\"]).dt.total_seconds()/86400.0).fillna(1e6).astype(\"float32\")\n",
    "    return pd.DataFrame({\n",
    "        user_col: q[user_col].values,\n",
    "        item_col: q[item_col].values,\n",
    "        q_ts_col: q[q_ts_col].values,\n",
    "        \"ui_seen_before\": (seen_before>0).astype(\"int8\"),\n",
    "        \"ui_seen_cnt\": seen_before.astype(\"int16\"),\n",
    "        \"ui_recency_days\": rec_days\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6228de",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_build_feats = W.Button(description=\"–°–æ–±—Ä–∞—Ç—å —Ñ–∏—á–∏ –¥–ª—è VAL/TEST\", button_style=\"success\", layout=BTN_LAYOUT)\n",
    "out_build_feats = W.Output()\n",
    "\n",
    "DATA = dict(X_val=None, y_val=None, group_val=None, X_test=None, group_test=None, feat_cols=None, meta=None)\n",
    "\n",
    "def build_features(split=\"val\"):\n",
    "    \"\"\"–°–æ–±–∏—Ä–∞–µ–º —Ñ–∏—á–∏, –∏—Å–ø–æ–ª—å–∑—É—è CACHE –∏ AGG.\"\"\"\n",
    "    assert split in (\"val\",\"test\")\n",
    "    inter, items, cands = CACHE[\"inter\"], CACHE[\"items\"], CACHE[\"cands\"]\n",
    "    if split==\"val\":\n",
    "        q = CACHE[\"qv\"][ [\"query_id\",\"user_id\",\"query_ts\"] ].copy()\n",
    "        pairs = CACHE[\"pv\"][ [\"query_id\",\"item_id\",\"label\"] ].copy()\n",
    "    else:\n",
    "        q = CACHE[\"qt\"][ [\"query_id\",\"user_id\",\"query_ts\"] ].copy()\n",
    "        pairs = None\n",
    "\n",
    "    # –æ–≥—Ä–∞–Ω–∏—á–∏–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Ç–µ–∫—É—â–∏–º–∏ query\n",
    "    cand = cands.merge(q[[\"query_id\",\"user_id\",\"query_ts\"]], on=\"query_id\", how=\"inner\")\n",
    "    # —Å–æ—Ä–∏—Ä—É–µ–º –∏ –æ–±—Ä–µ–∑–∞–µ–º –¥–æ K_IN (–Ω–∞ –≤—Å—è–∫–∏–π)\n",
    "    if \"score\" in cand.columns:\n",
    "        cand = cand.sort_values([\"query_id\",\"score\"], ascending=[True, False])\n",
    "    cand = cand.groupby(\"query_id\").head(STATE[\"K_IN\"]).reset_index(drop=True)\n",
    "\n",
    "    # --- SOURCE —Ñ–∏—á–∏\n",
    "    feats = []\n",
    "    base = cand[[\"query_id\",\"user_id\",\"item_id\",\"query_ts\"]].copy()\n",
    "    if FEATS[\"use_source_feats\"]:\n",
    "        sf = []\n",
    "        if \"source\" in cand.columns:\n",
    "            # one-hot –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ + —Å—á–µ—Ç—á–∏–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–µ—Å–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã –±—ã–ª–∏)\n",
    "            src_dummies = pd.get_dummies(cand[\"source\"], prefix=\"src\").astype(\"int8\")\n",
    "            tmp = pd.concat([cand[[\"query_id\",\"item_id\"]], src_dummies], axis=1)\n",
    "            g = tmp.groupby([\"query_id\",\"item_id\"]).sum().reset_index()\n",
    "            sf.append(g)\n",
    "        if \"score\" in cand.columns:\n",
    "            sc = cand[[\"query_id\",\"item_id\",\"score\"]].copy()\n",
    "            sc[\"score\"] = sc[\"score\"].astype(\"float32\")\n",
    "            sc = sc.groupby([\"query_id\",\"item_id\"])[\"score\"].max().reset_index().rename(columns={\"score\":\"cand_score_max\"})\n",
    "            sf.append(sc)\n",
    "        if \"source_rank\" in cand.columns:\n",
    "            rk = cand[[\"query_id\",\"item_id\",\"source_rank\"]].copy()\n",
    "            rk[\"source_rank\"] = rk[\"source_rank\"].astype(\"int32\")\n",
    "            rk = rk.groupby([\"query_id\",\"item_id\"])[\"source_rank\"].min().reset_index().rename(columns={\"source_rank\":\"src_rank_min\"})\n",
    "            sf.append(rk)\n",
    "        if sf:\n",
    "            s = sf[0]\n",
    "            for t in sf[1:]:\n",
    "                s = s.merge(t, on=[\"query_id\",\"item_id\"], how=\"outer\")\n",
    "            feats.append(s)\n",
    "\n",
    "    # --- POP/RECENCY (time-safe —á–µ—Ä–µ–∑ –∫—É–º—ã)\n",
    "    if FEATS[\"use_pop_recency\"]:\n",
    "        item_pop = item_pop_at_queries(AGG[\"item_day_cum\"], base.rename(columns={\"query_ts\":\"_qt\"}).rename(columns={\"_qt\":\"query_ts\"}), windows=FEATS[\"windows_days\"])\n",
    "        pr = pd.concat([base[[\"query_id\",\"item_id\"]], item_pop.drop(columns=[\"item_id\",\"query_ts\"])], axis=1)\n",
    "        feats.append(pr)\n",
    "\n",
    "    # --- USER HIST (—Å—á—ë—Ç—á–∏–∫–∏ –∑–∞ –æ–∫–Ω–∞ + gap –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏)\n",
    "    if FEATS[\"use_user_hist\"]:\n",
    "        ucnt = user_activity_at_queries(AGG[\"user_day_cum\"], base.rename(columns={\"query_ts\":\"_qt\"}).rename(columns={\"_qt\":\"query_ts\"}), windows=FEATS[\"windows_days\"])\n",
    "        ugap = user_last_event_gap(AGG[\"user_events\"], base.rename(columns={\"query_ts\":\"_qt\"}).rename(columns={\"_qt\":\"query_ts\"}))\n",
    "        uh = base[[\"query_id\"]].join(ucnt.drop(columns=[\"user_id\",\"query_ts\"]))\n",
    "        uh = pd.concat([uh, ugap.drop(columns=[\"user_id\",\"query_ts\"])], axis=1)\n",
    "        feats.append(pd.concat([base[[\"query_id\",\"item_id\"]], uh.drop(columns=[\"query_id\"])], axis=1))\n",
    "\n",
    "    # --- USER‚ÄìITEM match (–≤–∏–¥–µ–ª/—Å–∫–æ–ª—å–∫–æ/–¥–∞–≤–Ω–æ)\n",
    "    if FEATS[\"use_user_item_match\"]:\n",
    "        uim = user_item_seen_features(AGG[\"ui_timeline\"], base)\n",
    "        uim2 = pd.concat([base[[\"query_id\"]], uim.drop(columns=[\"user_id\",\"item_id\",\"query_ts\"])], axis=1)\n",
    "        feats.append(pd.concat([base[[\"query_id\",\"item_id\"]], uim2.drop(columns=[\"query_id\"])], axis=1))\n",
    "\n",
    "    # --- ITEM META (–Ω–∞–ø—Ä–∏–º–µ—Ä brand/category/price, —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å –Ω–µ–¥–∞–≤–Ω–∏–º–∏ –±—Ä–µ–Ω–¥–∞–º–∏ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏)\n",
    "    if FEATS[\"use_item_meta\"]:\n",
    "        join_cols = [\"item_id\"]\n",
    "        meta_cols = [c for c in [\"brand\",\"category\",\"price\",\"created_ts\"] if c in items.columns]\n",
    "        im = items[join_cols+meta_cols].copy()\n",
    "        if \"created_ts\" in im.columns:\n",
    "            im[\"item_age_days\"] = ((base[\"query_ts\"].iloc[0] if len(base) else pd.Timestamp.utcnow()) - pd.to_datetime(im[\"created_ts\"], utc=True)).dt.days.astype(\"float32\") if im[\"created_ts\"].notna().any() else 0.0\n",
    "            im = im.drop(columns=[\"created_ts\"])\n",
    "        # one-hot –¥–ª—è –∫—Ä—É–ø–Ω–æ–≥–æ brand/category ‚Äî –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø—Ä–æ–ø—É—Å—Ç–∏–º; –æ—Å—Ç–∞–≤–∏–º price + age\n",
    "        im[\"price\"] = im.get(\"price\", pd.Series(0.0, index=im.index)).astype(\"float32\")\n",
    "        feats.append(base[[\"query_id\",\"item_id\"]].merge(im, on=\"item_id\", how=\"left\"))\n",
    "\n",
    "    # --- CONTEXT\n",
    "    if FEATS[\"use_context\"]:\n",
    "        ctx = base[[\"query_id\",\"item_id\",\"query_ts\"]].copy()\n",
    "        ctx[\"hour\"] = ctx[\"query_ts\"].dt.hour.astype(\"int16\")\n",
    "        ctx[\"dow\"]  = ctx[\"query_ts\"].dt.dayofweek.astype(\"int16\")\n",
    "        ctx = ctx.drop(columns=[\"query_ts\"])\n",
    "        feats.append(ctx)\n",
    "\n",
    "    # --- COLD-flags (–ø–æ train-–ø–µ—Ä–∏–æ–¥—É –∏–∑ splits)\n",
    "    if FEATS[\"use_cold_flags\"]:\n",
    "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º train_end –∏–∑ splits.json\n",
    "        t_train_end = pd.to_datetime(CACHE[\"splits\"][\"bounds\"][\"train_end\"])\n",
    "        tr = inter[inter[\"ts\"]<=t_train_end]\n",
    "        cold_item = ~base[\"item_id\"].isin(tr[\"item_id\"].unique())\n",
    "        cold_user = ~base[\"user_id\"].isin(tr[\"user_id\"].unique())\n",
    "        cf = base[[\"query_id\",\"item_id\"]].copy()\n",
    "        cf[\"is_cold_item\"] = cold_item.astype(\"int8\").values\n",
    "        cf[\"is_cold_user\"] = cold_user.astype(\"int8\").values\n",
    "        feats.append(cf)\n",
    "\n",
    "    # --- merge –≤—Å–µ—Ö —Ñ–∏—á\n",
    "    X = base[[\"query_id\",\"item_id\"]].copy()\n",
    "    for f in feats:\n",
    "        X = X.merge(f, on=[\"query_id\",\"item_id\"], how=\"left\")\n",
    "    # –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ\n",
    "    for c in X.columns:\n",
    "        if c in (\"query_id\",\"item_id\"): continue\n",
    "        if pd.api.types.is_integer_dtype(X[c]): X[c] = X[c].fillna(0)\n",
    "        else: X[c] = X[c].fillna(0.0)\n",
    "\n",
    "    # label –∏ –≥—Ä—É–ø–ø—ã\n",
    "    if split==\"val\":\n",
    "        y = pairs.merge(X[[\"query_id\",\"item_id\"]], on=[\"query_id\",\"item_id\"], how=\"right\").fillna({\"label\":0})[\"label\"].astype(\"int8\")\n",
    "        group = X[\"query_id\"].value_counts().sort_index()\n",
    "        return X, y, group\n",
    "    else:\n",
    "        group = X[\"query_id\"].value_counts().sort_index()\n",
    "        return X, None, group\n",
    "\n",
    "def on_build_feats(_):\n",
    "    with out_build_feats:\n",
    "        clear_output()\n",
    "        if not CACHE or not AGG[\"item_day_cum\"] is not None:\n",
    "            print(\"–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –≤—Ö–æ–¥—ã –∏ –ø–æ—Å—Ç—Ä–æ–π—Ç–µ –∞–≥—Ä–µ–≥–∞—Ç—ã.\")\n",
    "            return\n",
    "        t0=time.time()\n",
    "        Xv, yv, gv = build_features(\"val\")\n",
    "        Xt, _,  gt = build_features(\"test\")\n",
    "        DATA.update(X_val=Xv, y_val=yv, group_val=gv, X_test=Xt, group_test=gt)\n",
    "        feat_cols = [c for c in Xv.columns if c not in (\"query_id\",\"item_id\")]\n",
    "        DATA[\"feat_cols\"]=feat_cols\n",
    "        print(\"VAL:\", Xv.shape, \"| TEST:\", Xt.shape, \"| #feats:\", len(feat_cols), \"| mem(Xv):\", f\"{mem_gb(Xv):.3f} GB\")\n",
    "        print(\"–ü—Ä–∏–º–µ—Ä—ã —Ñ–∏—á:\", feat_cols[:20])\n",
    "        # —Å–æ—Ö—Ä–∞–Ω–∏–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –Ω–∞–±–æ—Ä—ã\n",
    "        A = art_paths(STATE[\"DATASET_ID\"], STATE[\"PROFILE_PATH\"])\n",
    "        run_id = now_tag()\n",
    "        run_dir = A[\"ranker\"]/run_id\n",
    "        run_dir.mkdir(parents=True, exist_ok=True)\n",
    "        Xv.to_parquet(run_dir/\"X_rank_val.parquet\", index=False)\n",
    "        Xt.to_parquet(run_dir/\"X_rank_test.parquet\", index=False)\n",
    "        DATA[\"meta\"] = dict(run_id=run_id, run_dir=str(run_dir))\n",
    "        print(\"Saved to:\", run_dir)\n",
    "\n",
    "btn_build_feats.on_click(on_build_feats)\n",
    "W.VBox([btn_build_feats, out_build_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8aea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "\n",
    "FOLDS = dict(pairs=[])\n",
    "\n",
    "def make_folds(kind, n_splits, Xv, cv_meta=None):\n",
    "    # –≥—Ä—É–ø–ø—ã –ø–æ query_id –¥–ª—è —Ä–∞–Ω–∫–∏–Ω–≥–∞\n",
    "    qids = Xv[\"query_id\"].values\n",
    "    uniq_q = np.unique(qids)\n",
    "    if kind==\"kfold_queries\":\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=STATE[\"RANDOM_SEED\"])\n",
    "        return [(np.isin(qids, uniq_q[tr]), np.isin(qids, uniq_q[va])) for tr,va in kf.split(uniq_q)]\n",
    "    elif kind==\"group_user\":\n",
    "        # –Ω—É–∂–µ–Ω user_id –≤ Xv: –¥–æ–±–∞–≤–∏–º –≤—Ä–µ–º–µ–Ω–Ω–æ —á–µ—Ä–µ–∑ merge –∏–∑ CACHE[cands]\n",
    "        tmp = Xv.merge(CACHE[\"cands\"][ [\"query_id\",\"user_id\"] ].drop_duplicates(), on=\"query_id\", how=\"left\")\n",
    "        users = tmp[\"user_id\"].values\n",
    "        gkf = GroupKFold(n_splits=n_splits)\n",
    "        mask_pairs=[]\n",
    "        for tr, va in gkf.split(uniq_q, groups=tmp.drop_duplicates(\"query_id\")[\"user_id\"].values):\n",
    "            mask_pairs.append((np.isin(qids, uniq_q[tr]), np.isin(qids, uniq_q[va])))\n",
    "        return mask_pairs\n",
    "    else: # time\n",
    "        tmp = CACHE[\"qv\"][ [\"query_id\",\"query_ts\"] ].drop_duplicates().sort_values(\"query_ts\")\n",
    "        chunks = np.array_split(tmp[\"query_id\"].values, n_splits)\n",
    "        pairs=[]\n",
    "        for k in range(n_splits):\n",
    "            va_q = chunks[k]\n",
    "            tr_q = np.concatenate([chunks[i] for i in range(n_splits) if i!=k])\n",
    "            pairs.append((np.isin(qids, tr_q), np.isin(qids, va_q)))\n",
    "        return pairs\n",
    "\n",
    "btn_folds = W.Button(description=\"–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Ñ–æ–ª–¥—ã\", button_style=\"primary\", layout=BTN_LAYOUT)\n",
    "out_folds = W.Output()\n",
    "\n",
    "def on_folds(_):\n",
    "    with out_folds:\n",
    "        clear_output()\n",
    "        if DATA[\"X_val\"] is None:\n",
    "            print(\"–°–Ω–∞—á–∞–ª–∞ —Å–æ–±–µ—Ä–∏—Ç–µ —Ñ–∏—á–∏.\")\n",
    "            return\n",
    "        pairs = make_folds(CV[\"CV_KIND\"], CV[\"N_SPLITS\"], DATA[\"X_val\"])\n",
    "        FOLDS[\"pairs\"]=pairs\n",
    "        sizes = [int(mask_va.sum()) for _, mask_va in pairs]\n",
    "        print(\"FOLDS:\", len(pairs), \"| val sizes:\", sizes)\n",
    "\n",
    "btn_folds.on_click(on_folds)\n",
    "W.VBox([btn_folds, out_folds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_ndcg_at_k(qids, labels, scores, ks=(10,20,50,100)):\n",
    "    df = pd.DataFrame({\"q\":qids, \"y\":labels, \"s\":scores})\n",
    "    df = df.sort_values([\"q\",\"s\"], ascending=[True,False])\n",
    "    res={}\n",
    "    for K in ks:\n",
    "        hits=0; tot=0; dcg=0.0; idcg=0.0\n",
    "        for q, grp in df.groupby(\"q\", sort=False):\n",
    "            arr = grp[\"y\"].values[:K]\n",
    "            hit = int(arr.sum()>0)\n",
    "            hits += hit; tot += 1\n",
    "            # NDCG binary: –ø–æ–∑–∏—Ü–∏—è –ø–µ—Ä–≤–æ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ\n",
    "            nd = 0.0\n",
    "            for r,v in enumerate(arr, start=1):\n",
    "                if v>0:\n",
    "                    nd = 1.0/math.log2(r+1); break\n",
    "            dcg += nd; idcg += 1.0\n",
    "        res[f\"recall@{K}\"]=hits/max(1,tot)\n",
    "        res[f\"ndcg@{K}\"]=dcg/max(1.0,idcg)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb67e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_train = W.Button(description=\"–¢—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–Ω–∫–µ—Ä (CV + OOF)\", button_style=\"success\", layout=BTN_LAYOUT)\n",
    "out_train = W.Output()\n",
    "\n",
    "RUN = dict(oof=None, models=[], fi=None, cv_scores=None)\n",
    "\n",
    "def train_cv_lgbm(X, y, groups, folds_pairs, feat_cols):\n",
    "    oof_scores = np.zeros(len(X), dtype=\"float32\")\n",
    "    models=[]; fold_metrics=[]\n",
    "    for i,(mask_tr, mask_va) in enumerate(folds_pairs, start=1):\n",
    "        tr_idx = np.where(mask_tr)[0]; va_idx=np.where(mask_va)[0]\n",
    "        Xtr, ytr = X.iloc[tr_idx][feat_cols], y.iloc[tr_idx]\n",
    "        Xva, yva = X.iloc[va_idx][feat_cols], y.iloc[va_idx]\n",
    "        # –≥—Ä—É–ø–ø—ã ‚Äî –¥–ª–∏–Ω—ã –ø–æ query_id\n",
    "        gtr = X.iloc[tr_idx][\"query_id\"].value_counts().sort_index().values\n",
    "        gva = X.iloc[va_idx][\"query_id\"].value_counts().sort_index().values\n",
    "\n",
    "        params = dict(\n",
    "            objective=\"lambdarank\",\n",
    "            metric=\"ndcg\",\n",
    "            ndcg_at=[20],\n",
    "            learning_rate=MODEL[\"LR\"],\n",
    "            num_leaves=MODEL[\"NUM_LEAVES\"],\n",
    "            max_depth=MODEL[\"MAX_DEPTH\"],\n",
    "            min_data_in_leaf=MODEL[\"MIN_DATA_LEAF\"],\n",
    "            feature_fraction=MODEL[\"FEATURE_FRACT\"],\n",
    "            bagging_fraction=MODEL[\"BAGGING_FRACT\"],\n",
    "            bagging_freq=1,\n",
    "            lambda_l2=MODEL[\"L2\"],\n",
    "            verbose=-1,\n",
    "            seed=STATE[\"RANDOM_SEED\"],\n",
    "            num_threads=STATE[\"JOBS\"] if STATE[\"JOBS\"]>0 else None\n",
    "        )\n",
    "        train_set = lgb.Dataset(Xtr, ytr, group=gtr, free_raw_data=True)\n",
    "        valid_set = lgb.Dataset(Xva, yva, group=gva, reference=train_set, free_raw_data=True)\n",
    "        model = lgb.train(\n",
    "            params, train_set,\n",
    "            num_boost_round=MODEL[\"N_EST\"],\n",
    "            valid_sets=[valid_set],\n",
    "            valid_names=[\"valid\"],\n",
    "            callbacks=[lgb.early_stopping(MODEL[\"EARLY_ROUNDS\"], verbose=False)]\n",
    "        )\n",
    "        pred = model.predict(Xva, num_iteration=model.best_iteration)\n",
    "        oof_scores[va_idx]=pred.astype(\"float32\")\n",
    "        # –º–µ—Ç—Ä–∏–∫–∏\n",
    "        m = recall_ndcg_at_k(X.iloc[va_idx][\"query_id\"].values, yva.values, pred, ks=(10,20,50,100))\n",
    "        fold_metrics.append(m)\n",
    "        models.append(model)\n",
    "        print(f\"[fold {i}] best_iter={model.best_iteration} | metrics:\", {k:round(v,4) for k,v in m.items()})\n",
    "    # —Ñ–∏—á-–∏–º–ø–æ—Ä—Ç–∞–Ω—Å—ã\n",
    "    fi = None\n",
    "    try:\n",
    "        imp = np.mean([m.feature_importance(importance_type=\"gain\") for m in models], axis=0)\n",
    "        fi = pd.DataFrame({\"feature\":feat_cols, \"gain\":imp}).sort_values(\"gain\", ascending=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return oof_scores, models, fi, fold_metrics\n",
    "\n",
    "def train_cv_catboost(X, y, groups, folds_pairs, feat_cols):\n",
    "    oof_scores = np.zeros(len(X), dtype=\"float32\")\n",
    "    models=[]; fold_metrics=[]\n",
    "    for i,(mask_tr, mask_va) in enumerate(folds_pairs, start=1):\n",
    "        tr_idx = np.where(mask_tr)[0]; va_idx=np.where(mask_va)[0]\n",
    "        Xtr, ytr = X.iloc[tr_idx][feat_cols], y.iloc[tr_idx]\n",
    "        Xva, yva = X.iloc[va_idx][feat_cols], y.iloc[va_idx]\n",
    "        group_ids_tr = X.iloc[tr_idx][\"query_id\"].values\n",
    "        group_ids_va = X.iloc[va_idx][\"query_id\"].values\n",
    "\n",
    "        params = dict(\n",
    "            loss_function=\"YetiRankPairwise\",\n",
    "            iterations=MODEL[\"N_EST\"],\n",
    "            learning_rate=MODEL[\"LR\"],\n",
    "            depth=MODEL[\"MAX_DEPTH\"],\n",
    "            l2_leaf_reg=MODEL[\"L2\"],\n",
    "            random_seed=STATE[\"RANDOM_SEED\"],\n",
    "            verbose=False\n",
    "        )\n",
    "        train_pool = Pool(Xtr, label=ytr, group_id=group_ids_tr)\n",
    "        valid_pool = Pool(Xva, label=yva, group_id=group_ids_va)\n",
    "        model = CatBoostRanker(**params)\n",
    "        model.fit(train_pool, eval_set=valid_pool, use_best_model=True, verbose=False)\n",
    "        pred = model.predict(valid_pool)\n",
    "        oof_scores[va_idx]=pred.astype(\"float32\")\n",
    "        m = recall_ndcg_at_k(X.iloc[va_idx][\"query_id\"].values, yva.values, pred, ks=(10,20,50,100))\n",
    "        fold_metrics.append(m)\n",
    "        models.append(model)\n",
    "        print(f\"[fold {i}] metrics:\", {k:round(v,4) for k,v in m.items()})\n",
    "    fi=None\n",
    "    try:\n",
    "        imp = np.mean([m.get_feature_importance(type=\"FeatureImportance\", data=Pool(DATA[\"X_val\"][feat_cols], label=DATA[\"y_val\"], group_id=DATA[\"X_val\"][\"query_id\"].values)) for m in models], axis=0)\n",
    "        fi = pd.DataFrame({\"feature\":feat_cols, \"gain\":imp}).sort_values(\"gain\", ascending=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return oof_scores, models, fi, fold_metrics\n",
    "\n",
    "def on_train(_):\n",
    "    with out_train:\n",
    "        clear_output()\n",
    "        if DATA[\"X_val\"] is None or not FOLDS[\"pairs\"]:\n",
    "            print(\"–°–Ω–∞—á–∞–ª–∞ —Å–æ–±–µ—Ä–∏—Ç–µ —Ñ–∏—á–∏ –∏ —Å—Ñ–æ—Ä–º–∏—Ä—É–π—Ç–µ —Ñ–æ–ª–¥—ã.\")\n",
    "            return\n",
    "        Xv, yv = DATA[\"X_val\"], DATA[\"y_val\"]\n",
    "        feat_cols = DATA[\"feat_cols\"]\n",
    "        if MODEL[\"LIB\"]==\"lightgbm\":\n",
    "            if lgb is None:\n",
    "                print(\"LightGBM –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\")\n",
    "                return\n",
    "            oof, models, fi, fold_metrics = train_cv_lgbm(Xv, yv, DATA[\"group_val\"], FOLDS[\"pairs\"], feat_cols)\n",
    "        else:\n",
    "            if CatBoostRanker is None:\n",
    "                print(\"CatBoost –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω\")\n",
    "                return\n",
    "            oof, models, fi, fold_metrics = train_cv_catboost(Xv, yv, DATA[\"group_val\"], FOLDS[\"pairs\"], feat_cols)\n",
    "        RUN.update(oof=oof, models=models, fi=fi, cv_scores=fold_metrics)\n",
    "        # –û–±—â–∏–µ OOF-–º–µ—Ç—Ä–∏–∫–∏\n",
    "        oof_metrics = recall_ndcg_at_k(Xv[\"query_id\"].values, yv.values, oof, ks=(10,20,50,100))\n",
    "        print(\"OOF:\", {k:round(v,4) for k,v in oof_metrics.items()})\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º OOF\n",
    "        A = art_paths(STATE[\"DATASET_ID\"], STATE[\"PROFILE_PATH\"])\n",
    "        run_dir = Path(DATA[\"meta\"][\"run_dir\"])\n",
    "        oof_df = Xv[[\"query_id\",\"item_id\"]].copy()\n",
    "        oof_df[\"label\"]=yv.values\n",
    "        oof_df[\"oof_score\"]=oof\n",
    "        oof_df.to_parquet(run_dir/\"oof.parquet\", index=False)\n",
    "        if fi is not None:\n",
    "            fi.to_csv(run_dir/\"fi.csv\", index=False)\n",
    "        save_json(run_dir/\"cv_metrics.json\", dict(folds=RUN[\"cv_scores\"], oof=oof_metrics))\n",
    "        print(\"Saved:\", run_dir)\n",
    "\n",
    "btn_train.on_click(on_train)\n",
    "W.VBox([btn_train, out_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d21739",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_infer = W.Button(description=\"–ò–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ TEST ‚Üí ranked.parquet\", button_style=\"warning\", layout=BTN_LAYOUT)\n",
    "out_infer = W.Output()\n",
    "\n",
    "def on_infer(_):\n",
    "    with out_infer:\n",
    "        clear_output()\n",
    "        if DATA[\"X_test\"] is None or not RUN[\"models\"]:\n",
    "            print(\"–ù–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∏—á –∏–ª–∏ –Ω–µ –æ–±—É—á–µ–Ω—ã –º–æ–¥–µ–ª–∏.\")\n",
    "            return\n",
    "        Xt = DATA[\"X_test\"]\n",
    "        feat_cols = DATA[\"feat_cols\"]\n",
    "        # —É—Å—Ä–µ–¥–Ω–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ —Ñ–æ–ª–¥–∞–º\n",
    "        preds = np.zeros(len(Xt), dtype=\"float32\")\n",
    "        for m in RUN[\"models\"]:\n",
    "            p = m.predict(Xt[feat_cols]) if MODEL[\"LIB\"]==\"lightgbm\" else m.predict(Pool(Xt[feat_cols], group_id=Xt[\"query_id\"].values))\n",
    "            preds += p.astype(\"float32\")\n",
    "        preds /= max(1,len(RUN[\"models\"]))\n",
    "        ranked = Xt[[\"query_id\",\"item_id\"]].copy()\n",
    "        ranked[\"score\"]=preds\n",
    "        ranked = ranked.sort_values([\"query_id\",\"score\"], ascending=[True,False]).groupby(\"query_id\").head(STATE[\"K_OUT\"])\n",
    "        ranked[\"rank\"] = ranked.groupby(\"query_id\")[\"score\"].rank(ascending=False, method=\"first\").astype(\"int32\")\n",
    "        run_dir = Path(DATA[\"meta\"][\"run_dir\"])\n",
    "        ranked.to_parquet(run_dir/\"ranked.parquet\", index=False)\n",
    "        # (–æ–ø—Ü.) —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ —Å–∫–æ—Ä—ã\n",
    "        Xt.assign(score=preds).to_parquet(run_dir/\"test_scores.parquet\", index=False)\n",
    "        print(\"‚úì saved:\", run_dir/\"ranked.parquet\")\n",
    "\n",
    "btn_infer.on_click(on_infer)\n",
    "W.VBox([btn_infer, out_infer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761fc72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_slices = W.Button(description=\"OOF —Å—Ä–µ–∑—ã –∏ —Å–≤–æ–¥–∫–∞\", button_style=\"\", layout=BTN_LAYOUT)\n",
    "out_slices = W.Output()\n",
    "\n",
    "def on_slices(_):\n",
    "    with out_slices:\n",
    "        clear_output()\n",
    "        if RUN[\"oof\"] is None:\n",
    "            print(\"–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏ –º–æ–¥–µ–ª—å, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å OOF.\")\n",
    "            return\n",
    "        Xv, yv, oof = DATA[\"X_val\"], DATA[\"y_val\"], RUN[\"oof\"]\n",
    "        # –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "        base = recall_ndcg_at_k(Xv[\"query_id\"].values, yv.values, oof, ks=(10,20,50,100))\n",
    "        print(\"OOF:\", {k:round(v,4) for k,v in base.items()})\n",
    "        # —Ö–æ–ª–æ–¥–Ω—ã–µ —Å—Ä–µ–∑—ã\n",
    "        if \"is_cold_item\" in DATA[\"X_val\"].columns:\n",
    "            mask = DATA[\"X_val\"][\"is_cold_item\"]==1\n",
    "            m_cold = recall_ndcg_at_k(Xv[\"query_id\"].values[mask.values], yv.values[mask.values], oof[mask.values], ks=(20,))\n",
    "            m_warm = recall_ndcg_at_k(Xv[\"query_id\"].values[~mask.values], yv.values[~mask.values], oof[~mask.values], ks=(20,))\n",
    "            print(\"cold_item@20:\", m_cold, \"| warm_item@20:\", m_warm)\n",
    "        # —Å–æ—Ö—Ä–∞–Ω–∏–º –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç—á—ë—Ç\n",
    "        run_dir = Path(DATA[\"meta\"][\"run_dir\"])\n",
    "        save_json(run_dir/\"oof_report.json\", dict(overall=base))\n",
    "        print(\"Report saved:\", run_dir/\"oof_report.json\")\n",
    "\n",
    "btn_slices.on_click(on_slices)\n",
    "W.VBox([btn_slices, out_slices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e9d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_pass = W.Button(description=\"–°–æ—Ö—Ä–∞–Ω–∏—Ç—å ranker_report.json\", button_style=\"success\", layout=BTN_LAYOUT)\n",
    "out_pass = W.Output()\n",
    "\n",
    "def on_pass(_):\n",
    "    with out_pass:\n",
    "        clear_output()\n",
    "        run_dir = Path(DATA[\"meta\"][\"run_dir\"])\n",
    "        oof_metrics = {}\n",
    "        try:\n",
    "            oof_metrics = json.loads((run_dir/\"cv_metrics.json\").read_text())[\"oof\"]\n",
    "        except Exception:\n",
    "            pass\n",
    "        report = dict(\n",
    "            dataset_id = STATE[\"DATASET_ID\"],\n",
    "            profile    = STATE[\"PROFILE_PATH\"],\n",
    "            run_id     = DATA[\"meta\"][\"run_id\"],\n",
    "            params = dict(CV=CV, MODEL=MODEL, FEATS=FEATS, K_IN=STATE[\"K_IN\"], K_OUT=STATE[\"K_OUT\"], seed=STATE[\"RANDOM_SEED\"]),\n",
    "            files = dict(\n",
    "                X_val=str(run_dir/\"X_rank_val.parquet\"),\n",
    "                X_test=str(run_dir/\"X_rank_test.parquet\"),\n",
    "                oof=str(run_dir/\"oof.parquet\"),\n",
    "                ranked=str(run_dir/\"ranked.parquet\")\n",
    "            ),\n",
    "            oof_metrics = oof_metrics,\n",
    "            saved_at = datetime.utcnow().isoformat()+\"Z\"\n",
    "        )\n",
    "        save_json(run_dir/\"ranker_report.json\", report)\n",
    "        print(\"‚úì saved:\", run_dir/\"ranker_report.json\")\n",
    "        print(json.dumps(report[\"oof_metrics\"], indent=2))\n",
    "\n",
    "btn_pass.on_click(on_pass)\n",
    "W.VBox([btn_pass, out_pass])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a37979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_panic = W.Button(description=\"‚ö° PANIC: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä —Ñ–∏—á + –±—ã—Å—Ç—Ä—ã–π —Ä–∞–Ω–∫–µ—Ä\", button_style=\"danger\", layout=BTN_LAYOUT)\n",
    "out_panic = W.Output()\n",
    "\n",
    "def on_panic(_):\n",
    "    with out_panic:\n",
    "        clear_output()\n",
    "        # –∂—ë—Å—Ç–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "        FEATS.update(use_source_feats=True, use_pop_recency=True, use_user_hist=False, use_user_item_match=True,\n",
    "                     use_item_meta=False, use_context=False, use_cold_flags=True, windows_days=[7,30], user_last_k=3)\n",
    "        MODEL.update(LIB=\"lightgbm\", N_EST=250, LR=0.07, MAX_DEPTH=6, NUM_LEAVES=63, EARLY_ROUNDS=30, FEATURE_FRACT=0.9, BAGGING_FRACT=0.8, MIN_DATA_LEAF=20, L2=1.0)\n",
    "        print(\"PANIC config applied. Rebuild feats ‚Üí Train ‚Üí Infer.\")\n",
    "        # –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–±–æ—Ä–∫—É —Ñ–∏—á, –æ–±—É—á–µ–Ω–∏–µ –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å\n",
    "        on_build_feats(None)\n",
    "        on_folds(None)\n",
    "        on_train(None)\n",
    "        on_infer(None)\n",
    "\n",
    "btn_panic.on_click(on_panic)\n",
    "W.VBox([btn_panic, out_panic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba72d2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_all = W.Button(description=\"üöÄ –ë—ã—Å—Ç—Ä—ã–π –ø—Ä–æ–≥–æ–Ω: check ‚Üí preagg ‚Üí feats ‚Üí folds ‚Üí train ‚Üí infer ‚Üí report\", button_style=\"success\", layout=BTN_LAYOUT)\n",
    "out_all = W.Output()\n",
    "\n",
    "def on_all(_):\n",
    "    with out_all:\n",
    "        clear_output()\n",
    "        try:\n",
    "            on_check(None)\n",
    "            on_preagg(None)\n",
    "            on_build_feats(None)\n",
    "            on_folds(None)\n",
    "            on_train(None)\n",
    "            on_infer(None)\n",
    "            on_slices(None)\n",
    "            on_pass(None)\n",
    "            print(\"\\n‚úì –ì–æ—Ç–æ–≤–æ.\")\n",
    "        except Exception as e:\n",
    "            print(\"–û–®–ò–ë–ö–ê:\", e)\n",
    "            raise\n",
    "\n",
    "btn_all.on_click(on_all)\n",
    "W.VBox([btn_all, out_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1b6053",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_list = W.Button(description=\"–ü–æ–∫–∞–∑–∞—Ç—å —Ñ–∞–π–ª—ã —Ä–∞–Ω–∫–µ—Ä–∞\", button_style=\"\", layout=BTN_LAYOUT)\n",
    "out_list = W.Output()\n",
    "\n",
    "def on_list(_):\n",
    "    with out_list:\n",
    "        clear_output()\n",
    "        A = art_paths(STATE[\"DATASET_ID\"], STATE[\"PROFILE_PATH\"])\n",
    "        d = A[\"ranker\"]\n",
    "        if not d.exists():\n",
    "            print(\"–ü–∞–ø–∫–∞ –µ—â—ë –Ω–µ —Å–æ–∑–¥–∞–Ω–∞:\", d); return\n",
    "        files=[]\n",
    "        for p in sorted(d.rglob(\"*\")):\n",
    "            if p.is_file():\n",
    "                try:\n",
    "                    size = p.stat().st_size/1024/1024\n",
    "                except Exception:\n",
    "                    size = float(\"nan\")\n",
    "                files.append({\"path\":str(p), \"size_MB\": round(size,3)})\n",
    "        df = pd.DataFrame(files)\n",
    "        display(df if len(df) else \"—Ñ–∞–π–ª–æ–≤ –ø–æ–∫–∞ –Ω–µ—Ç\")\n",
    "\n",
    "btn_list.on_click(on_list)\n",
    "W.VBox([btn_list, out_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb31a66",
   "metadata": {},
   "source": [
    "### –ü–æ—Ä–æ–≥–∏ –∏ –ø–æ–¥—Å–∫–∞–∑–∫–∏\n",
    "- **–¶–µ–ª—å**: Recall@20 ‚â• 0.90 (OOF). –í —Ü–µ–π—Ç–Ω–æ—Ç–µ (PANIC) ‚Äî ‚â• 0.88.\n",
    "- –ï—Å–ª–∏ –º–Ω–æ–≥–æ –ø—É—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Ä–∞–Ω–∫-–≤—Ö–æ–¥–µ ‚Üí –≤–µ—Ä–Ω–∏—Å—å –≤ 02 –∏ —É—Å–∏–ª–∏ fallbacks.\n",
    "- –•–æ–ª–æ–¥–Ω—ã–µ –ø—Ä–æ—Å–∞–¥–∫–∏ ‚Üí –¥–æ–±–∞–≤—å –∫–æ–Ω—Ç–µ–Ω—Ç/brand-aware —Ñ–∏—á–∏, —Ä–∞—Å—à–∏—Ä—å –æ–∫–Ω–∞ covis (–≤ 02 –∏ –∑–¥–µ—Å—å –≤ user_hist).\n",
    "- –ï—Å–ª–∏ OOF ‚â´ holdout ‚Üí —É–º–µ–Ω—å—à–∏ –≥–ª—É–±–∏–Ω—É/—É—Å–∏–ª—å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
