{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d56364",
   "metadata": {},
   "source": [
    "# 05 ¬∑ Submit ‚Äî —É–ø–∞–∫–æ–≤–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å–∞–±–º–∏—Ç–∞ üöÄ\n",
    "\n",
    "–ë–µ—Ä—ë–º `ranked.parquet` –∏–∑ 04_blend_eval ‚Üí –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ/—Ñ–æ—Ä–º–∞—Ç ‚Üí —Å—Ç–∞–±–∏–ª—å–Ω–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ —Ä–∞–Ω–≥–∏ ‚Üí –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è\n",
    "–≤ —Ñ–æ—Ä–º–∞—Ç —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è ‚Üí –ø–æ–¥–ø–∏—Å—å/–≤–µ—Ä—Å–∏—Ä–æ–≤–∞–Ω–∏–µ ‚Üí zip (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ) ‚Üí (–æ–ø—Ü.) –∞–≤—Ç–æ—Å–∞–±–º–∏—Ç.\n",
    "\n",
    "–ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Å–∞–±–º–∏—Ç–∞: `artifacts/recsys/submits/<dataset>/<profile>/<SUBMIT_ID>/...`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce5174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "/* ipywidgets v8 (JupyterLab 4) */\n",
    ".jp-OutputArea .widget-button .widget-label { \n",
    "  white-space: normal !important; \n",
    "  overflow: visible !important; \n",
    "  text-overflow: clip !important;\n",
    "  line-height: 1.2 !important;\n",
    "}\n",
    "/* fallback –¥–ª—è ipywidgets v7 */\n",
    ".jupyter-widgets.widget-button .widget-label {\n",
    "  white-space: normal !important; \n",
    "  overflow: visible !important; \n",
    "  text-overflow: clip !important;\n",
    "  line-height: 1.2 !important;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d4e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, time, math, warnings, hashlib, zipfile, random\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import ipywidgets as W\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_colwidth\", 140)\n",
    "\n",
    "REPO = Path.cwd()\n",
    "\n",
    "def mem_gb(obj=None):\n",
    "    try:\n",
    "        import psutil\n",
    "        if obj is None: return psutil.Process().memory_info().rss/(1024**3)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if hasattr(obj, \"memory_usage\"):\n",
    "            return float(obj.memory_usage(deep=True).sum())/(1024**3)\n",
    "        return float(np.array(obj).nbytes)/(1024**3)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def save_json(path: Path, obj: dict):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(obj, ensure_ascii=False, indent=2))\n",
    "\n",
    "def now_tag(): \n",
    "    return datetime.now().strftime(\"%m%d_%H%M\")\n",
    "\n",
    "def short_hash(s: str, n=8) -> str:\n",
    "    return hashlib.sha1(s.encode()).hexdigest()[:n]\n",
    "\n",
    "def art_paths(dataset_id: str, profile_path: str):\n",
    "    prof = Path(profile_path).stem\n",
    "    root = Path(\"artifacts\")\n",
    "    return dict(\n",
    "        dataio  = root/\"recsys\"/\"dataio\"/dataset_id/prof,\n",
    "        rankers = root/\"recsys\"/\"ranker\"/dataset_id/prof,\n",
    "        ensemble= root/\"recsys\"/\"ensemble\"/dataset_id/prof,\n",
    "        submits = root/\"recsys\"/\"submits\"/dataset_id/prof,\n",
    "        profile = prof\n",
    "    )\n",
    "\n",
    "def read_parquet_safe(path: Path, cols: List[str] = None):\n",
    "    df = pd.read_parquet(path)\n",
    "    if cols:\n",
    "        miss = [c for c in cols if c not in df.columns]\n",
    "        if miss:\n",
    "            raise AssertionError(f\"{path} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Ç–æ–ª–±—Ü—ã: {miss}\")\n",
    "        return df[cols].copy()\n",
    "    return df\n",
    "\n",
    "def fingerprint(path: Path):\n",
    "    h1 = hashlib.sha1(); h2 = hashlib.md5()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1<<20), b\"\"):\n",
    "            h1.update(chunk); h2.update(chunk)\n",
    "    return dict(sha1=h1.hexdigest(), md5=h2.hexdigest(), bytes=path.stat().st_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad2fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "BTN_LAYOUT = W.Layout(min_width=\"220px\", width=\"auto\", height=\"36px\", flex=\"0 0 auto\")\n",
    "ROW_LAYOUT = W.Layout(flex_flow=\"row wrap\", grid_gap=\"8px\")\n",
    "GRID_LAYOUT = W.Layout(grid_template_columns=\"repeat(3, minmax(220px, 1fr))\", grid_gap=\"8px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5150c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE = dict(\n",
    "    DATASET_ID   = \"s5e11\",\n",
    "    PROFILE_PATH = \"configs/recsys/profiles/gate.yaml\",\n",
    "    K_OUT        = 100,\n",
    "    QUERY_COL    = \"query_id\",\n",
    "    ITEM_COL     = \"item_id\",\n",
    "    RANK_COL     = \"rank\",\n",
    "    SUBMIT_FORMAT= \"pairs_csv\",     # pairs_csv | space_separated | jsonl | custom\n",
    "    STRICT_SCHEMA= True,\n",
    "    STABLE_TIEBREAK = True,\n",
    "    COMPRESS_ZIP = True,\n",
    "    RANDOM_SEED  = 42,\n",
    "    CUSTOM_SCHEMA_PATH = \"configs/recsys/submission_schema.json\",  # –¥–ª—è custom\n",
    "    KAGGLE_COMP  = \"\",              # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: id —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è –¥–ª—è Kaggle API\n",
    ")\n",
    "\n",
    "w_ds   = W.Text(STATE[\"DATASET_ID\"], description=\"dataset_id:\", layout=W.Layout(width=\"240px\"))\n",
    "w_prof = W.Text(STATE[\"PROFILE_PATH\"], description=\"profile:\", layout=W.Layout(width=\"520px\"))\n",
    "w_kout = W.IntText(STATE[\"K_OUT\"], description=\"K_OUT:\", layout=W.Layout(width=\"160px\"))\n",
    "w_qc   = W.Text(STATE[\"QUERY_COL\"], description=\"QUERY_COL:\", layout=W.Layout(width=\"220px\"))\n",
    "w_ic   = W.Text(STATE[\"ITEM_COL\"], description=\"ITEM_COL:\", layout=W.Layout(width=\"220px\"))\n",
    "w_rc   = W.Text(STATE[\"RANK_COL\"], description=\"RANK_COL:\", layout=W.Layout(width=\"220px\"))\n",
    "w_fmt  = W.Dropdown(options=[\"pairs_csv\",\"space_separated\",\"jsonl\",\"custom\"], value=STATE[\"SUBMIT_FORMAT\"], description=\"format:\")\n",
    "w_str  = W.Checkbox(STATE[\"STRICT_SCHEMA\"], description=\"strict_schema\")\n",
    "w_tie  = W.Checkbox(STATE[\"STABLE_TIEBREAK\"], description=\"stable_tiebreak\")\n",
    "w_zip  = W.Checkbox(STATE[\"COMPRESS_ZIP\"], description=\"compress_zip\")\n",
    "w_seed = W.IntText(STATE[\"RANDOM_SEED\"], description=\"seed:\")\n",
    "w_schema = W.Text(STATE[\"CUSTOM_SCHEMA_PATH\"], description=\"custom_schema:\", layout=W.Layout(width=\"520px\"))\n",
    "w_kaggle = W.Text(STATE[\"KAGGLE_COMP\"], description=\"kaggle_comp:\", layout=W.Layout(width=\"320px\"))\n",
    "\n",
    "btn_scan = W.Button(description=\"–°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –∞–Ω—Å–∞–º–±–ª–∏\", button_style=\"primary\", layout=BTN_LAYOUT)\n",
    "out_scan = W.Output()\n",
    "SEL = dict(ens=[], widget=None)\n",
    "\n",
    "def list_ensemble_runs(dataset_id, profile_path) -> List[Path]:\n",
    "    A = art_paths(dataset_id, profile_path)\n",
    "    d = A[\"ensemble\"]\n",
    "    if not d.exists(): return []\n",
    "    runs = [p for p in sorted(d.iterdir()) if p.is_dir()]\n",
    "    # —Ç–æ–ª—å–∫–æ —Ç–µ, –≥–¥–µ –µ—Å—Ç—å ranked.parquet\n",
    "    return [r for r in runs if (r/\"ranked.parquet\").exists()]\n",
    "\n",
    "def on_scan(_):\n",
    "    with out_scan:\n",
    "        clear_output()\n",
    "        # –æ–±–Ω–æ–≤–∏–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ\n",
    "        STATE.update(\n",
    "            DATASET_ID=w_ds.value.strip(), PROFILE_PATH=w_prof.value.strip(),\n",
    "            K_OUT=int(w_kout.value), QUERY_COL=w_qc.value.strip(), ITEM_COL=w_ic.value.strip(),\n",
    "            RANK_COL=w_rc.value.strip(), SUBMIT_FORMAT=w_fmt.value, STRICT_SCHEMA=bool(w_str.value),\n",
    "            STABLE_TIEBREAK=bool(w_tie.value), COMPRESS_ZIP=bool(w_zip.value), RANDOM_SEED=int(w_seed.value),\n",
    "            CUSTOM_SCHEMA_PATH=w_schema.value.strip(), KAGGLE_COMP=w_kaggle.value.strip()\n",
    "        )\n",
    "        ens = list_ensemble_runs(STATE[\"DATASET_ID\"], STATE[\"PROFILE_PATH\"])\n",
    "        if not ens:\n",
    "            print(\"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∞–Ω—Å–∞–º–±–ª–µ–π. –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–µ—Ä–∏ –≤ 04_blend_eval.\")\n",
    "            return\n",
    "        options = [(f\"{e.name}\", str(e)) for e in ens]\n",
    "        SEL[\"ens\"]=ens\n",
    "        SEL[\"widget\"]= W.Select(options=options, description=\"ens_id:\", rows=min(len(options), 12), layout=W.Layout(width=\"520px\"))\n",
    "        display(SEL[\"widget\"])\n",
    "\n",
    "W.VBox([\n",
    "    W.HTML(\"<h3>–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–∞–±–º–∏—Ç–∞</h3>\"),\n",
    "    W.HBox([w_ds, w_seed, w_kout], layout=ROW_LAYOUT),\n",
    "    w_prof,\n",
    "    W.HBox([w_qc, w_ic, w_rc], layout=ROW_LAYOUT),\n",
    "    W.HBox([w_fmt, w_str, w_tie, w_zip], layout=ROW_LAYOUT),\n",
    "    w_schema,\n",
    "    w_kaggle,\n",
    "    btn_scan, out_scan\n",
    "])\n",
    "\n",
    "btn_scan.on_click(on_scan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_load = W.Button(description=\"–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å\", button_style=\"success\", layout=BTN_LAYOUT)\n",
    "out_load = W.Output()\n",
    "\n",
    "DATA = dict(ranked=None, scores=None, ens_dir=None, test_queries=None, blender_config=None)\n",
    "\n",
    "def try_load_test_queries(A) -> Optional[pd.Series]:\n",
    "    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç-queries –∏–∑ dataio-–ø–∞–ø–∫–∏\n",
    "    candidates = [\n",
    "        A[\"dataio\"]/ \"test_queries.parquet\",\n",
    "        A[\"dataio\"]/ \"test_queries.csv\",\n",
    "        A[\"dataio\"]/ \"test.parquet\",\n",
    "        A[\"dataio\"]/ \"test.csv\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            df = pd.read_parquet(p) if p.suffix==\".parquet\" else pd.read_csv(p)\n",
    "            # –∏—â–µ–º –∫–æ–ª–æ–Ω–∫—É query_id\n",
    "            qcol = STATE[\"QUERY_COL\"]\n",
    "            if qcol not in df.columns:\n",
    "                # fallback: –ø–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ —á—Ç–æ-—Ç–æ –ø–æ—Ö–æ–∂–µ–µ\n",
    "                for c in df.columns:\n",
    "                    if \"query\" in c.lower():\n",
    "                        qcol = c; break\n",
    "            if qcol not in df.columns:\n",
    "                continue\n",
    "            return df[qcol].astype(str)\n",
    "    return None\n",
    "\n",
    "def on_load(_):\n",
    "    with out_load:\n",
    "        clear_output()\n",
    "        if not SEL.get(\"widget\"):\n",
    "            print(\"–°–Ω–∞—á–∞–ª–∞ –æ—Ç—Å–∫–∞–Ω–∏—Ä—É–π –∞–Ω—Å–∞–º–±–ª–∏ –∏ –≤—ã–±–µ—Ä–∏ –æ–¥–∏–Ω.\")\n",
    "            return\n",
    "        ens_dir = Path(SEL[\"widget\"].value)\n",
    "        ranked_path = ens_dir/\"ranked.parquet\"\n",
    "        scores_path = ens_dir/\"scores_blend.parquet\"\n",
    "        config_path = ens_dir/\"blender_config.json\"\n",
    "        if not ranked_path.exists():\n",
    "            print(f\"–ù–µ—Ç ranked.parquet –≤ {ens_dir}\")\n",
    "            return\n",
    "        ranked = pd.read_parquet(ranked_path)\n",
    "        ranked[STATE[\"QUERY_COL\"]] = ranked[STATE[\"QUERY_COL\"]].astype(str)\n",
    "        ranked[STATE[\"ITEM_COL\"]]  = ranked[STATE[\"ITEM_COL\"]].astype(str)\n",
    "        print(\"ranked:\", ranked.shape, \"mem:\", round(mem_gb(ranked), 3), \"GB\")\n",
    "        display(ranked.head(3))\n",
    "        scores = None\n",
    "        if scores_path.exists():\n",
    "            scores = pd.read_parquet(scores_path)\n",
    "            print(\"scores_blend:\", scores.shape, \"mem:\", round(mem_gb(scores), 3), \"GB\")\n",
    "        blender_config = json.loads(config_path.read_text()) if config_path.exists() else {}\n",
    "        A = art_paths(STATE[\"DATASET_ID\"], STATE[\"PROFILE_PATH\"])\n",
    "        test_q = try_load_test_queries(A)\n",
    "\n",
    "        DATA.update(ranked=ranked, scores=scores, ens_dir=str(ens_dir), test_queries=test_q, blender_config=blender_config)\n",
    "        if test_q is not None:\n",
    "            print(\"test_queries –Ω–∞–π–¥–µ–Ω–æ:\", len(test_q.unique()))\n",
    "        else:\n",
    "            print(\"test_queries –Ω–µ –Ω–∞–π–¥–µ–Ω—ã ‚Äî –ø—Ä–æ–≤–µ—Ä–∏–º –ø–æ–∫—Ä—ã—Ç–∏–µ –ø–æ –∏–º–µ—é—â–µ–º—É—Å—è ranked.\")\n",
    "\n",
    "btn_load.on_click(on_load)\n",
    "W.VBox([btn_load, out_load])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e667d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_fix = W.Button(description=\"–ü—Ä–æ–≤–µ—Ä–∏—Ç—å/–∏—Å–ø—Ä–∞–≤–∏—Ç—å –ø–æ–∫—Ä—ã—Ç–∏–µ –∏ —Ä–∞–Ω–≥–∏\", button_style=\"warning\", layout=BTN_LAYOUT)\n",
    "out_fix = W.Output()\n",
    "\n",
    "FIXED = dict(df=None, report=None)\n",
    "\n",
    "def build_fallback_pop(ranked_df: pd.DataFrame) -> List[str]:\n",
    "    # –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å item_id –ø–æ –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ –≤ —Ä–∞–Ω–∫–∞—Ö\n",
    "    pop = ranked_df.groupby(STATE[\"ITEM_COL\"]).size().sort_values(ascending=False).index.astype(str).tolist()\n",
    "    return pop\n",
    "\n",
    "def ensure_k_per_query(df: pd.DataFrame, all_queries: Optional[pd.Series], K: int) -> Tuple[pd.DataFrame, dict]:\n",
    "    qcol, icol = STATE[\"QUERY_COL\"], STATE[\"ITEM_COL\"]\n",
    "    # –¥–µ–¥—É–ø –ø–æ (q,i)\n",
    "    df = df.drop_duplicates(subset=[qcol, icol]).copy()\n",
    "    # –µ—Å–ª–∏ –Ω–µ—Ç –ø–æ–ª–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ queries ‚Äî –±–µ—Ä—ë–º –∏–∑ df\n",
    "    if all_queries is None:\n",
    "        all_queries = df[qcol].astype(str).drop_duplicates().sort_values()\n",
    "    else:\n",
    "        all_queries = all_queries.astype(str).drop_duplicates().sort_values()\n",
    "\n",
    "    # fallback pool\n",
    "    fallback_items = build_fallback_pop(df)\n",
    "    pool = np.array(fallback_items, dtype=str)\n",
    "\n",
    "    rows_out = []\n",
    "    added = 0\n",
    "    missing_q = 0\n",
    "    for q in all_queries:\n",
    "        part = df.loc[df[qcol]==q, [qcol, icol, \"score\"]]\n",
    "        if part.empty:\n",
    "            # —Ü–µ–ª–∏–∫–æ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç query ‚Üí –∑–∞–ø–æ–ª–Ω–∏–º top-K –ø–æ–ø—É–ª—è—Ä–Ω—ã–º–∏\n",
    "            take = pool[:K]\n",
    "            rows_out.append(pd.DataFrame({qcol:q, icol:take, \"score\":np.linspace(0.0, -1e-6*(K-1), K)}))\n",
    "            added += K; missing_q += 1\n",
    "            continue\n",
    "        # –µ—Å–ª–∏ –º–µ–Ω—å—à–µ K ‚Äî –¥–æ–ø–æ–ª–Ω–∏–º\n",
    "        have = set(part[icol].astype(str).tolist())\n",
    "        need = K - len(part)\n",
    "        if need>0:\n",
    "            fill = [it for it in pool if it not in have][:need]\n",
    "            if fill:\n",
    "                filler = pd.DataFrame({qcol:q, icol:fill, \"score\":np.linspace(part[\"score\"].min()-1e-6, part[\"score\"].min()-1e-6*need, need)})\n",
    "                part = pd.concat([part, filler], axis=0, ignore_index=True)\n",
    "                added += need\n",
    "        # –µ—Å–ª–∏ –±–æ–ª—å—à–µ K ‚Äî –æ–±—Ä–µ–∂–µ–º –ø–æ–∑–∂–µ –Ω–∞ —Å—Ç–∞–¥–∏–∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏\n",
    "        rows_out.append(part)\n",
    "    fixed = pd.concat(rows_out, axis=0, ignore_index=True)\n",
    "    return fixed, dict(added=int(added), missing_queries=int(missing_q), total_queries=int(len(all_queries)))\n",
    "\n",
    "def stable_sort_and_rank(df: pd.DataFrame, K: int) -> pd.DataFrame:\n",
    "    qcol, icol = STATE[\"QUERY_COL\"], STATE[\"ITEM_COL\"]\n",
    "    # —Å—Ç–∞–±–∏–ª—å–Ω—ã–π —Ç–∞–π-–±—Ä–µ–π–∫: (-score, item_id)\n",
    "    df = df.copy()\n",
    "    df[icol] = df[icol].astype(str)\n",
    "    df = df.sort_values([qcol, \"score\", icol], ascending=[True, False, True])\n",
    "    df[STATE[\"RANK_COL\"]] = df.groupby(qcol)[\"score\"].rank(ascending=False, method=\"first\").astype(\"int32\")\n",
    "    df = df[df[STATE[\"RANK_COL\"]]<=K]\n",
    "    return df[[qcol, icol, \"score\", STATE[\"RANK_COL\"]]].reset_index(drop=True)\n",
    "\n",
    "def on_fix(_):\n",
    "    with out_fix:\n",
    "        clear_output()\n",
    "        if DATA.get(\"ranked\") is None:\n",
    "            print(\"–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∞–Ω—Å–∞–º–±–ª—å.\")\n",
    "            return\n",
    "        df = DATA[\"ranked\"].copy()\n",
    "        qcol, icol = STATE[\"QUERY_COL\"], STATE[\"ITEM_COL\"]\n",
    "        # –±–∞–∑–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–∏–ø–æ–≤\n",
    "        df[qcol] = df[qcol].astype(str); df[icol] = df[icol].astype(str)\n",
    "        # –ø–æ–∫—Ä—ã—Ç–∏–µ –∏ –¥–æ—É–∫–æ–º–ø–ª–µ–∫—Ç–æ–≤–∞–Ω–∏–µ\n",
    "        fixed, rep = ensure_k_per_query(df, DATA[\"test_queries\"], STATE[\"K_OUT\"])\n",
    "        # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞/—Ä–∞–Ω–≥–∏\n",
    "        fixed = stable_sort_and_rank(fixed, STATE[\"K_OUT\"])\n",
    "        # sanity\n",
    "        byq = fixed.groupby(qcol).size()\n",
    "        ok = int((byq==STATE[\"K_OUT\"]).mean()*100)\n",
    "        dup_in_query = (fixed.duplicated(subset=[qcol, icol]).sum())\n",
    "        FIXED.update(df=fixed, report=dict(coverage_ok=ok, added=rep[\"added\"], missing_queries=rep[\"missing_queries\"], total_queries=rep[\"total_queries\"], dup_pairs=int(dup_in_query)))\n",
    "        print(\"coverage_ok%:\", ok, \"| added:\", rep[\"added\"], \"| missing_queries:\", rep[\"missing_queries\"], \"| total_queries:\", rep[\"total_queries\"], \"| dup_pairs:\", dup_in_query)\n",
    "        display(fixed.head(5))\n",
    "\n",
    "btn_fix.on_click(on_fix)\n",
    "W.VBox([btn_fix, out_fix])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af2ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_fmt = W.Button(description=\"–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª —Å–∞–±–º–∏—Ç–∞ –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ\", button_style=\"info\", layout=BTN_LAYOUT)\n",
    "out_fmt = W.Output()\n",
    "\n",
    "SUBMIT = dict(df=None, path=None, format=None)\n",
    "\n",
    "def to_pairs_csv(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df[[STATE[\"QUERY_COL\"], STATE[\"ITEM_COL\"], STATE[\"RANK_COL\"]]].copy()\n",
    "\n",
    "def to_space_separated(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    qcol, icol, rcol = STATE[\"QUERY_COL\"], STATE[\"ITEM_COL\"], STATE[\"RANK_COL\"]\n",
    "    # –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –ø–æ—Ä—è–¥–æ–∫: –ø–æ —Ä–∞–Ω–≥—É\n",
    "    d = df.sort_values([qcol, rcol])\n",
    "    agg = d.groupby(qcol)[icol].apply(lambda s: \" \".join(map(str, s.tolist()))).reset_index()\n",
    "    agg.columns = [qcol, \"items\"]\n",
    "    return agg\n",
    "\n",
    "def write_jsonl(df: pd.DataFrame, path: Path):\n",
    "    qcol, icol, rcol = STATE[\"QUERY_COL\"], STATE[\"ITEM_COL\"], STATE[\"RANK_COL\"]\n",
    "    d = df.sort_values([qcol, rcol])\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        cur_q = None; bucket=[]\n",
    "        for row in d.itertuples(index=False):\n",
    "            q = getattr(row, qcol); i = getattr(row, icol); \n",
    "            if cur_q is None: cur_q = q\n",
    "            if q != cur_q:\n",
    "                f.write(json.dumps({\"query_id\": str(cur_q), \"items\": bucket}, ensure_ascii=False) + \"\n",
    "\")\n",
    "                cur_q = q; bucket=[]\n",
    "            bucket.append(str(i))\n",
    "        if cur_q is not None:\n",
    "            f.write(json.dumps({\"query_id\": str(cur_q), \"items\": bucket}, ensure_ascii=False) + \"\n",
    "\")\n",
    "\n",
    "def apply_custom_schema(df: pd.DataFrame, schema_path: Path) -> Tuple[pd.DataFrame, Optional[str]]:\n",
    "    # –û–∂–∏–¥–∞–µ–º JSON –≤–∏–¥–∞: {\"type\":\"pairs_csv\"|\"space_separated\"|\"jsonl\",\"fields\":{\"query\":\"query_id\",\"item\":\"item_id\",\"rank\":\"rank\",\"pred_str\":\"PredictionString\"}}\n",
    "    spec = json.loads(schema_path.read_text())\n",
    "    t = spec.get(\"type\",\"pairs_csv\")\n",
    "    fields = spec.get(\"fields\",{})\n",
    "    if t==\"pairs_csv\":\n",
    "        d = df[[STATE[\"QUERY_COL\"], STATE[\"ITEM_COL\"], STATE[\"RANK_COL\"]]].copy()\n",
    "        d.columns = [fields.get(\"query\",\"query_id\"), fields.get(\"item\",\"item_id\"), fields.get(\"rank\",\"rank\")]\n",
    "        return d, \"csv\"\n",
    "    if t==\"space_separated\":\n",
    "        agg = to_space_separated(df)\n",
    "        agg.columns = [fields.get(\"query\",\"query_id\"), fields.get(\"pred_str\",\"PredictionString\")]\n",
    "        return agg, \"csv\"\n",
    "    if t==\"jsonl\":\n",
    "        # –±—É–¥–µ–º —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ –≤ write_jsonl\n",
    "        return df.copy(), \"jsonl\"\n",
    "    return df.copy(), \"csv\"\n",
    "\n",
    "def on_fmt(_):\n",
    "    with out_fmt:\n",
    "        clear_output()\n",
    "        if FIXED.get(\"df\") is None:\n",
    "            print(\"–°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ/–∏—Å–ø—Ä–∞–≤—å—Ç–µ –ø–æ–∫—Ä—ã—Ç–∏–µ.\")\n",
    "            return\n",
    "        df = FIXED[\"df\"].copy()\n",
    "        SUBMIT[\"format\"] = STATE[\"SUBMIT_FORMAT\"]\n",
    "        SUBMIT[\"df\"] = None; SUBMIT[\"path\"]=None\n",
    "        if STATE[\"SUBMIT_FORMAT\"]==\"pairs_csv\":\n",
    "            SUBMIT[\"df\"]=to_pairs_csv(df)\n",
    "            display(SUBMIT[\"df\"].head(3))\n",
    "        elif STATE[\"SUBMIT_FORMAT\"]==\"space_separated\":\n",
    "            SUBMIT[\"df\"]=to_space_separated(df)\n",
    "            display(SUBMIT[\"df\"].head(3))\n",
    "        elif STATE[\"SUBMIT_FORMAT\"]==\"jsonl\":\n",
    "            # –ø–æ–∫–∞–∂–µ–º –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –ø–µ—Ä–≤—ã—Ö 2—Ö —Å—Ç—Ä–æ–∫\n",
    "            tmp = to_space_separated(df).head(2)\n",
    "            display(tmp)\n",
    "        elif STATE[\"SUBMIT_FORMAT\"]==\"custom\":\n",
    "            schema_path = Path(STATE[\"CUSTOM_SCHEMA_PATH\"])\n",
    "            if not schema_path.exists():\n",
    "                print(\"custom schema –Ω–µ –Ω–∞–π–¥–µ–Ω:\", schema_path); return\n",
    "            out_df, kind = apply_custom_schema(df, schema_path)\n",
    "            SUBMIT[\"df\"]=out_df; SUBMIT[\"format\"]=(\"jsonl\" if kind==\"jsonl\" else \"pairs_csv\")\n",
    "            display(out_df.head(3))\n",
    "        else:\n",
    "            print(\"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç:\", STATE[\"SUBMIT_FORMAT\"])\n",
    "\n",
    "btn_fmt.on_click(on_fmt)\n",
    "W.VBox([btn_fmt, out_fmt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a10b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_check = W.Button(description=\"–ñ—ë—Å—Ç–∫–∏–µ —Å–∞–Ω–∏—Ç–∏-—á–µ–∫–∏\", button_style=\"danger\", layout=BTN_LAYOUT)\n",
    "out_check = W.Output()\n",
    "\n",
    "def hard_checks(df: pd.DataFrame):\n",
    "    qcol, icol, rcol = STATE[\"QUERY_COL\"], STATE[\"ITEM_COL\"], STATE[\"RANK_COL\"]\n",
    "    issues = []\n",
    "    # –¥—É–±–ª–∏–∫–∞—Ç—ã –≤–Ω—É—Ç—Ä–∏ –∑–∞–ø—Ä–æ—Å–∞\n",
    "    dups = df.duplicated(subset=[qcol, icol]).sum()\n",
    "    if dups>0: issues.append(f\"–î—É–±–ª–∏–∫–∞—Ç—ã (q,i): {dups}\")\n",
    "    # –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å —Ä–∞–Ω–≥–æ–≤\n",
    "    if rcol in df.columns:\n",
    "        wrong = df.groupby(qcol)[rcol].apply(lambda s: (s.sort_values().values != np.arange(1, len(s)+1)).any()).sum()\n",
    "        if wrong>0: issues.append(f\"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å/–ø—Ä–æ–±–µ–ª—ã –≤ —Ä–∞–Ω–≥–∞—Ö —É {wrong} –∑–∞–ø—Ä–æ—Å–æ–≤\")\n",
    "    # NaN\n",
    "    nan_any = df[[qcol]].isna().sum()[0] + (df[[icol]].isna().sum()[0] if icol in df.columns else 0)\n",
    "    if nan_any>0: issues.append(f\"NaN –≤ ID: {nan_any}\")\n",
    "    # —Ç–∏–ø—ã\n",
    "    if df[qcol].dtype!=object:\n",
    "        issues.append(\"QUERY_COL –Ω–µ string ‚Äî –ª—É—á—à–µ –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ str\")\n",
    "    return issues\n",
    "\n",
    "def on_check(_):\n",
    "    with out_check:\n",
    "        clear_output()\n",
    "        if SUBMIT.get(\"df\") is None and STATE[\"SUBMIT_FORMAT\"]!=\"jsonl\":\n",
    "            print(\"–°–Ω–∞—á–∞–ª–∞ —Å—Ñ–æ—Ä–º–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª —Å–∞–±–º–∏—Ç–∞.\")\n",
    "            return\n",
    "        df = SUBMIT[\"df\"] if STATE[\"SUBMIT_FORMAT\"]!=\"jsonl\" else FIXED[\"df\"]\n",
    "        issues = hard_checks(df)\n",
    "        if issues and STATE[\"STRICT_SCHEMA\"]:\n",
    "            print(\"–û—à–∏–±–∫–∏ —Å—Ö–µ–º—ã:\")\n",
    "            for s in issues: print(\" ‚Ä¢\", s)\n",
    "            raise AssertionError(\"STRICT_SCHEMA: true ‚Äî –∏—Å–ø—Ä–∞–≤—å—Ç–µ –æ—à–∏–±–∫–∏ –∏ –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ.\")\n",
    "        else:\n",
    "            print(\"Sanity OK (–∏–ª–∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –Ω–∏–∂–µ –ø—Ä–∏ STRICT=false):\")\n",
    "            for s in issues: print(\"warn:\", s)\n",
    "\n",
    "btn_check.on_click(on_check)\n",
    "W.VBox([btn_check, out_check])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a3b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_cmp = W.Button(description=\"–°—Ä–∞–≤–Ω–∏—Ç—å —Å –ø—Ä–æ—à–ª—ã–º–∏ —Å–∞–±–º–∏—Ç–∞–º–∏\", button_style=\"\", layout=BTN_LAYOUT)\n",
    "out_cmp = W.Output()\n",
    "\n",
    "def load_last_submits(A, max_n=5) -> List[Path]:\n",
    "    d = A[\"submits\"]\n",
    "    if not d.exists(): return []\n",
    "    runs = sorted([p for p in d.iterdir() if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return runs[:max_n]\n",
    "\n",
    "def parse_submit_generic(path: Path, format_hint: Optional[str]=None) -> pd.DataFrame:\n",
    "    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π DF: query_id, item_id, rank\n",
    "    qcol, icol, rcol = STATE[\"QUERY_COL\"], STATE[\"ITEM_COL\"], STATE[\"RANK_COL\"]\n",
    "    # –∏—â–µ–º —Ñ–∞–π–ª –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É\n",
    "    cand = None\n",
    "    for fn in [\"submission.csv\",\"submission.txt\",\"submission.jsonl\"]:\n",
    "        if (path/fn).exists(): cand = path/fn; break\n",
    "    if cand is None:\n",
    "        # –ø–æ–ø—Ä–æ–±—É–µ–º –ª—é–±–æ–π csv\n",
    "        cands = list(path.glob(\"*.csv\")) + list(path.glob(\"*.txt\")) + list(path.glob(\"*.jsonl\"))\n",
    "        if not cands: return pd.DataFrame(columns=[qcol, icol, rcol])\n",
    "        cand = cands[0]\n",
    "    if cand.suffix in [\".csv\",\".txt\"]:\n",
    "        df = pd.read_csv(cand)\n",
    "        cols = [c.lower() for c in df.columns]\n",
    "        if rcol in df.columns or \"rank\" in cols:\n",
    "            # –ø–∞—Ä—ã\n",
    "            # –ø–æ–ø—ã—Ç–∞–µ–º—Å—è –ø—Ä–∏–≤–µ—Å—Ç–∏ –∏–º–µ–Ω–∞\n",
    "            df = df.rename(columns={list(df.columns)[cols.index(\"rank\")]: rcol} if \"rank\" in cols else {})\n",
    "            # map query/item\n",
    "            if qcol not in df.columns:\n",
    "                for c in df.columns:\n",
    "                    if \"query\" in c.lower(): df = df.rename(columns={c: qcol}); break\n",
    "            if icol not in df.columns:\n",
    "                for c in df.columns:\n",
    "                    if \"item\" in c.lower() or \"product\" in c.lower(): df = df.rename(columns={c: icol}); break\n",
    "            return df[[qcol, icol, rcol]].astype({qcol:str, icol:str})\n",
    "        else:\n",
    "            # –≤–µ—Ä–æ—è—Ç–Ω–æ space-separated\n",
    "            qc = qcol if qcol in df.columns else list(df.columns)[0]\n",
    "            pc = \"PredictionString\" if \"predictionstring\" in cols else list(df.columns)[1]\n",
    "            out_rows=[]\n",
    "            for r in df.itertuples(index=False):\n",
    "                q = str(getattr(r, qc)); items = str(getattr(r, pc)).split()\n",
    "                for k,it in enumerate(items, start=1):\n",
    "                    out_rows.append((q, str(it), k))\n",
    "            return pd.DataFrame(out_rows, columns=[qcol, icol, rcol])\n",
    "    elif cand.suffix==\".jsonl\":\n",
    "        rows=[]\n",
    "        with open(cand, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                obj = json.loads(line)\n",
    "                q = str(obj.get(\"query_id\") or obj.get(\"query\") or obj.get(qcol))\n",
    "                items = obj.get(\"items\") or obj.get(\"predictions\") or []\n",
    "                for k,it in enumerate(items, start=1):\n",
    "                    rows.append((q, str(it), k))\n",
    "        return pd.DataFrame(rows, columns=[qcol, icol, rcol])\n",
    "    return pd.DataFrame(columns=[qcol, icol, rcol])\n",
    "\n",
    "def jaccard_at_k(df_a, df_b, K: int) -> float:\n",
    "    qcol, icol, rcol = STATE[\"QUERY_COL\"], STATE[\"ITEM_COL\"], STATE[\"RANK_COL\"]\n",
    "    a = df_a[df_a[rcol]<=K].groupby(qcol)[icol].apply(lambda s: set(s.values))\n",
    "    b = df_b[df_b[rcol]<=K].groupby(qcol)[icol].apply(lambda s: set(s.values))\n",
    "    qs = sorted(set(a.index) & set(b.index))\n",
    "    if not qs: return float(\"nan\")\n",
    "    vals=[]\n",
    "    for q in qs:\n",
    "        u = a[q] | b[q]; i = a[q] & b[q]\n",
    "        vals.append(len(i)/max(1,len(u)))\n",
    "    return float(np.mean(vals))\n",
    "\n",
    "def on_cmp(_):\n",
    "    with out_cmp:\n",
    "        clear_output()\n",
    "        A = art_paths(STATE[\"DATASET_ID\"], STATE[\"PROFILE_PATH\"])\n",
    "        prev = load_last_submits(A, max_n=5)\n",
    "        if not prev:\n",
    "            print(\"–ü—Ä–æ—à–ª—ã—Ö —Å–∞–±–º–∏—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.\")\n",
    "            return\n",
    "        # —Ç–µ–∫—É—â–∏–π –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–æ–≤–∞–Ω–Ω–æ–º –≤–∏–¥–µ\n",
    "        cur = FIXED[\"df\"].copy()\n",
    "        qcol, icol, rcol = STATE[\"QUERY_COL\"], STATE[\"ITEM_COL\"], STATE[\"RANK_COL\"]\n",
    "        cur = cur[[qcol, icol, rcol]].astype({qcol:str, icol:str})\n",
    "        rows=[]\n",
    "        for p in prev:\n",
    "            dfp = parse_submit_generic(p)\n",
    "            if len(dfp)==0: \n",
    "                rows.append(dict(submit=p.name, jaccard_atK=np.nan)); continue\n",
    "            jac = jaccard_at_k(cur, dfp, STATE[\"K_OUT\"])\n",
    "            rows.append(dict(submit=p.name, jaccard_atK=jac))\n",
    "        res = pd.DataFrame(rows).sort_values(\"submit\", ascending=False)\n",
    "        display(res)\n",
    "\n",
    "btn_cmp.on_click(on_cmp)\n",
    "W.VBox([btn_cmp, out_cmp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30be8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_save = W.Button(description=\"–°–æ—Ö—Ä–∞–Ω–∏—Ç—å/–ø–æ–¥–ø–∏—Å–∞—Ç—å —Å–∞–±–º–∏—Ç (+zip)\", button_style=\"success\", layout=BTN_LAYOUT)\n",
    "out_save = W.Output()\n",
    "\n",
    "PATHS = dict(submit_dir=None, submit_file=None, zip_file=None, manifest=None, submit_id=None)\n",
    "\n",
    "def on_save(_):\n",
    "    with out_save:\n",
    "        clear_output()\n",
    "        if FIXED.get(\"df\") is None:\n",
    "            print(\"–ù–µ—Ç FIXED.df ‚Äî —Å–Ω–∞—á–∞–ª–∞ —à–∞–≥–∏ –≤—ã—à–µ.\")\n",
    "            return\n",
    "        A = art_paths(STATE[\"DATASET_ID\"], STATE[\"PROFILE_PATH\"])\n",
    "        A[\"submits\"].mkdir(parents=True, exist_ok=True)\n",
    "        ens_dir = Path(DATA[\"ens_dir\"])\n",
    "        ens_id = ens_dir.name\n",
    "\n",
    "        # SUBMIT_ID\n",
    "        fmt = STATE[\"SUBMIT_FORMAT\"]\n",
    "        tag = now_tag()\n",
    "        h = short_hash(f\"{ens_id}_{fmt}_{STATE['K_OUT']}_{tag}\")\n",
    "        submit_id = f\"{tag}_{ens_id}_{fmt}_{STATE['K_OUT']}_{h}\"\n",
    "        out_dir = A[\"submits\"]/submit_id\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å–∞–±–º–∏—Ç–∞\n",
    "        if STATE[\"SUBMIT_FORMAT\"]==\"pairs_csv\":\n",
    "            df = SUBMIT[\"df\"].copy()\n",
    "            file_path = out_dir/\"submission.csv\"\n",
    "            df.to_csv(file_path, index=False)\n",
    "        elif STATE[\"SUBMIT_FORMAT\"]==\"space_separated\":\n",
    "            df = SUBMIT[\"df\"].copy()\n",
    "            file_path = out_dir/\"submission.csv\"\n",
    "            df.to_csv(file_path, index=False)\n",
    "        elif STATE[\"SUBMIT_FORMAT\"]==\"jsonl\":\n",
    "            file_path = out_dir/\"submission.jsonl\"\n",
    "            write_jsonl(FIXED[\"df\"], file_path)\n",
    "        elif STATE[\"SUBMIT_FORMAT\"]==\"custom\":\n",
    "            # custom ‚Üí –º–æ–≥–ª–∏ –ø–æ–ª—É—á–∏—Ç—å df –∏–ª–∏ jsonl\n",
    "            if SUBMIT[\"format\"]==\"jsonl\":\n",
    "                file_path = out_dir/\"submission.jsonl\"\n",
    "                write_jsonl(FIXED[\"df\"], file_path)\n",
    "            else:\n",
    "                df = SUBMIT[\"df\"].copy()\n",
    "                file_path = out_dir/\"submission.csv\"\n",
    "                df.to_csv(file_path, index=False)\n",
    "        else:\n",
    "            raise ValueError(\"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–∞–±–º–∏—Ç–∞\")\n",
    "\n",
    "        # manifest\n",
    "        manifest = dict(\n",
    "            submit_id = submit_id,\n",
    "            dataset_id = STATE[\"DATASET_ID\"],\n",
    "            profile = STATE[\"PROFILE_PATH\"],\n",
    "            ensemble_id = ens_id,\n",
    "            format = STATE[\"SUBMIT_FORMAT\"],\n",
    "            K_OUT = STATE[\"K_OUT\"],\n",
    "            file = str(file_path),\n",
    "            file_fp = fingerprint(file_path),\n",
    "            blender_config = DATA[\"blender_config\"],\n",
    "            created_utc = datetime.utcnow().isoformat()+\"Z\"\n",
    "        )\n",
    "        save_json(out_dir/\"manifest.json\", manifest)\n",
    "\n",
    "        # readme\n",
    "        (out_dir/\"readme.txt\").write_text(\n",
    " f\"\"\"SUBMIT_ID: {submit_id}\n",
    "Ensemble:  {ens_id}\n",
    "Format:    {STATE['SUBMIT_FORMAT']}\n",
    "K_OUT:     {STATE['K_OUT']}\n",
    "\n",
    "How to reproduce:\n",
    "  1) Run 04_blend_eval to produce {ens_id}\n",
    "  2) Run 05_submit with the same dataset/profile/K_OUT and ENS={ens_id}\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "        zpath = None\n",
    "        if STATE[\"COMPRESS_ZIP\"]:\n",
    "            zpath = out_dir/\"submission.zip\"\n",
    "            with zipfile.ZipFile(zpath, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "                zf.write(file_path, arcname=file_path.name)\n",
    "                zf.write(out_dir/\"manifest.json\", arcname=\"manifest.json\")\n",
    "\n",
    "        PATHS.update(submit_dir=str(out_dir), submit_file=str(file_path), zip_file=(None if zpath is None else str(zpath)), manifest=str(out_dir/\"manifest.json\"), submit_id=submit_id)\n",
    "        print(\"‚úì saved:\", file_path)\n",
    "        if zpath: print(\"‚úì zipped:\", zpath)\n",
    "\n",
    "btn_save.on_click(on_save)\n",
    "W.VBox([btn_save, out_save])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b05806",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_kaggle = W.Button(description=\"(–û–ø—Ü.) Kaggle submit —á–µ—Ä–µ–∑ CLI\", button_style=\"\", layout=BTN_LAYOUT)\n",
    "out_kaggle = W.Output()\n",
    "\n",
    "def on_kaggle(_):\n",
    "    with out_kaggle:\n",
    "        clear_output()\n",
    "        if not PATHS.get(\"submit_file\"):\n",
    "            print(\"–ù–µ—Ç —Ñ–∞–π–ª–∞ —Å–∞–±–º–∏—Ç–∞ ‚Äî —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞.\")\n",
    "            return\n",
    "        if not STATE[\"KAGGLE_COMP\"]:\n",
    "            print(\"KAGGLE_COMP –ø—É—Å—Ç. –£–∫–∞–∂–∏—Ç–µ id —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'titanic').\")\n",
    "            return\n",
    "        file_to_send = Path(PATHS[\"submit_file\"])\n",
    "        comp = STATE[\"KAGGLE_COMP\"]\n",
    "        msg = PATHS[\"submit_id\"]\n",
    "        print(\"–ö–æ–º–∞–Ω–¥–∞ (–≤—ã–ø–æ–ª–Ω–∏—Ç–µ –≤—Ä—É—á–Ω—É—é, –µ—Å–ª–∏ CLI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏):\")\n",
    "        print(f\"!kaggle competitions submit -c {comp} -f {file_to_send} -m \"{msg}\"\")\n",
    "\n",
    "btn_kaggle.on_click(on_kaggle)\n",
    "W.VBox([btn_kaggle, out_kaggle])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c78af",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_panic = W.Button(description=\"‚ö° PANIC: –±—ã—Å—Ç—Ä–æ —É–ø–∞–∫–æ–≤–∞—Ç—å pairs_csv –∏–∑ ranked\", button_style=\"danger\", layout=BTN_LAYOUT)\n",
    "out_panic = W.Output()\n",
    "\n",
    "def on_panic(_):\n",
    "    with out_panic:\n",
    "        clear_output()\n",
    "        if DATA.get(\"ranked\") is None:\n",
    "            print(\"–ù–µ—Ç ranked ‚Äî –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∞–Ω—Å–∞–º–±–ª—å.\")\n",
    "            return\n",
    "        # –ü–µ—Ä–µ—Å–æ–±–µ—Ä—ë–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –Ω–∞–ø—Ä—è–º—É—é –∏–∑ ranked (–±–µ–∑ –¥–æ—É–∫–æ–º–ø–ª–µ–∫—Ç–æ–≤–∞–Ω–∏—è, —Ç–æ–ª—å–∫–æ —Å–æ—Ä—Ç/—Ä–∞–Ω–≥–∏)\n",
    "        df = DATA[\"ranked\"].copy()\n",
    "        df[STATE[\"QUERY_COL\"]] = df[STATE[\"QUERY_COL\"]].astype(str)\n",
    "        df[STATE[\"ITEM_COL\"]]  = df[STATE[\"ITEM_COL\"]].astype(str)\n",
    "        df = df.sort_values([STATE[\"QUERY_COL\"], \"score\", STATE[\"ITEM_COL\"]], ascending=[True, False, True])\n",
    "        df[STATE[\"RANK_COL\"]] = df.groupby(STATE[\"QUERY_COL\"])[\"score\"].rank(ascending=False, method=\"first\").astype(\"int32\")\n",
    "        df = df[df[STATE[\"RANK_COL\"]]<=STATE[\"K_OUT\"]][[STATE[\"QUERY_COL\"], STATE[\"ITEM_COL\"], STATE[\"RANK_COL\"]]].reset_index(drop=True)\n",
    "\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–ø—Ä—è–º—É—é\n",
    "        A = art_paths(STATE[\"DATASET_ID\"], STATE[\"PROFILE_PATH\"])\n",
    "        ens_id = Path(DATA[\"ens_dir\"]).name\n",
    "        tag = now_tag()\n",
    "        h = short_hash(f\"panic_{ens_id}_{STATE['K_OUT']}_{tag}\")\n",
    "        submit_id = f\"{tag}_{ens_id}_panic_{STATE['K_OUT']}_{h}\"\n",
    "        out_dir = A[\"submits\"]/submit_id\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        file_path = out_dir/\"submission.csv\"\n",
    "        df.to_csv(file_path, index=False)\n",
    "        save_json(out_dir/\"manifest.json\", dict(\n",
    "            submit_id=submit_id, panic=True, dataset_id=STATE[\"DATASET_ID\"], profile=STATE[\"PROFILE_PATH\"],\n",
    "            ensemble_id=ens_id, format=\"pairs_csv\", K_OUT=STATE[\"K_OUT\"], file=str(file_path), file_fp=fingerprint(file_path),\n",
    "            created_utc=datetime.utcnow().isoformat()+\"Z\"\n",
    "        ))\n",
    "        if STATE[\"COMPRESS_ZIP\"]:\n",
    "            zpath = out_dir/\"submission.zip\"\n",
    "            with zipfile.ZipFile(zpath, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "                zf.write(file_path, arcname=file_path.name)\n",
    "                zf.write(out_dir/\"manifest.json\", arcname=\"manifest.json\")\n",
    "            print(\"‚úì PANIC saved & zipped:\", zpath)\n",
    "        else:\n",
    "            print(\"‚úì PANIC saved:\", file_path)\n",
    "\n",
    "btn_panic.on_click(on_panic)\n",
    "W.VBox([btn_panic, out_panic])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f161b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_report = W.Button(description=\"–°–æ–±—Ä–∞—Ç—å –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç —Å–∞–±–º–∏—Ç–∞\", button_style=\"\", layout=BTN_LAYOUT)\n",
    "out_report = W.Output()\n",
    "\n",
    "def on_report(_):\n",
    "    with out_report:\n",
    "        clear_output()\n",
    "        if not PATHS.get(\"submit_dir\"):\n",
    "            print(\"–°–Ω–∞—á–∞–ª–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ —Å–∞–±–º–∏—Ç.\")\n",
    "            return\n",
    "        ens_dir = Path(DATA[\"ens_dir\"])\n",
    "        submit_dir = Path(PATHS[\"submit_dir\"])\n",
    "        report = dict(\n",
    "            submit_id = PATHS[\"submit_id\"],\n",
    "            dataset_id= STATE[\"DATASET_ID\"],\n",
    "            profile   = STATE[\"PROFILE_PATH\"],\n",
    "            ensemble_id = ens_dir.name,\n",
    "            files = dict(submit=PATHS[\"submit_file\"], zip=PATHS.get(\"zip_file\"), manifest=PATHS[\"manifest\"]),\n",
    "            blender_config = DATA[\"blender_config\"],\n",
    "            fixed_stats = FIXED.get(\"report\"),\n",
    "            created_utc = datetime.utcnow().isoformat()+\"Z\"\n",
    "        )\n",
    "        save_json(submit_dir/\"submission_report.json\", report)\n",
    "        print(\"‚úì submission_report.json –∑–∞–ø–∏—Å–∞–Ω:\", submit_dir/\"submission_report.json\")\n",
    "        display(pd.DataFrame([report]).T)\n",
    "\n",
    "btn_report.on_click(on_report)\n",
    "W.VBox([btn_report, out_report])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9db962",
   "metadata": {},
   "source": [
    "### –ü–∞–º—è—Ç–∫–∞\n",
    "- –£–±–µ–¥–∏—Å—å, —á—Ç–æ –¥–ª—è **–∫–∞–∂–¥–æ–≥–æ** –∑–∞–ø—Ä–æ—Å–∞ —Ä–æ–≤–Ω–æ `K_OUT` –ø–æ–∑–∏—Ü–∏–π, –±–µ–∑ –¥—É–±–ª–µ–π.\n",
    "- –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Å—Ç—Ä–æ–∫–æ–≤—ã–µ (–∏–ª–∏ –∫–∞–∫ —Ç—Ä–µ–±—É–µ—Ç –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞), –Ω–µ—Ç `NaN/Inf`.\n",
    "- –§–æ—Ä–º–∞—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ä–µ–≥–ª–∞–º–µ–Ω—Ç—É (–∫–æ–ª–æ–Ω–∫–∏/–ø–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–∞–∑–≤–∞–Ω—ã).\n",
    "\n",
    "**–°—Ç–æ–ø-—Ñ–ª–∞–≥–∏**\n",
    "- –ù–µ –≤—Å–µ queries –ø–æ–∫—Ä—ã—Ç—ã.\n",
    "- –î—É–±–ª–∏–∫–∞—Ç—ã `(query_id, item_id)` –∏–ª–∏ –¥—É–±–ª–∏ `item_id` –≤–Ω—É—Ç—Ä–∏ query.\n",
    "- –ù–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–µ —Ç–∞–π-–±—Ä–µ–π–∫–∏ (–ø–ª–∞–≤–∞—é—â–∏–π –ø–æ—Ä—è–¥–æ–∫ –ø—Ä–∏ —Ä–∞–≤–Ω—ã—Ö —Å–∫–æ—Ä–∏–Ω–≥–∞—Ö).\n",
    "\n",
    "–ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –≥–æ—Ä–∏—Ç ‚Äî –∂–º–∏ **PANIC**: –±—ã—Å—Ç—Ä—ã–π pairs_csv –∏–∑ —É–∂–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ `ranked`.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
