{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "780e88a7",
   "metadata": {},
   "source": [
    "### Параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PARAMS ===\n",
    "DATA_PATH = \"/path/to/data\"\n",
    "TRAIN_FILE = \"train.parquet\"   # или .csv\n",
    "TEST_FILE  = \"test.parquet\"\n",
    "\n",
    "ID_COL      = \"id\"\n",
    "TARGET_COL  = \"target\"         # для классификации/регрессии; если нет — None\n",
    "DATE_COL    = None             # например \"date\" если есть время\n",
    "NUM_COLS    = None             # список или None (авто-детект)\n",
    "CAT_COLS    = None             # список или None (авто-детект)\n",
    "MULTI_COLS  = []               # кат-колонки со списками/множествами (строка с разделителем)\n",
    "LAT_COL = None        # например: \"lat\"\n",
    "LON_COL = None        # например: \"lon\"\n",
    "GEO_STEPS_M = [300, 1000]   # размеры «гридов» в метрах для агрегаций\n",
    "NEIGHBOR_RADII_M = [300, 1000]  # радиусы соседей для плотностей\n",
    "\n",
    "FAST = False                   # True => сэмплинг для быстрого прогона\n",
    "SAMPLE_SIZE = 200_000          # если FAST=True, ограничение строк\n",
    "\n",
    "# Настройки визуализации (опционально)\n",
    "MAX_CATEG_UNIQUE_PREVIEW = 30\n",
    "RARE_THRESHOLD = 0.01          # доля для пометки редких категорий\n",
    "NEAR_CONST_STD_THRESH = 1e-6   # почти константные числовые"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa995ea",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62247c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(DATA_PATH)\n",
    "def read_any(p):\n",
    "    return pd.read_parquet(p) if p.suffix==\".parquet\" else pd.read_csv(p)\n",
    "\n",
    "train = read_any(path/TRAIN_FILE)\n",
    "test  = read_any(path/TEST_FILE)\n",
    "\n",
    "if FAST:\n",
    "    train = train.sample(min(len(train), SAMPLE_SIZE), random_state=42)\n",
    "\n",
    "# первичный обзор\n",
    "display(train.head(3)); display(test.head(3))\n",
    "print(\"train:\", train.shape, \"test:\", test.shape)\n",
    "print(\"mem train MB:\", train.memory_usage(deep=True).sum()/1e6)\n",
    "print(\"mem test  MB:\", test.memory_usage(deep=True).sum()/1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041087de",
   "metadata": {},
   "source": [
    "Смотреть: размеры, соответствие колонок, пример строк.\n",
    "Реакция: если колонки не совпадают — зафиксировать список общих признаков; если память большая — сразу думать про downcast/categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53e7d47",
   "metadata": {},
   "source": [
    "# СХЕМА И ЧИСТОТА"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48e46ed",
   "metadata": {},
   "source": [
    "### Выявление типов, списки NUM/CAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be5bdfd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "all_cols = [c for c in train.columns if c != TARGET_COL]\n",
    "if NUM_COLS is None:\n",
    "    NUM_COLS = [c for c in all_cols if pd.api.types.is_numeric_dtype(train[c])]\n",
    "if CAT_COLS is None:\n",
    "    CAT_COLS = [c for c in all_cols if c not in NUM_COLS and c != DATE_COL]\n",
    "\n",
    "print(\"ID:\", ID_COL, \"TARGET:\", TARGET_COL, \"DATE:\", DATE_COL)\n",
    "print(\"NUM:\", len(NUM_COLS), \"CAT:\", len(CAT_COLS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc2bf2b",
   "metadata": {},
   "source": [
    "Смотреть: разумность списков, нет ли утечки (например, hash-колонка в NUM, которая по сути id).\n",
    "Реакция: вынести технические id/ключи из фичей; явные даты — в DATE_COL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e06a51d",
   "metadata": {},
   "source": [
    "### Пропуски, dtypes, константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557db1a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# dtypes и пропуски\n",
    "schema = pd.DataFrame({\n",
    "    \"dtype\": train.dtypes.astype(str),\n",
    "    \"n_missing\": train.isna().sum(),\n",
    "    \"missing_ratio\": train.isna().mean()\n",
    "}).sort_values(\"missing_ratio\", ascending=False)\n",
    "display(schema.head(20))\n",
    "\n",
    "# почти константные числовые\n",
    "near_const = []\n",
    "for c in NUM_COLS:\n",
    "    s = train[c].dropna()\n",
    "    if s.size and s.std() < NEAR_CONST_STD_THRESH:\n",
    "        near_const.append(c)\n",
    "print(\"Near-constant numeric:\", near_const[:20])\n",
    "\n",
    "# полностью константные\n",
    "const_cols = [c for c in all_cols if train[c].nunique(dropna=False)<=1]\n",
    "print(\"Constant cols:\", const_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b8b940",
   "metadata": {},
   "source": [
    "Смотреть: столбцы с высокой долей NaN, неподходящие dtypes (object вместо категорий), константы.\n",
    "Реакция: наметить стратегию: дроп констант; для NaN >30–40% — либо продуманная импутация/таргет-aware, либо drop; object→category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e5444a",
   "metadata": {},
   "source": [
    "### Уникальность ID, дубликаты\n",
    "Проверка уникальности ключа и дублей строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d4e2a2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if ID_COL in train:\n",
    "    print(\"ID unique train:\", train[ID_COL].is_unique)\n",
    "if ID_COL in test:\n",
    "    print(\"ID unique test:\",  test[ID_COL].is_unique)\n",
    "\n",
    "# дубликаты по всем колонкам (кроме TARGET)\n",
    "dups = train.drop(columns=[TARGET_COL] if TARGET_COL in train else []).duplicated(keep=False).sum()\n",
    "print(\"duplicate rows (w/o target):\", dups)\n",
    "\n",
    "# пересечения train/test по ID\n",
    "if ID_COL in train and ID_COL in test:\n",
    "    inter = np.intersect1d(train[ID_COL].values, test[ID_COL].values)\n",
    "    print(\"ID overlap train/test:\", len(inter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f5a74a",
   "metadata": {},
   "source": [
    "Смотреть: неуникальные id, дубликаты, пересечения train/test.\n",
    "Реакция: исправить ключ или собрать композитный; удалить дубли или агрегировать; пересечения train/test — флаг потенциальной утечки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42fb809",
   "metadata": {},
   "source": [
    "### Аномалии числовых (ноль/отрицательные/выбросы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50358644",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "num_profile = []\n",
    "for c in NUM_COLS:\n",
    "    s = pd.to_numeric(train[c], errors=\"coerce\")\n",
    "    q = s.quantile([.01,.05,.5,.95,.99])\n",
    "    num_profile.append(dict(\n",
    "        col=c, n_null=s.isna().sum(), zero=(s==0).sum(),\n",
    "        neg=(s<0).sum(), p01=q.iloc[0], p05=q.iloc[1], p50=q.iloc[2],\n",
    "        p95=q.iloc[3], p99=q.iloc[4], skew=s.skew()\n",
    "    ))\n",
    "pd.DataFrame(num_profile).sort_values(\"p99\", ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f47dd7",
   "metadata": {},
   "source": [
    "Смотреть: отрицательные там, где быть не должно; сильная асимметрия, хвосты.\n",
    "Реакция: лог-преобразование (log1p), клиппинг по перцентилям (1–99%), биннинг."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e6bf5",
   "metadata": {},
   "source": [
    "### Дата/время (если есть)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f70634",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if DATE_COL:\n",
    "    ts = pd.to_datetime(train[DATE_COL], errors=\"coerce\")\n",
    "    print(\"min/max:\", ts.min(), ts.max())\n",
    "    by_day = ts.dt.to_period(\"D\").value_counts().sort_index()\n",
    "    display(by_day.tail(10).to_frame(\"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0dc2d6",
   "metadata": {},
   "source": [
    "Смотреть: «дыры», сдвиги, хвост train vs test.\n",
    "Реакция: планировать TimeSplit, эмбарго, временные фичи (сезон/неделя/скользящие)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb24386",
   "metadata": {},
   "source": [
    "# ЦЕЛЕВАЯ ПЕРЕМЕННАЯ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee994f",
   "metadata": {},
   "source": [
    "### Базовая статистика таргета\n",
    "\n",
    "Распределение, выбросы, лог-масштаб (для регрессии); баланс классов (для классификации)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162e32a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if TARGET_COL:\n",
    "    y = train[TARGET_COL].dropna()\n",
    "    display(y.describe(percentiles=[.01,.05,.5,.95,.99]))\n",
    "    plt.figure(); y.hist(bins=50); plt.title(\"target\"); plt.show()\n",
    "    # лог-масштаб\n",
    "    if (y>0).all():\n",
    "        plt.figure(); np.log1p(y).hist(bins=50); plt.title(\"log1p(target)\"); plt.show()\n",
    "    # бинарный/мультикласс баланс\n",
    "    if y.nunique()<=10 and pd.api.types.is_integer_dtype(y):\n",
    "        display(y.value_counts(normalize=True).to_frame(\"ratio\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9de4aa",
   "metadata": {},
   "source": [
    "Смотреть: скос, хвосты, нули/отрицательные, дисбаланс классов.\n",
    "Реакция: для регрессии — лог-таргет или робаст-лоссы; для сильного дисбаланса — class_weight/стратификация."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d4e06e",
   "metadata": {},
   "source": [
    "### «Таргет во времени» (если DATE_COL)\n",
    "\n",
    "Стабильность/дрейф."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fedfe5b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if TARGET_COL and DATE_COL:\n",
    "    ts = pd.to_datetime(train[DATE_COL], errors=\"coerce\")\n",
    "    df = pd.DataFrame({TARGET_COL: train[TARGET_COL].values, \"dt\": ts})\n",
    "    by_period = df.groupby(df[\"dt\"].dt.to_period(\"W\"))[TARGET_COL].agg([\"count\",\"mean\",\"median\"])\n",
    "    display(by_period.tail(8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e38af3",
   "metadata": {},
   "source": [
    "### Бэйзлайны таргета\n",
    "\n",
    "Простейшие ориентиpы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab354ea2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if TARGET_COL:\n",
    "    if train[TARGET_COL].nunique() > 10:  # регрессия\n",
    "        mae_mean = np.abs(train[TARGET_COL] - train[TARGET_COL].mean()).mean()\n",
    "        print(\"MAE(mean baseline):\", mae_mean)\n",
    "    else:  # классификация\n",
    "        maj = train[TARGET_COL].mode()[0]\n",
    "        acc_maj = (train[TARGET_COL]==maj).mean()\n",
    "        print(\"Majority class:\", maj, \"accuracy:\", acc_maj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b91d8e",
   "metadata": {},
   "source": [
    "Смотреть: насколько «тяжёл» таргет vs совсем тупой бейзлайн.\n",
    "Реакция: понимать целевую дельту, которую надо превзойти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2174de2e",
   "metadata": {},
   "source": [
    "### Подозрения на утечки\n",
    "\n",
    "Быстрая эвристика: «слишком идеальные» корреляции, клоны таргета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a5bc9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if TARGET_COL:\n",
    "    corrs = []\n",
    "    for c in NUM_COLS[:200]:  # ограничим\n",
    "        try:\n",
    "            corrs.append((c, np.corrcoef(train[c].fillna(train[c].median()), train[TARGET_COL])[0,1]))\n",
    "        except Exception:\n",
    "            pass\n",
    "    corr_df = pd.DataFrame(corrs, columns=[\"col\",\"corr\"]).sort_values(\"corr\", ascending=False)\n",
    "    display(corr_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeb8db9",
   "metadata": {},
   "source": [
    "Смотреть: подозрительно высокие |corr|, особенно для «технических» колонок.\n",
    "Реакция: проверить происхождение фичи, возможно исключить/обработать от утечки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1b498d",
   "metadata": {},
   "source": [
    "# КАТЕГОРИАЛЬНЫЕ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c79e67",
   "metadata": {},
   "source": [
    "### Кардинальность и пропуски по категориальным\n",
    "\n",
    "Таблица «сколько уникальных», доля редких."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f7d4bb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cat_stats=[]\n",
    "for c in CAT_COLS:\n",
    "    vc = train[c].astype(\"object\").value_counts(dropna=True)\n",
    "    nuniq = vc.shape[0]\n",
    "    rare_ratio = (vc[vc/train.shape[0] < RARE_THRESHOLD].sum())/train.shape[0] if nuniq else 0\n",
    "    cat_stats.append({\"col\":c,\"nunique\":nuniq,\"missing\":train[c].isna().mean(),\"rare_ratio\":rare_ratio})\n",
    "cat_stats = pd.DataFrame(cat_stats).sort_values(\"nunique\", ascending=False)\n",
    "display(cat_stats.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0ecb03",
   "metadata": {},
   "source": [
    "Смотреть: очень высокая кардинальность, большая доля редких, много NaN.\n",
    "Реакция: high-card → планировать target/freq/hashing/CTR; rare→ объединение в __RARE__; продумать иммпутацию категориальных (в т.ч. явный __NONE__)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f277951",
   "metadata": {},
   "source": [
    "### Частоты топ-значений (просмотр)\n",
    "\n",
    "Короткий превью для первых колонок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d159e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for c in CAT_COLS[:10]:\n",
    "    print(f\"\\n{c} top values:\")\n",
    "    display(train[c].astype(\"object\").value_counts(dropna=True).head(MAX_CATEG_UNIQUE_PREVIEW))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b697bb",
   "metadata": {},
   "source": [
    "Смотреть: «мусорные» значения, разные написания одного и того же, спецсимволы.\n",
    "Реакция: нормализовать (lower/strip), сопоставить с тестом."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cd916f",
   "metadata": {},
   "source": [
    "### Совместные пары/тройки (кроссы)\n",
    "\n",
    "Оценка «силы» комбинаций и потенциальных пересечений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8bb38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "pairs = list(combinations(CAT_COLS[: min(8,len(CAT_COLS))], 2))\n",
    "pair_stats=[]\n",
    "for a,b in pairs:\n",
    "    nun = train[a].astype(\"object\").str.cat(train[b].astype(\"object\"), sep=\"__\").nunique()\n",
    "    pair_stats.append({\"pair\":f\"{a}×{b}\", \"nunique\":nun})\n",
    "display(pd.DataFrame(pair_stats).sort_values(\"nunique\", ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbba98e8",
   "metadata": {},
   "source": [
    "Смотреть: пары с огромной кардинальностью (может «взорвать» OHE), пары с небольшой — кандидаты для явных перекрёстных фич.\n",
    "Реакция: для больших — hashing/TE; для компактных — можно OHE/CTR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7bae50",
   "metadata": {},
   "source": [
    "### «Таргет по категориям» (только осмотр, не для обучения)\n",
    "\n",
    "Грубый намёк на информативность (не использовать агрегаты из этой ячейки в модели — только EDA!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7bff23",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if TARGET_COL and train[TARGET_COL].notna().any():\n",
    "    previews=[]\n",
    "    for c in CAT_COLS[:8]:\n",
    "        g = train.groupby(c, dropna=True)[TARGET_COL].agg([\"count\",\"mean\",\"median\"]).sort_values(\"count\", ascending=False).head(20)\n",
    "        previews.append((c,g))\n",
    "    for c,g in previews:\n",
    "        print(\"\\n\", c); display(g.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c00c4d",
   "metadata": {},
   "source": [
    "Смотреть: категориальные уровни, сильно сдвигающие таргет; редкие, но «яркие» категории.\n",
    "Реакция: кандидаты для TE/WOE (но в пайплайне — строго OOF), объединение редких, контроль утечек."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504a9827",
   "metadata": {},
   "source": [
    "### unseen/only-in-test категории\n",
    "\n",
    "Доля уровней, которых нет в train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa2581",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "unseen_report=[]\n",
    "for c in CAT_COLS:\n",
    "    tr = set(train[c].dropna().astype(\"object\").unique())\n",
    "    te = set(test[c].dropna().astype(\"object\").unique())\n",
    "    unseen = len(te - tr)\n",
    "    unseen_report.append({\"col\":c,\"only_in_test\":unseen,\"ratio_only_in_test\": unseen/max(1,len(te))})\n",
    "display(pd.DataFrame(unseen_report).sort_values(\"only_in_test\", ascending=False).head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b36475",
   "metadata": {},
   "source": [
    "Смотреть: много unseen → риск; особенно для ключевых фич.\n",
    "Реакция: в пайплайне — маппинг __UNK__, freq/hashing/CTR вместо чистого OHE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e395aa",
   "metadata": {},
   "source": [
    "### MULTI_COLS (мультизначные категорики)\n",
    "\n",
    "Быстрые агрегаты по множествам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c00c06",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "multi_stats=[]\n",
    "for c in MULTI_COLS:\n",
    "    # предполагаем строку с разделителем \",\" → правишь под свой формат\n",
    "    lists = train[c].fillna(\"\").astype(str).str.split(\",\")\n",
    "    lens  = lists.map(len)\n",
    "    multi_stats.append({\n",
    "        \"col\": c, \"mean_len\": lens.mean(), \"p95_len\": lens.quantile(.95),\n",
    "        \"empty_ratio\": (lens==0).mean()\n",
    "    })\n",
    "    # энтропия разметки (примерно)\n",
    "    from collections import Counter\n",
    "    cnt = Counter([tok for lst in lists for tok in lst if tok])\n",
    "    tot = sum(cnt.values())\n",
    "    probs = np.array([v/tot for v in cnt.values()])\n",
    "    entropy = -(probs*np.log(probs+1e-12)).sum()\n",
    "    print(c, \"unique tokens:\", len(cnt), \"entropy:\", round(entropy,2))\n",
    "display(pd.DataFrame(multi_stats))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65725e80",
   "metadata": {},
   "source": [
    "Смотреть: длины множеств, пустота, количество токенов — масштаб проблемы.\n",
    "Реакция: multi-hot/частоты/TF-подобные агрегаты, avg rarity, top1 share, OOF-счётчики по токенам."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81790ac",
   "metadata": {},
   "source": [
    "### Память и оптимизация типов\n",
    "\n",
    "Прикинуть выгоду от приведения типов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e183ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_mb(df): return df.memory_usage(deep=True).sum()/1e6\n",
    "base = mem_mb(train)\n",
    "opt = train.copy()\n",
    "\n",
    "# downcast числовых\n",
    "for c in NUM_COLS:\n",
    "    if pd.api.types.is_float_dtype(opt[c]): opt[c] = pd.to_numeric(opt[c], downcast=\"float\")\n",
    "    if pd.api.types.is_integer_dtype(opt[c]): opt[c] = pd.to_numeric(opt[c], downcast=\"integer\")\n",
    "\n",
    "# категории\n",
    "for c in CAT_COLS:\n",
    "    opt[c] = opt[c].astype(\"category\")\n",
    "\n",
    "print(\"MB before:\", round(base,1), \"after:\", round(mem_mb(opt),1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd383de3",
   "metadata": {},
   "source": [
    "Смотреть: экономия памяти → ускорение I/O/fit.\n",
    "Реакция: применить в рабочем пайплайне (особенно на кластере с жёсткими лимитами)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f48b2",
   "metadata": {},
   "source": [
    "### Числовые: профиль распределений и перекосы\n",
    "\n",
    "Смотри: пропуски, нули/отрицательные, хвосты (p99/p01), сильный скос.\n",
    "Реагируй: лог-преобразования (если >0), квантильный клиппинг, биннинг; проверить «аномальные» отрицательные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f9bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "def profile_numeric(df, num_cols, pcts=(.01,.05,.5,.95,.99)):\n",
    "    rows = []\n",
    "    for c in num_cols:\n",
    "        s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        q = s.quantile(pcts)\n",
    "        rows.append({\n",
    "            \"col\": c,\n",
    "            \"missing_ratio\": s.isna().mean(),\n",
    "            \"zero_ratio\": (s==0).mean(),\n",
    "            \"neg_ratio\": (s<0).mean(),\n",
    "            \"p01\": q.iloc[0], \"p05\": q.iloc[1], \"p50\": q.iloc[2], \"p95\": q.iloc[3], \"p99\": q.iloc[4],\n",
    "            \"skew\": s.skew(), \"kurt\": s.kurt()\n",
    "        })\n",
    "    out = pd.DataFrame(rows).sort_values(\"skew\", ascending=False)\n",
    "    print(\"\\n[ЧИСЛОВЫЕ] Топ-15 по |скосу|:\")\n",
    "    print(out.reindex(out[\"skew\"].abs().sort_values(ascending=False).index).head(15).to_string(index=False))\n",
    "    return out\n",
    "\n",
    "num_profile = profile_numeric(train, NUM_COLS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd7e752",
   "metadata": {},
   "source": [
    "### Числовые: выбросы и кандидаты на лог/клиппинг\n",
    "\n",
    "Смотри: сколько значений попадает за перцентили 1/99 и 5/95.\n",
    "Реагируй: лог1p (если все > −1), клип по квантилям, Huber/Quantile лоссы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab08f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_suggestions(prof):\n",
    "    print(\"\\n[ВЫБРОСЫ] Предварительные предложения:\")\n",
    "    for _, r in prof.iterrows():\n",
    "        high_tail = (r[\"p99\"] - r[\"p95\"]) / (abs(r[\"p50\"]) + 1e-9)\n",
    "        low_tail  = (r[\"p05\"] - r[\"p01\"]) / (abs(r[\"p50\"]) + 1e-9)\n",
    "        log_ok = (r[\"p01\"] > -0.9999)  # условно, чтобы log1p был безопасен\n",
    "        tips = []\n",
    "        if r[\"skew\"] > 1 and log_ok:\n",
    "            tips.append(\"log1p\")\n",
    "        if r[\"p99\"] > r[\"p95\"]*2 or r[\"p01\"] < r[\"p05\"]/2:\n",
    "            tips.append(\"clip[1..99%]\")\n",
    "        if r[\"neg_ratio\"] > 0 and r[\"p95\"] > 0 and abs(r[\"skew\"])>1:\n",
    "            tips.append(\"robust_loss(Quantile/Huber)\")\n",
    "        if tips:\n",
    "            print(f\"  {r['col']}: \" + \", \".join(tips))\n",
    "\n",
    "outlier_suggestions(num_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151e7ca7",
   "metadata": {},
   "source": [
    "### Числовые: корреляции и мультиколлинеарность\n",
    "\n",
    "Смотри: пары с |corr|>0.95; подозрительно дублирующие признаки.\n",
    "Реагируй: удалить клоны/ко-лайнерные из пар, оставить один (или регуляризация)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52350615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman — устойчивее к монотоничным нелинейностям\n",
    "corr = train[NUM_COLS].corr(method=\"spearman\")\n",
    "pairs = []\n",
    "for i, a in enumerate(NUM_COLS):\n",
    "    for b in NUM_COLS[i+1:]:\n",
    "        v = corr.loc[a,b]\n",
    "        if pd.notna(v) and abs(v) >= 0.95:\n",
    "            pairs.append((a,b,float(v)))\n",
    "pairs = sorted(pairs, key=lambda x: -abs(x[2]))[:20]\n",
    "print(\"\\n[КОРРЕЛЯЦИИ] Топ пар |Spearman| ≥ 0.95:\")\n",
    "for a,b,v in pairs:\n",
    "    print(f\"  {a:>24s}  ~  {b:<24s}  corr={v:.3f}\")\n",
    "\n",
    "# VIF (опционально, если statsmodels установлен)\n",
    "try:\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    import numpy as np\n",
    "    X = train[NUM_COLS].select_dtypes(include=[np.number]).dropna().sample(min(50000, train.shape[0]), random_state=42)\n",
    "    vifs = []\n",
    "    for i, c in enumerate(X.columns):\n",
    "        vifs.append((c, variance_inflation_factor(X.values, i)))\n",
    "    print(\"\\n[VIF] Топ-15 признаков с высокой мультиколлинеарностью:\")\n",
    "    for c, v in sorted(vifs, key=lambda x: -x[1])[:15]:\n",
    "        print(f\"  {c:>24s}  VIF={v:.2f}\")\n",
    "except Exception as e:\n",
    "    print(\"\\n[VIF] Пропущено (нет statsmodels или ошибка):\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce8d00",
   "metadata": {},
   "source": [
    "### Мультизначные: глубже метрики редкости/энтропии\n",
    "\n",
    "Смотри: средняя длина, доля пустых, число уникальных токенов, средняя «редкость», доля доминирующего токена.\n",
    "Реагируй: OOF-счётчики частот/TE для токенов, avg_rarity, top1_share, ограничить словарь/хешировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bfde0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def multi_stats(df, cols, sep=\",\"):\n",
    "    print(\"\\n[МУЛЬТИЗНАЧНЫЕ] Сводка:\")\n",
    "    for c in cols:\n",
    "        lists = df[c].fillna(\"\").astype(str).str.split(sep)\n",
    "        lens  = lists.map(lambda x: len([t for t in x if t]))\n",
    "        tokens = [t for lst in lists for t in lst if t]\n",
    "        cnt = Counter(tokens); tot = sum(cnt.values()) or 1\n",
    "        probs = np.array([v/tot for v in cnt.values()])\n",
    "        # метрики\n",
    "        mean_len = lens.mean(); p95_len = lens.quantile(.95)\n",
    "        empty_ratio = (lens==0).mean()\n",
    "        uniq = len(cnt)\n",
    "        entropy = float(-(probs * np.log(probs+1e-12)).sum())\n",
    "        top1_share = max(probs) if len(probs) else 0.0\n",
    "        # средняя редкость токенов на строку\n",
    "        inv_freq = {k: tot/v for k,v in cnt.items()}\n",
    "        avg_rarity = lens.map(lambda L: np.mean([inv_freq.get(t,0) for t in L if t]) if L else 0).mean()\n",
    "        print(f\"  {c}: mean_len={mean_len:.2f}, p95_len={p95_len:.1f}, empty={empty_ratio:.2%}, \"\n",
    "              f\"uniq={uniq}, entropy={entropy:.2f}, top1_share={top1_share:.2%}, avg_rarity={avg_rarity:.1f}\")\n",
    "\n",
    "multi_stats(train, MULTI_COLS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc6200a",
   "metadata": {},
   "source": [
    "### Время: покрытие, дыры, доля «свежих» данных\n",
    "\n",
    "Смотри: min/max, пропуски дней/недель, доля последних 7/14 дней (для холдаута).\n",
    "Реагируй: выбрать TimeSplit, подумать про эмбарго, сделать временные фичи/затухания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e754d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "\n",
    "if DATE_COL:\n",
    "    ts = pd.to_datetime(train[DATE_COL], errors=\"coerce\")\n",
    "    print(\"\\n[ВРЕМЯ] Диапазон:\", ts.min(), \"→\", ts.max())\n",
    "    by_day = ts.dt.floor(\"D\").value_counts().sort_index()\n",
    "    total_days = (ts.max() - ts.min()).days + 1\n",
    "    missing_days = total_days - by_day.shape[0]\n",
    "    print(f\"  дней в диапазоне: {total_days}, из них пустых: {missing_days}\")\n",
    "    # доля последних N дней\n",
    "    for nd in [3,7,14,30]:\n",
    "        cutoff = ts.max() - pd.Timedelta(days=nd-1)\n",
    "        frac = (ts >= cutoff).mean()\n",
    "        print(f\"  доля записей за последние {nd:>2} дней: {frac:.2%}\")\n",
    "    # базовые признаки сезонности (по желанию)\n",
    "    by_week = ts.dt.to_period(\"W\").value_counts().sort_index()\n",
    "    print(\"\\n  хвост распределения по неделям:\")\n",
    "    print(by_week.tail(6).to_string())\n",
    "else:\n",
    "    print(\"\\n[ВРЕМЯ] DATE_COL не задан — пропускаем блок.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08310b18",
   "metadata": {},
   "source": [
    "### Train/Test сравнение: числовые (сдвиги)\n",
    "\n",
    "Смотри: относительные сдвиги mean/std, KS-тест; большие расхождения → риск дрейфа.\n",
    "Реагируй: робастные трансформы, калибровки/клиппинг, адверсариалка, переоценка фич."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a242310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "try:\n",
    "    from scipy.stats import ks_2samp\n",
    "except Exception:\n",
    "    ks_2samp = None\n",
    "\n",
    "def num_drift_report(tr, te, num_cols):\n",
    "    rows=[]\n",
    "    for c in num_cols:\n",
    "        a = pd.to_numeric(tr[c], errors=\"coerce\").dropna()\n",
    "        b = pd.to_numeric(te[c], errors=\"coerce\").dropna()\n",
    "        if len(a)==0 or len(b)==0:\n",
    "            continue\n",
    "        mean_a, std_a = a.mean(), a.std()\n",
    "        mean_b, std_b = b.mean(), b.std()\n",
    "        rel_mean = 0 if mean_a==0 else (mean_b-mean_a)/ (abs(mean_a)+1e-9)\n",
    "        rel_std  = 0 if std_a==0  else (std_b-std_a) / (abs(std_a)+1e-9)\n",
    "        ks_p = ks_2samp(a,b).pvalue if ks_2samp else np.nan\n",
    "        rows.append({\"col\":c, \"rel_mean\":rel_mean, \"rel_std\":rel_std, \"ks_p\":ks_p})\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"drift_score\"] = df[\"rel_mean\"].abs() + 0.5*df[\"rel_std\"].abs() + (df[\"ks_p\"].apply(lambda p: 1 if (not np.isnan(p) and p<1e-3) else 0))\n",
    "    print(\"\\n[DRIFT NUM] Топ-15 подозрительных по сдвигам mean/std/KS:\")\n",
    "    print(df.reindex(df[\"drift_score\"].sort_values(ascending=False).index).head(15).to_string(index=False))\n",
    "    return df\n",
    "\n",
    "num_drift = num_drift_report(train, test, NUM_COLS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1feac25",
   "metadata": {},
   "source": [
    "### Train/Test сравнение: категориальные (дрейф, unseen)\n",
    "\n",
    "Смотри: unseen-доли, JS-дивергенция топ-распределений.\n",
    "Реагируй: __UNK__/__RARE__, freq/TE/hashing, ослабить зависимость модели от нестабильных фич."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3703571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "def js_divergence(p, q):\n",
    "    p = np.asarray(p, dtype=float); q = np.asarray(q, dtype=float)\n",
    "    p = p / (p.sum() + 1e-12); q = q / (q.sum() + 1e-12)\n",
    "    m = 0.5*(p+q)\n",
    "    def H(x): \n",
    "        x = np.where(x>0, x, 1)  # 0*log0=0\n",
    "        return -(x*np.log2(x)).sum()\n",
    "    return H(m) - 0.5*H(p) - 0.5*H(q)\n",
    "\n",
    "def cat_drift_report(tr, te, cat_cols, topn=50):\n",
    "    rows=[]\n",
    "    for c in cat_cols:\n",
    "        vtr = tr[c].astype(\"object\").value_counts()\n",
    "        vte = te[c].astype(\"object\").value_counts()\n",
    "        unseen = len(set(vte.index) - set(vtr.index))\n",
    "        # JS по топ-N объединённому множеству\n",
    "        keys = list(set(vtr.head(topn).index) | set(vte.head(topn).index))\n",
    "        p = np.array([vtr.get(k,0) for k in keys], dtype=float)\n",
    "        q = np.array([vte.get(k,0) for k in keys], dtype=float)\n",
    "        js = js_divergence(p, q) if (p.sum()>0 and q.sum()>0) else np.nan\n",
    "        rows.append({\"col\":c, \"unseen_cnt\":unseen, \"js_top\":js})\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"drift_score\"] = df[\"unseen_cnt\"].rank(pct=True) + df[\"js_top\"].fillna(0).rank(pct=True)\n",
    "    print(\"\\n[DRIFT CAT] Топ-15 по unseen и JS-дивергенции:\")\n",
    "    print(df.reindex(df[\"drift_score\"].sort_values(ascending=False).index).head(15).to_string(index=False))\n",
    "    return df\n",
    "\n",
    "cat_drift = cat_drift_report(train, test, CAT_COLS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb99848a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dca8b1a5",
   "metadata": {},
   "source": [
    "### Адверсариальная валидация (быстрый снэпшот)\n",
    "\n",
    "Смотри: AUC>0.7 — сильный дрейф/различимость train/test; какие фичи ведут к этому (importance).\n",
    "Реагируй: переосмыслить эти фичи, добавить робастности/калибровок, корректнее валидацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde1739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    # только числовые для скорости/простоты\n",
    "    X_tr = train[NUM_COLS].copy()\n",
    "    X_te = test[NUM_COLS].copy()\n",
    "    X = pd.concat([X_tr, X_te], axis=0).fillna(0)\n",
    "    y = np.array([0]*len(X_tr) + [1]*len(X_te))\n",
    "    X = StandardScaler(with_mean=False).fit_transform(X)  # sparse-friendly если что\n",
    "\n",
    "    Xtr, Xva, ytr, yva = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    clf = LogisticRegression(max_iter=200, n_jobs=4)\n",
    "    clf.fit(Xtr, ytr)\n",
    "    auc = roc_auc_score(yva, clf.predict_proba(Xva)[:,1])\n",
    "    print(f\"\\n[ADVERSARIAL] ROC-AUC (train vs test) на числовых: {auc:.3f}  (≥0.70 тревожно)\")\n",
    "except Exception as e:\n",
    "    print(\"\\n[ADVERSARIAL] Пропущено (нет sklearn или ошибка):\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32459a28",
   "metadata": {},
   "source": [
    "### Рекомендации (авто-вывод по эвристикам)\n",
    "\n",
    "Смотри: итоговую сводку — это «to-do» для фичеринга/валидации/постпроца.\n",
    "Реагируй: перенеси пункты в свой рабочий план следующего ноутбука (features/train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb75dc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== РЕКОМЕНДАЦИИ (эвристики) ===\")\n",
    "\n",
    "# 1) числовые: лог/клип/робаст-лосс\n",
    "skew_cols = num_profile[(num_profile[\"skew\"]>1) & (num_profile[\"p01\"]>-0.9999)][\"col\"].tolist()\n",
    "if skew_cols:\n",
    "    print(\"• Лог-преобразование кандидаты:\", \", \".join(skew_cols[:10]), (\"…+ ещё\" if len(skew_cols)>10 else \"\"))\n",
    "\n",
    "clip_cols = []\n",
    "for _, r in num_profile.iterrows():\n",
    "    if (r[\"p99\"]>r[\"p95\"]*2) or (r[\"p01\"]<r[\"p05\"]/2):\n",
    "        clip_cols.append(r[\"col\"])\n",
    "if clip_cols:\n",
    "    print(\"• Квантильный клиппинг (1–99%) для:\", \", \".join(clip_cols[:10]), (\"…+ ещё\" if len(clip_cols)>10 else \"\"))\n",
    "\n",
    "neg_cols = num_profile[num_profile[\"neg_ratio\"]>0][\"col\"].tolist()\n",
    "if neg_cols:\n",
    "    print(\"• Отрицательные значения проверять (ошибки/семантика):\", \", \".join(neg_cols[:10]), (\"…+ ещё\" if len(neg_cols)>10 else \"\"))\n",
    "\n",
    "# 2) мультизначные\n",
    "if MULTI_COLS:\n",
    "    print(\"• Мультизначные: считать OOF-частоты токенов, avg_rarity, top1_share; ограничить словарь/хешировать.\")\n",
    "\n",
    "# 3) категориальные\n",
    "hi_card = [c for c in CAT_COLS if train[c].nunique() > 1000]\n",
    "if hi_card:\n",
    "    print(\"• High-card категориальные → target/freq/WOE/CTR (строго OOF), hashing крестов:\", \", \".join(hi_card[:10]))\n",
    "\n",
    "# 4) корреляции/ко-лайнерность\n",
    "if len(pairs) > 0:\n",
    "    print(\"• Сильные линейные связи между числовыми (|ρ|≥0.95): оставить один из пары/регуляризовать.\")\n",
    "\n",
    "# 5) дрейф числовых\n",
    "if not num_drift.empty and (num_drift[\"drift_score\"].max() > 1.0):\n",
    "    top_drift = num_drift.sort_values(\"drift_score\", ascending=False).head(5)[\"col\"].tolist()\n",
    "    print(\"• Числовой дрейф в test → робастные трансформы/калибровки/адверсариалка. Топ:\", \", \".join(top_drift))\n",
    "\n",
    "# 6) дрейф категориальных\n",
    "if not cat_drift.empty:\n",
    "    w = cat_drift.sort_values(\"drift_score\", ascending=False).head(5)\n",
    "    print(\"• Категориальный дрейф (unseen/JS) → __UNK__/__RARE__, freq/TE/hashing. Топ:\", \", \".join(w[\"col\"].tolist()))\n",
    "\n",
    "# 7) время\n",
    "if DATE_COL:\n",
    "    print(\"• Использовать TimeSplit; при сильной «хвостатости» — эмбарго и экспоненциальные затухания фич.\")\n",
    "\n",
    "# 8) валидация\n",
    "print(\"• Валидация: без утечек (OOF для TE/CTR), проверка стабильности на альтернативном сплите.\")\n",
    "print(\"• Пост-проц: клиппинг/квантиль для регрессии, подбор порога τ для бинарной, инварианты top-k при ранжировании/мульти-лейбл.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ef698",
   "metadata": {},
   "source": [
    "### GEO-1 — Валидация координат и базовая сводка\n",
    "\n",
    "Смотри: NaN, выходы за диапазоны, (0,0), дубликаты точек, разброс по широте/долготе.\n",
    "Реагируй: почистить/заполнить, проверить источники данных; слишком много (0,0) → явный баг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac1654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if LAT_COL and LON_COL and LAT_COL in train and LON_COL in train:\n",
    "    lat = pd.to_numeric(train[LAT_COL], errors=\"coerce\")\n",
    "    lon = pd.to_numeric(train[LON_COL], errors=\"coerce\")\n",
    "\n",
    "    n = len(train)\n",
    "    bad_lat = ((lat<-90) | (lat>90) | lat.isna()).sum()\n",
    "    bad_lon = ((lon<-180)| (lon>180)| lon.isna()).sum()\n",
    "    zero_zero = ((lat.fillna(0)==0) & (lon.fillna(0)==0)).sum()\n",
    "\n",
    "    print(\"\\n[GEO] Валидация координат (train):\")\n",
    "    print(f\"  всего: {n:,}\")\n",
    "    print(f\"  некорректных lat: {bad_lat:,}  ({bad_lat/n:.2%})\")\n",
    "    print(f\"  некорректных lon: {bad_lon:,}  ({bad_lon/n:.2%})\")\n",
    "    print(f\"  ровно (0,0):      {zero_zero:,}  ({zero_zero/n:.2%})\")\n",
    "\n",
    "    lat_ok = lat.clip(-90, 90)\n",
    "    lon_ok = lon.clip(-180, 180)\n",
    "    print(\"\\n[GEO] Диапазоны (после клипа к валидным границам):\")\n",
    "    print(f\"  lat min/max: {lat_ok.min():.6f} .. {lat_ok.max():.6f}\")\n",
    "    print(f\"  lon min/max: {lon_ok.min():.6f} .. {lon_ok.max():.6f}\")\n",
    "\n",
    "    # простая оценка дубликатов точек\n",
    "    dup_points = pd.Series(list(zip(lat_ok.round(6), lon_ok.round(6)))).duplicated().sum()\n",
    "    print(f\"  дубликатов точек (до 1e-6 град): {dup_points:,}\")\n",
    "else:\n",
    "    print(\"\\n[GEO] LAT_COL/LON_COL не заданы или нет в данных — гео-блок пропущен.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0567daf1",
   "metadata": {},
   "source": [
    "### «Грид» (агрегации по ячейкам) на 300м/1000м\n",
    "\n",
    "Без внешних зависимостей: переводим метры в градусы (лат≈111.32км/°; lon зависит от cos(lat̄)).\n",
    "Смотри: топ-плотности, размер «хвоста» (сколько ячеек с единичками).\n",
    "Реагируй: фичи контрастов (среднее/медиана/плотности в радиусе), H3/квадробины в пайплайне."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def meters_to_deg_lat(m): return m/111_320.0\n",
    "def meters_to_deg_lon(m, lat_deg):\n",
    "    lat_rad = np.deg2rad(np.clip(lat_deg, -89.9, 89.9))\n",
    "    return m/(111_320.0*np.cos(lat_rad))\n",
    "\n",
    "if LAT_COL and LON_COL and LAT_COL in train and LON_COL in train:\n",
    "    lat = pd.to_numeric(train[LAT_COL], errors=\"coerce\")\n",
    "    lon = pd.to_numeric(train[LON_COL], errors=\"coerce\")\n",
    "    lat_mean = float(lat.dropna().mean()) if lat.notna().any() else 0.0\n",
    "\n",
    "    for step_m in GEO_STEPS_M:\n",
    "        dlat = meters_to_deg_lat(step_m)\n",
    "        dlon = meters_to_deg_lon(step_m, lat_mean)\n",
    "        lat_bin = np.floor(lat/dlat).astype(\"Int64\")\n",
    "        lon_bin = np.floor(lon/dlon).astype(\"Int64\")\n",
    "        grid_id = (lat_bin.astype(str) + \"_\" + lon_bin.astype(str))\n",
    "\n",
    "        vc = grid_id.value_counts(dropna=True)\n",
    "        tot_cells = vc.shape[0]\n",
    "        singletons = int((vc==1).sum())\n",
    "        print(f\"\\n[GEO] Грид {step_m} м:\")\n",
    "        print(f\"  всего ячеек: {tot_cells:,}, одиночных: {singletons:,}  ({singletons/max(1,len(grid_id)):.2%} записей-одиночек)\")\n",
    "        print(\"  топ-10 плотных ячеек (count):\")\n",
    "        print(vc.head(10).to_string())\n",
    "else:\n",
    "    print(\"\\n[GEO] Пропуск «грид»-аналитики: нет координат.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd917f",
   "metadata": {},
   "source": [
    "### GEO-3 — Плотность соседей в радиусах (BallTree, если есть sklearn)\n",
    "\n",
    "Смотри: quantiles соседей при 300/1000м — это «плотность»; длинный «хвост» → разреженные регионы.\n",
    "Реагируй: использовать эти плотности как фичи; для пустынных зон — бэкоф на глобальные/региональные агрегаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb9d41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.neighbors import BallTree\n",
    "    import numpy as np, pandas as pd\n",
    "\n",
    "    if LAT_COL and LON_COL and LAT_COL in train and LON_COL in train:\n",
    "        # haversine требует радианы\n",
    "        lat_r = np.deg2rad(pd.to_numeric(train[LAT_COL], errors=\"coerce\"))\n",
    "        lon_r = np.deg2rad(pd.to_numeric(train[LON_COL], errors=\"coerce\"))\n",
    "        mask = lat_r.notna() & lon_r.notna()\n",
    "        coords = np.c_[lat_r[mask], lon_r[mask]]\n",
    "        if len(coords) == 0:\n",
    "            print(\"\\n[GEO] BallTree: нет валидных координат.\")\n",
    "        else:\n",
    "            tree = BallTree(coords, metric=\"haversine\")\n",
    "            R = 6_371_000.0  # радиус Земли (м)\n",
    "            print(\"\\n[GEO] Плотность соседей (BallTree, haversine):\")\n",
    "            for r_m in NEIGHBOR_RADII_M:\n",
    "                r_rad = r_m / R\n",
    "                ind = tree.query_radius(coords, r=r_rad, count_only=False)\n",
    "                # Кол-во соседей без себя:\n",
    "                neigh_counts = np.array([len(ix)-1 for ix in ind], dtype=int)\n",
    "                q = pd.Series(neigh_counts).quantile([.0,.25,.5,.75,.9,.99,1.0])\n",
    "                print(f\"  радиус {r_m:>4} м → quantiles соседей:\")\n",
    "                print(q.to_string())\n",
    "    else:\n",
    "        print(\"\\n[GEO] Пропуск BallTree: нет LAT/LON.\")\n",
    "except Exception as e:\n",
    "    print(\"\\n[GEO] BallTree/ sklearn недоступен — блок пропущен.\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbd17f",
   "metadata": {},
   "source": [
    "### GEO-4 — Центроиды/кластеры (быстрая оценка структуры)\n",
    "\n",
    "Смотри: среднюю/макс дистанции до центроида; при больших значениях — «расползание».\n",
    "Реагируй: фичи «расстояние до ближайшего кластера/центра»; региональные агрегаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b108367",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.cluster import KMeans\n",
    "    import numpy as np, pandas as pd\n",
    "\n",
    "    if LAT_COL and LON_COL and LAT_COL in train and LON_COL in train:\n",
    "        lat = pd.to_numeric(train[LAT_COL], errors=\"coerce\")\n",
    "        lon = pd.to_numeric(train[LON_COL], errors=\"coerce\")\n",
    "        m = lat.notna() & lon.notna()\n",
    "        XY = np.c_[lat[m], lon[m]]\n",
    "        n = len(XY)\n",
    "        if n >= 1000:\n",
    "            k = min(20, max(2, n//5000))  # грубая эвристика\n",
    "            km = KMeans(n_clusters=k, random_state=42, n_init=\"auto\").fit(XY)\n",
    "            # евклидово в градусах (для грубой оценки)\n",
    "            dist = ((XY - km.cluster_centers_[km.labels_])**2).sum(1)**0.5\n",
    "            q = pd.Series(dist).quantile([.5,.9,.99])\n",
    "            print(\"\\n[GEO] Кластеры KMeans:\")\n",
    "            print(f\"  k={k}, медиана/0.9/0.99 дистанции до центра (в градусах): {q.to_dict()}\")\n",
    "        else:\n",
    "            print(\"\\n[GEO] KMeans пропущен — слишком мало валидных точек (<1000).\")\n",
    "    else:\n",
    "        print(\"\\n[GEO] KMeans пропущен — нет координат.\")\n",
    "except Exception as e:\n",
    "    print(\"\\n[GEO] KMeans недоступен — блок пропущен.\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521e2a20",
   "metadata": {},
   "source": [
    "### GEO-5 — Train/Test покрытие по «гридам» и дрейф\n",
    "\n",
    "Смотри: Jaccard пересечение ячеек, доля «ячеек только в test», топ-дрейф ячеек по долям.\n",
    "Реагируй: делать back-off в постпроц/фичах для «новых» регионов (unseen bins), усилить глобальные/соседские агрегаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106eb34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "def grid_bins(df, lat_col, lon_col, step_m):\n",
    "    lat = pd.to_numeric(df[lat_col], errors=\"coerce\")\n",
    "    lon = pd.to_numeric(df[lon_col], errors=\"coerce\")\n",
    "    lat_mean = float(lat.dropna().mean()) if lat.notna().any() else 0.0\n",
    "    dlat = step_m/111_320.0\n",
    "    dlon = step_m/(111_320.0*np.cos(np.deg2rad(np.clip(lat_mean, -89.9, 89.9))))\n",
    "    lat_bin = np.floor(lat/dlat).astype(\"Int64\")\n",
    "    lon_bin = np.floor(lon/dlon).astype(\"Int64\")\n",
    "    return (lat_bin.astype(str) + \"_\" + lon_bin.astype(str))\n",
    "\n",
    "if LAT_COL and LON_COL and LAT_COL in train and LON_COL in train:\n",
    "    for step_m in GEO_STEPS_M:\n",
    "        gtr = grid_bins(train, LAT_COL, LON_COL, step_m)\n",
    "        gte = grid_bins(test,  LAT_COL, LON_COL, step_m) if (LAT_COL in test and LON_COL in test) else pd.Series([],dtype=str)\n",
    "\n",
    "        s_tr = set(gtr.dropna().unique())\n",
    "        s_te = set(gte.dropna().unique())\n",
    "        inter = len(s_tr & s_te)\n",
    "        jacc = inter / max(1, len(s_tr | s_te))\n",
    "        only_te = len(s_te - s_tr)\n",
    "\n",
    "        print(f\"\\n[GEO] Train/Test по гриду {step_m} м:\")\n",
    "        print(f\"  ячеек train: {len(s_tr):,}, test: {len(s_te):,}, пересечение: {inter:,}, Jaccard: {jacc:.3f}\")\n",
    "        print(f\"  только в test: {only_te:,}  ({only_te/max(1,len(s_te)):.2%} от test-яч.)\")\n",
    "\n",
    "        # дрейф долей по топ-ячейкам\n",
    "        vc_tr = gtr.value_counts(normalize=True)\n",
    "        vc_te = gte.value_counts(normalize=True)\n",
    "        keys = list((set(vc_tr.head(100).index) | set(vc_te.head(100).index)))\n",
    "        drift = []\n",
    "        for k in keys:\n",
    "            drift.append((k, float(vc_tr.get(k,0)-vc_te.get(k,0))))\n",
    "        drift = sorted(drift, key=lambda x: -abs(x[1]))[:10]\n",
    "        print(\"  топ-10 ячеек по |разнице долей train-test|:\")\n",
    "        for k, d in drift:\n",
    "            print(f\"    {k}: Δ={d:+.4f}\")\n",
    "else:\n",
    "    print(\"\\n[GEO] Нет координат — сравнение покрытий пропущено.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258af31a",
   "metadata": {},
   "source": [
    "### GEO-6 — Таргет vs гео (если есть TARGET_COL)\n",
    "\n",
    "Смотри: насколько таргет «гуляет» по ячейкам — это кандидат для локальных агрегатов/калибровок.\n",
    "Реагируй: добавить контрасты: mean/median/quantiles таргета по гриду (OOF!), расстояние до «дорогих/дешёвых» зон."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2158a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "if TARGET_COL and LAT_COL and LON_COL and (TARGET_COL in train):\n",
    "    y = pd.to_numeric(train[TARGET_COL], errors=\"coerce\")\n",
    "    for step_m in GEO_STEPS_M:\n",
    "        g = grid_bins(train, LAT_COL, LON_COL, step_m)\n",
    "        df = pd.DataFrame({\"grid\": g, \"y\": y}).dropna()\n",
    "        if df.empty:\n",
    "            print(f\"\\n[GEO] Таргет по гриду {step_m} м: нет валидных точек.\")\n",
    "            continue\n",
    "        agg = df.groupby(\"grid\")[\"y\"].agg([\"count\",\"mean\",\"median\",\"std\"]).sort_values(\"count\", ascending=False)\n",
    "        print(f\"\\n[GEO] Таргет по гриду {step_m} м (топ-10 по покрытию):\")\n",
    "        print(agg.head(10).to_string())\n",
    "        print(\"  разброс mean по ячейкам (q05/median/q95):\",\n",
    "              agg[\"mean\"].quantile(.05), agg[\"mean\"].quantile(.5), agg[\"mean\"].quantile(.95))\n",
    "else:\n",
    "    print(\"\\n[GEO] Нет TARGET или координат — анализ таргета по гео пропущен.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dd5f73",
   "metadata": {},
   "source": [
    "### GEO-7 — Резюме рекомендаций по гео\n",
    "\n",
    "Смотри: краткий список действий, завязанный на факты выше.\n",
    "Реагируй: перенеси пункты в следующий ноут (features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8a52b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== РЕКОМЕНДАЦИИ ПО ГЕО ===\")\n",
    "\n",
    "# 1) Валидность\n",
    "if LAT_COL and LON_COL and LAT_COL in train and LON_COL in train:\n",
    "    lat = pd.to_numeric(train[LAT_COL], errors=\"coerce\")\n",
    "    lon = pd.to_numeric(train[LON_COL], errors=\"coerce\")\n",
    "    if ((lat.fillna(0)==0) & (lon.fillna(0)==0)).mean() > 0.001:\n",
    "        print(\"• Есть (0,0) точки → почистить/маскировать, иначе испортят локальные агрегаты.\")\n",
    "\n",
    "# 2) Гриды/контрасты\n",
    "print(\"• Добавить локальные контрасты по гриду (например,\", \", \".join([f\"{m}м\" for m in GEO_STEPS_M]), \") — count/mean/median/quantiles таргета (строго OOF).\")\n",
    "\n",
    "# 3) Плотности/соседи\n",
    "print(\"• Фичи плотности: число соседей в радиусах\", NEIGHBOR_RADII_M, \"метров; доля одиночек; расстояние до ближайшего центра/кластера.\")\n",
    "\n",
    "# 4) Train/Test покрытие\n",
    "print(\"• Есть unseen/only-in-test ячейки? → бэкоф на глобальные/региональные агрегаты; избегать жёсткой привязки к редким ячейкам.\")\n",
    "\n",
    "# 5) Постпроц/устойчивость\n",
    "print(\"• Если сильный гео-дрейф таргета: клиппинг по регионам/локальные калибровки; TimeSplit с эмбарго, если есть время.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
