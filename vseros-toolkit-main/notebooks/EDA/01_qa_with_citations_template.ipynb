{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610e9505",
   "metadata": {},
   "source": [
    "# 01 — QA с цитатами (RAG шаблон для олимпиадных задач)\n",
    "\n",
    "Этот ноутбук — универсальный шаблон для задач формата «вопрос → ответ + цитаты» с использованием RAG. Шаги пайплайна:\n",
    "1. Считываем корпус и вопросы.\n",
    "2. Строим индексы (BM25 и/или dense).\n",
    "3. Ретривим кандидаты, при желании реранкаем и делаем MMR.\n",
    "4. Собираем контекст и формируем промпт.\n",
    "5. Вызываем LLM, парсим JSON-ответ с цитатами.\n",
    "6. Считаем метрики, готовим сабмит.\n",
    "\n",
    "Все места, которые надо адаптировать под конкретное соревнование, помечены `TODO` и снабжены пояснениями."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaff010",
   "metadata": {},
   "source": [
    "## 1. Подключение данных задачи\n",
    "\n",
    "### 1.1 Понимание формата задачи\n",
    "\n",
    "Чек-лист перед стартом:\n",
    "- Какие колонки есть в train/val/test: `id`/`qid`, `question`, `answer` (gold) и т.д.\n",
    "- Где лежит корпус: один файл или директория с множеством документов.\n",
    "- Какой формат сабмита: одна колонка `answer`, JSON-строка с полями или несколько колонок. Отредактируйте ниже соответствующие шаги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Пути к данным. Подставьте реальные пути под свою задачу.\n",
    "DATA_DIR = \"data\"  # TODO: поменяйте, если данные лежат в другом месте\n",
    "TRAIN_PATH = f\"{DATA_DIR}/train.csv\"  # TODO: поставьте путь к train (csv/json/tsv)\n",
    "VAL_PATH = f\"{DATA_DIR}/val.csv\"      # TODO: если вал в отдельном файле\n",
    "TEST_PATH = f\"{DATA_DIR}/test.csv\"    # TODO: путь к тесту\n",
    "CORPUS_PATH = f\"{DATA_DIR}/corpus\"    # TODO: директория или файл с корпусом\n",
    "OUTPUT_DIR = \"outputs\"                # TODO: куда сохранять предсказания/сабмиты\n",
    "\n",
    "# Флаги контроля процесса\n",
    "HAS_VAL = False  # TODO: True, если есть отдельный validation split\n",
    "HAS_GOLD_ANSWERS = True  # TODO: False, если в train нет правильных ответов\n",
    "FAST_DEV_RUN = True  # Включите, чтобы прогонять пайплайн только на нескольких примерах для быстрой проверки\n",
    "\n",
    "# Создадим директорию для выводов, если нужно\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1114de31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: подстройте под формат своих файлов (csv/json/tsv/parquet)\n",
    "train_df = pd.read_csv(TRAIN_PATH) if os.path.exists(TRAIN_PATH) else pd.DataFrame()\n",
    "val_df = pd.read_csv(VAL_PATH) if HAS_VAL and os.path.exists(VAL_PATH) else pd.DataFrame()\n",
    "test_df = pd.read_csv(TEST_PATH) if os.path.exists(TEST_PATH) else pd.DataFrame()\n",
    "\n",
    "# TODO: замените имена колонок на реальные из вашего датасета\n",
    "# Например, если в файле колонка называется \"question_text\", переименуйте её в \"question\" ниже.\n",
    "renames = {\n",
    "    'id': 'qid',  # TODO: замените, если идентификатор называется иначе\n",
    "    'question': 'question',  # TODO: замените на реальное имя\n",
    "    'answer': 'answer',  # TODO: если gold ответы есть в train\n",
    "}\n",
    "train_df = train_df.rename(columns={k: v for k, v in renames.items() if k in train_df.columns})\n",
    "val_df = val_df.rename(columns={k: v for k, v in renames.items() if k in val_df.columns})\n",
    "test_df = test_df.rename(columns={k: v for k, v in renames.items() if k in test_df.columns})\n",
    "\n",
    "# Проверим обязательные колонки\n",
    "required_cols = ['qid', 'question']\n",
    "missing = [c for c in required_cols if (not train_df.empty and c not in train_df.columns)]\n",
    "if missing:\n",
    "    print(f\"[WARNING] В train отсутствуют колонки: {missing}. Проверьте rename выше.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f0d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Размеры данных:\")\n",
    "print({\n",
    "    'train': train_df.shape,\n",
    "    'val': val_df.shape,\n",
    "    'test': test_df.shape,\n",
    "})\n",
    "\n",
    "if not train_df.empty:\n",
    "    print()\n",
    "    print(\"Первые строки train (проверьте, что колонки правильные):\")\n",
    "    display(train_df.head())\n",
    "    if 'question' in train_df.columns:\n",
    "        print()\n",
    "        print(\"Пример вопросов:\")\n",
    "        print(train_df['question'].head(3).tolist())\n",
    "    if 'answer' in train_df.columns:\n",
    "        print()\n",
    "        print(\"Пример ответов:\")\n",
    "        print(train_df['answer'].head(3).tolist())\n",
    "else:\n",
    "    print(\"[INFO] train_df пустой — заполните пути и перезагрузите.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60456ca2",
   "metadata": {},
   "source": [
    "## 2. Конфигурация RAG\n",
    "\n",
    "Гиперпараметры, которые чаще всего двигают качество:\n",
    "- `TOP_K_RETRIEVAL`: сколько чанков брать на первом шаге ретрива (чем больше, тем медленнее, но выше recall).\n",
    "- `TOP_K_CE`: сколько оставить для cross-encoder; обычно меньше, чем TOP_K_RETRIEVAL.\n",
    "- `MAX_CONTEXT_TOKENS`: ограничение размера контекста для LLM.\n",
    "- `ALPHA_HYBRID`: баланс BM25 vs dense в гибридном retriever (0 — только BM25, 1 — только dense).\n",
    "- Флаги: `USE_BM25`, `USE_DENSE`, `USE_HYBRID`, `USE_CROSS_ENCODER`, `USE_MMR` — включайте/выключайте каналы, когда тестируете разные режимы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cc5519",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Базовые гиперпараметры retriever/LLM\n",
    "TOP_K_RETRIEVAL = 50  # TODO: уменьшите для скорости, увеличьте для полноты\n",
    "TOP_K_CE = 20         # TODO: сколько кандидатов отдавать cross-encoder\n",
    "MAX_CONTEXT_TOKENS = 1024  # TODO: ограничение на суммарные токены контекста\n",
    "ALPHA_HYBRID = 0.5    # TODO: 0.0 => чистый BM25, 1.0 => чистый dense\n",
    "\n",
    "USE_BM25 = True       # TODO: выключите, если хотите тестировать только dense\n",
    "USE_DENSE = True      # TODO: выключите, если нет эмбеддеров\n",
    "USE_HYBRID = True     # TODO: False, если хотите явно выбрать bm25_candidates или dense_candidates\n",
    "USE_CROSS_ENCODER = False  # TODO: включите, если есть готовый cross-encoder\n",
    "USE_MMR = False       # TODO: включите, если хотите диверсифицировать выдачу (требует поддержки в retrieval функциях)\n",
    "\n",
    "VERBOSE_RAG = True\n",
    "SHOW_PROGRESS = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049c9b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import random\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from rag.corpus import (\n",
    "    Corpus,\n",
    "    build_corpus_from_texts,\n",
    "    add_chunks_to_corpus,\n",
    "    get_chunk_texts,\n",
    "    get_chunk,\n",
    "    get_document,\n",
    ")\n",
    "from rag import chunkers\n",
    "from rag.indices import build_bm25_index, build_dense_index, BM25Index, DenseIndex\n",
    "from rag.embeddings import EmbeddingCache\n",
    "from rag.retrieval import bm25_candidates, dense_candidates, hybrid_candidates\n",
    "from rag.rerank import rerank_cross_encoder\n",
    "from rag.context import build_context_window\n",
    "from rag.debug import inspect_and_print, print_candidates_summary, print_stage_transition\n",
    "from rag.candidates import Candidate\n",
    "\n",
    "# Если какие-то импорты пока не нужны, можно закомментировать, но пусть остаются как шпаргалка доступных функций.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f95676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "# Фиксируем сиды, чтобы результаты были воспроизводимыми при перезапуске ноутбука.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd95dce1",
   "metadata": {},
   "source": [
    "## 3. Корпус и чанки\n",
    "\n",
    "### 3.1 Откуда брать корпус\n",
    "- Корпус может быть заранее собран в одном файле (txt/json) или лежать как набор файлов в директории.\n",
    "- Под конкретную задачу надо:\n",
    "  - либо поменять чтение файлов ниже,\n",
    "  - либо заменить блок на загрузку готового `Corpus` из pickle/JSON (этот шаблон не сохраняет/не грузит на диск, но вы можете добавить)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f29fd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: определите, как устроен корпус. Ниже — простейший пример: каждый файл в директории CORPUS_PATH становится отдельным документом.\n",
    "\n",
    "def load_raw_corpus(corpus_path: str) -> List[str]:\n",
    "    texts = []\n",
    "    if os.path.isdir(corpus_path):\n",
    "        for fname in sorted(os.listdir(corpus_path)):\n",
    "            fpath = os.path.join(corpus_path, fname)\n",
    "            if os.path.isfile(fpath):\n",
    "                with open(fpath, 'r', encoding='utf-8') as f:\n",
    "                    texts.append(f.read())\n",
    "    elif os.path.isfile(corpus_path):\n",
    "        with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "            texts.append(f.read())\n",
    "    else:\n",
    "        print(f\"[WARNING] Корпус по пути {corpus_path} не найден. Заполните CORPUS_PATH.\")\n",
    "    return texts\n",
    "\n",
    "raw_texts = load_raw_corpus(CORPUS_PATH)\n",
    "corpus = build_corpus_from_texts(raw_texts)\n",
    "print(f\"Загружено документов: {len(corpus.documents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1290b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: выберите тип чанкинга под свой корпус.\n",
    "# Варианты:\n",
    "# - chunkers.char_chunker(chunk_size=1000, overlap=200)\n",
    "# - chunkers.ParagraphChunker() если абзацы уже размечены\n",
    "# - chunkers.TokenChunker(tokenizer=..., chunk_size=256, overlap=32) если нужен контроль по токенам LLM\n",
    "\n",
    "chunker = chunkers.char_chunker(chunk_size=1000, overlap=200)\n",
    "add_chunks_to_corpus(corpus, chunker=chunker)\n",
    "print(f\"После чанкинга чанков: {len(corpus.chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb0c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if corpus.chunks:\n",
    "    avg_len = sum(len(ch.text) for ch in corpus.chunks) / len(corpus.chunks)\n",
    "    print(f\"Средняя длина чанка (символов): {avg_len:.1f}\")\n",
    "    print(\"Пример первых 3 чанков:\")\n",
    "    for idx, ch in enumerate(corpus.chunks[:3]):\n",
    "        print(f\"[chunk_idx={idx}] doc_id={ch.document_id}\\n{ch.text[:400]}\\n---\")\n",
    "else:\n",
    "    print(\"[WARNING] Чанков нет — проверьте шаг загрузки корпуса и чанкинг.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc0927",
   "metadata": {},
   "source": [
    "## 4. Индексы BM25 и Dense\n",
    "\n",
    "Зачем два индекса:\n",
    "- **BM25** — быстро ищет по ключевым словам, устойчив к редким терминам.\n",
    "- **Dense** — семантический поиск, работает на эмбеддингах, может требовать GPU.\n",
    "- **Гибрид** совмещает преимущества: BM25 для точных совпадений, dense для перефразов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aa8043",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunk_texts = get_chunk_texts(corpus)\n",
    "chunk_meta = []\n",
    "for idx, ch in enumerate(corpus.chunks):\n",
    "    chunk_meta.append({\n",
    "        'doc_id': ch.document_id,\n",
    "        'chunk_idx': idx,\n",
    "    })\n",
    "print(f\"Подготовлено {len(chunk_texts)} текстов для индексации.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da81d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bm25_index = None\n",
    "if USE_BM25:\n",
    "    bm25_index = build_bm25_index(\n",
    "        docs=chunk_texts,\n",
    "        meta=chunk_meta,\n",
    "        verbose=VERBOSE_RAG,\n",
    "        show_progress=SHOW_PROGRESS,\n",
    "    )\n",
    "    # TODO: при желании сохраните bm25_index на диск и подгружайте при следующих запусках.\n",
    "else:\n",
    "    print(\"[INFO] BM25 выключен.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d211e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dense_index = None\n",
    "emb_cache = None\n",
    "if USE_DENSE:\n",
    "    # TODO: инициализируйте модель эмбеддингов. Пример для SentenceTransformers:\n",
    "    # from sentence_transformers import SentenceTransformer\n",
    "    # emb_model = SentenceTransformer(\"your-emb-model-id\")\n",
    "    emb_model = None  # TODO: замените на реальную модель с методом .encode\n",
    "    emb_cache = EmbeddingCache(max_size=10_000)\n",
    "    dense_index = build_dense_index(\n",
    "        docs=chunk_texts,\n",
    "        emb_model=emb_model,\n",
    "        cache=emb_cache,\n",
    "        model_id=\"your-emb-model-id\",  # TODO: укажите фактический id модели\n",
    "        verbose=VERBOSE_RAG,\n",
    "        show_progress=SHOW_PROGRESS,\n",
    "    )\n",
    "    # TODO: как и BM25, dense_index можно кэшировать на диск.\n",
    "else:\n",
    "    print(\"[INFO] Dense индекс выключен.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c15789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if bm25_index and dense_index:\n",
    "    assert bm25_index.n_docs == dense_index.n_docs == len(corpus.chunks), \"Несогласованное число документов в индексах\"\n",
    "    print(f\"Размерность эмбеддингов dense: {dense_index.dim}\")\n",
    "elif bm25_index:\n",
    "    assert bm25_index.n_docs == len(corpus.chunks)\n",
    "elif dense_index:\n",
    "    assert dense_index.n_docs == len(corpus.chunks)\n",
    "else:\n",
    "    print(\"[WARNING] Оба индекса выключены — RAG работать не будет.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be33e0",
   "metadata": {},
   "source": [
    "## 5. LLM-клиент и промпт\n",
    "\n",
    "Модель должна отвечать **строго в JSON**:\n",
    "```json\n",
    "{\n",
    "  \"answer\": \"Краткий текстовый ответ\",\n",
    "  \"citations\": [12, 47]\n",
    "}\n",
    "```\n",
    "В промпт передаются:\n",
    "- вопрос,\n",
    "- несколько чанков с их `chunk_idx` и текстом,\n",
    "- инструкция отвечать коротко и на основе контекста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90442714",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LLM_MODEL_NAME = \"local-llm\"  # TODO: укажите модель или endpoint\n",
    "LLM_MAX_TOKENS = 256          # TODO: ограничение длины ответа\n",
    "LLM_TEMPERATURE = 0.0         # TODO: температуру можно поднять для более креативных ответов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_prompt(question: str, context_chunks: List[dict], extra_instructions: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Собирает промпт для LLM:\n",
    "    - формулировка вопроса;\n",
    "    - список чанков с их индексами и текстом;\n",
    "    - инструкция отвечать строго в JSON-формате {\"answer\": ..., \"citations\": [...]}.\n",
    "\n",
    "    TODO: адаптируйте тон инструкции под язык и требования конкретной задачи.\n",
    "    \"\"\"\n",
    "    context_str = \"\\n\\n\".join(\n",
    "        [f\"[chunk_idx={c['chunk_idx']}]\\n{c['text']}\" for c in context_chunks]\n",
    "    )\n",
    "    instructions = (\n",
    "        \"Ответь на вопрос кратко, используя только факты из контекста. \"\n",
    "        \"Формат: JSON с полями 'answer' и 'citations' (список chunk_idx). \"\n",
    "        f\"{extra_instructions}\"\n",
    "    )\n",
    "    prompt = (\n",
    "        \"Ты — помощник, который отвечает строго на основе приведённых фрагментов.\\n\"\n",
    "        f\"Вопрос: {question}\\n\\n\"\n",
    "        f\"Контекст:\\n{context_str}\\n\\n\"\n",
    "        f\"Инструкция: {instructions}\\n\"\n",
    "    )\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ebf17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def call_llm(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    TODO: реализуйте конкретный вызов LLM.\n",
    "    Варианты:\n",
    "    - HTTP-запрос к локальному серверу с моделью.\n",
    "    - Вызов модели из transformers/Pipeline.\n",
    "    - Использование готового клиента из инфраструктуры соревнования.\n",
    "\n",
    "    Функция должна вернуть сырую строку ответа модели.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Заполните call_llm под свою среду LLM.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b853d63",
   "metadata": {},
   "source": [
    "## 6. Пайплайн для одного вопроса\n",
    "\n",
    "Шаги:\n",
    "1. Ретривим кандидаты через `hybrid_candidates` (или отдельно BM25/dense).\n",
    "2. Опционально реранкаем через `rerank_cross_encoder`.\n",
    "3. Опционально применяем MMR для диверсификации.\n",
    "4. Собираем контекст через `build_context_window`.\n",
    "5. Формируем промпт → вызываем LLM → парсим JSON.\n",
    "6. Возвращаем ответ, цитаты и отладочную информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd3b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def answer_one_question(\n",
    "    qid: Any,\n",
    "    question: str,\n",
    "    corpus: Corpus,\n",
    "    bm25_index: Optional[BM25Index],\n",
    "    dense_index: Optional[DenseIndex],\n",
    "    emb_model: Any,\n",
    "    emb_cache: Optional[EmbeddingCache] = None,\n",
    "    use_hybrid: bool = True,\n",
    "    use_ce: bool = False,\n",
    "    use_mmr: bool = False,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Выполняет полный шаг RAG для одного вопроса:\n",
    "    - ретрив (bm25/dense/hybrid)\n",
    "    - (опционально) cross-encoder\n",
    "    - (опционально) MMR\n",
    "    - сбор контекста\n",
    "    - вызов LLM\n",
    "    - парсинг ответа и цитат\n",
    "\n",
    "    Возвращает dict с полями:\n",
    "    {\n",
    "      \"qid\": ...,\n",
    "      \"question\": ...,\n",
    "      \"answer\": ...,\n",
    "      \"citations\": [...],\n",
    "      \"raw_llm_output\": ...,\n",
    "      \"retrieval_candidates\": candidates_after_all_steps,\n",
    "    }\n",
    "    \"\"\"\n",
    "    if use_hybrid and bm25_index and dense_index:\n",
    "        candidates = hybrid_candidates(\n",
    "            question,\n",
    "            bm25_index=bm25_index,\n",
    "            dense_index=dense_index,\n",
    "            top_k=TOP_K_RETRIEVAL,\n",
    "            alpha=ALPHA_HYBRID,\n",
    "            emb_model=emb_model,\n",
    "            emb_cache=emb_cache,\n",
    "            verbose=VERBOSE_RAG,\n",
    "        )\n",
    "    elif bm25_index and USE_BM25:\n",
    "        candidates = bm25_candidates(question, bm25_index=bm25_index, top_k=TOP_K_RETRIEVAL, verbose=VERBOSE_RAG)\n",
    "    elif dense_index and USE_DENSE:\n",
    "        candidates = dense_candidates(\n",
    "            question,\n",
    "            dense_index=dense_index,\n",
    "            emb_model=emb_model,\n",
    "            emb_cache=emb_cache,\n",
    "            top_k=TOP_K_RETRIEVAL,\n",
    "            verbose=VERBOSE_RAG,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Нет доступных индексов для ретрива. Включите USE_BM25 или USE_DENSE.\")\n",
    "\n",
    "    print_stage_transition(\"retrieval -> rerank\")\n",
    "\n",
    "    if use_ce and USE_CROSS_ENCODER:\n",
    "        # TODO: передайте сюда свою модель cross-encoder и токенайзер, если они есть\n",
    "        candidates = rerank_cross_encoder(\n",
    "            question,\n",
    "            candidates,\n",
    "            model=None,  # TODO: замените на реальную модель\n",
    "            tokenizer=None,  # TODO: замените на реальный токенайзер\n",
    "            top_k=TOP_K_CE,\n",
    "            verbose=VERBOSE_RAG,\n",
    "        )\n",
    "    else:\n",
    "        candidates = candidates[:TOP_K_CE]\n",
    "\n",
    "    # TODO: если ваш retrieval поддерживает MMR, включите его здесь\n",
    "    if use_mmr and hasattr(candidates, 'apply_mmr'):\n",
    "        candidates = candidates.apply_mmr(lambda_=0.5)  # параметр lambda_ подберите под задачу\n",
    "\n",
    "    print_candidates_summary(candidates, corpus=corpus, limit=5)\n",
    "\n",
    "    context_chunks = build_context_window(\n",
    "        question=question,\n",
    "        candidates=candidates,\n",
    "        corpus=corpus,\n",
    "        max_tokens=MAX_CONTEXT_TOKENS,\n",
    "    )\n",
    "\n",
    "    prompt = build_prompt(question=question, context_chunks=context_chunks, extra_instructions=\"\")\n",
    "    raw_output = call_llm(prompt)\n",
    "    answer, citations = parse_llm_output_to_answer_and_citations(raw_output)\n",
    "\n",
    "    return {\n",
    "        \"qid\": qid,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"citations\": citations,\n",
    "        \"raw_llm_output\": raw_output,\n",
    "        \"retrieval_candidates\": candidates,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0daaeb4",
   "metadata": {},
   "source": [
    "## 7. Нормализация и парсинг ответа\n",
    "\n",
    "Для метрик вроде EM/F1 важно нормализовать ответы: привести к нижнему регистру, убрать лишние пробелы и пунктуацию (если это соответствует метрике задачи)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c36fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "def normalize_answer(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Базовая нормализация: нижний регистр, убираем пунктуацию и лишние пробелы.\n",
    "    TODO: адаптируйте под метрику конкретной задачи (например, не убирать пунктуацию, если она важна).\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2bf869",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_llm_output_to_answer_and_citations(raw_output: str) -> tuple[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Парсит строку raw_output в (answer, citations).\n",
    "\n",
    "    Ожидается JSON:\n",
    "    {\n",
    "      \"answer\": \"...\",\n",
    "      \"citations\": [12, 47]\n",
    "    }\n",
    "\n",
    "    TODO: добавьте try/except, fallback и более строгую валидацию под задачу.\n",
    "    \"\"\"\n",
    "    answer = \"\"\n",
    "    citations: List[int] = []\n",
    "    try:\n",
    "        parsed = json.loads(raw_output)\n",
    "        answer = parsed.get(\"answer\", \"\") if isinstance(parsed, dict) else \"\"\n",
    "        citations = parsed.get(\"citations\", []) if isinstance(parsed, dict) else []\n",
    "        # Возможен случай, когда citations пришёл как строка или список строк\n",
    "        if isinstance(citations, str):\n",
    "            try:\n",
    "                citations = json.loads(citations)\n",
    "            except json.JSONDecodeError:\n",
    "                citations = []\n",
    "        citations = [int(c) for c in citations if str(c).isdigit()]\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] Не удалось распарсить JSON ответа: {e}. Возвращаем исходную строку.\")\n",
    "        answer = raw_output\n",
    "        citations = []\n",
    "    return answer, citations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc50b5d",
   "metadata": {},
   "source": [
    "## 8. Метрики и валидация\n",
    "\n",
    "Базовые метрики для QA:\n",
    "- **Exact Match (EM)** по нормализованным строкам.\n",
    "- **F1** по токенам (precision/recall на уровне слов).\n",
    "\n",
    "Если у одного вопроса несколько верных ответов, адаптируйте функции ниже (например, берите максимум по кандидату). Если в соревновании есть официальная метрика, лучше скопировать её реализацию сюда вместо базовых EM/F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bf27f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_em(pred: str, gold: str) -> float:\n",
    "    return float(normalize_answer(pred) == normalize_answer(gold))\n",
    "\n",
    "\n",
    "def compute_f1(pred: str, gold: str) -> float:\n",
    "    pred_tokens = normalize_answer(pred).split()\n",
    "    gold_tokens = normalize_answer(gold).split()\n",
    "    common = set(pred_tokens) & set(gold_tokens)\n",
    "    if not pred_tokens or not gold_tokens:\n",
    "        return float(pred_tokens == gold_tokens)\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(gold_tokens)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48697349",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: если нет отдельной валидации, можно сделать сплит из train\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# if not HAS_VAL and not train_df.empty:\n",
    "#     train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=RANDOM_SEED)\n",
    "#     print(f\"Создана валидация размером {val_df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee41d359",
   "metadata": {},
   "source": [
    "## 9. Быстрый прогон на валидации\n",
    "\n",
    "Начните с малого: прогоните пару вопросов, чтобы увидеть, адекватен ли контекст и в правильном ли формате модель возвращает JSON и цитаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7f13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_SMALL = 10\n",
    "small_val_df = val_df.head(N_SMALL) if not val_df.empty else pd.DataFrame()\n",
    "small_results = []\n",
    "\n",
    "if small_val_df.empty:\n",
    "    print(\"[INFO] val_df пустой или отсутствует — пропускаем мини-валидацию.\")\n",
    "else:\n",
    "    for _, row in tqdm(small_val_df.iterrows(), total=len(small_val_df)):\n",
    "        res = answer_one_question(\n",
    "            qid=row.get('qid'),\n",
    "            question=row.get('question'),\n",
    "            corpus=corpus,\n",
    "            bm25_index=bm25_index,\n",
    "            dense_index=dense_index,\n",
    "            emb_model=None,  # TODO: передайте emb_model, если используете dense/hybrid\n",
    "            emb_cache=emb_cache,\n",
    "            use_hybrid=USE_HYBRID,\n",
    "            use_ce=USE_CROSS_ENCODER,\n",
    "            use_mmr=USE_MMR,\n",
    "        )\n",
    "        res['gold_answer'] = row.get('answer', '')\n",
    "        small_results.append(res)\n",
    "\n",
    "    for r in small_results[:3]:\n",
    "        print(\"\\n=== Пример ===\")\n",
    "        print(f\"qid: {r['qid']}\")\n",
    "        print(f\"Вопрос: {r['question']}\")\n",
    "        print(f\"Ответ модели: {r['answer']}\")\n",
    "        print(f\"Цитаты: {r['citations']}\")\n",
    "        if 'gold_answer' in r:\n",
    "            print(f\"Gold: {r['gold_answer']}\")\n",
    "        inspect_and_print(r['retrieval_candidates'], corpus=corpus, limit=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b46373",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_predictions = []\n",
    "if not val_df.empty and HAS_GOLD_ANSWERS:\n",
    "    for _, row in tqdm(val_df.iterrows(), total=len(val_df)):\n",
    "        res = answer_one_question(\n",
    "            qid=row.get('qid'),\n",
    "            question=row.get('question'),\n",
    "            corpus=corpus,\n",
    "            bm25_index=bm25_index,\n",
    "            dense_index=dense_index,\n",
    "            emb_model=None,  # TODO: передайте emb_model\n",
    "            emb_cache=emb_cache,\n",
    "            use_hybrid=USE_HYBRID,\n",
    "            use_ce=USE_CROSS_ENCODER,\n",
    "            use_mmr=USE_MMR,\n",
    "        )\n",
    "        pred_answer = res['answer']\n",
    "        gold_answer = row.get('answer', '')\n",
    "        res['gold_answer'] = gold_answer\n",
    "        res['em'] = compute_em(pred_answer, gold_answer)\n",
    "        res['f1'] = compute_f1(pred_answer, gold_answer)\n",
    "        val_predictions.append(res)\n",
    "\n",
    "    val_pred_df = pd.DataFrame(val_predictions)\n",
    "    print(\"Средний EM:\", val_pred_df['em'].mean())\n",
    "    print(\"Средний F1:\", val_pred_df['f1'].mean())\n",
    "    val_pred_path = os.path.join(OUTPUT_DIR, \"val_predictions.csv\")\n",
    "    val_pred_df.to_csv(val_pred_path, index=False)\n",
    "    print(f\"Сохранено: {val_pred_path}\")\n",
    "else:\n",
    "    print(\"[INFO] Полная валидация пропущена (нет val или gold ответов).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d473003",
   "metadata": {},
   "source": [
    "## 10. Быстрая подстройка гиперпараметров ретрива (опционально)\n",
    "\n",
    "Что крутить в первую очередь:\n",
    "- `TOP_K_RETRIEVAL` (recall vs скорость)\n",
    "- `ALPHA_HYBRID` (баланс BM25/dense)\n",
    "- `TOP_K_CE` (насколько глубоко реранкать)\n",
    "- `MAX_CONTEXT_TOKENS` (обрезка контекста)\n",
    "- включение/выключение MMR и cross-encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9588919",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: небольшой перебор конфигов на подмножестве валидации\n",
    "val_df_sample = val_df.head(50) if not val_df.empty else pd.DataFrame()\n",
    "configs = [\n",
    "    {\"TOP_K_RETRIEVAL\": 30, \"ALPHA_HYBRID\": 0.3},\n",
    "    {\"TOP_K_RETRIEVAL\": 50, \"ALPHA_HYBRID\": 0.5},\n",
    "]\n",
    "\n",
    "results_grid = []\n",
    "if val_df_sample.empty or not HAS_GOLD_ANSWERS:\n",
    "    print(\"[INFO] Пропускаем гипер-перебор (нет валидации или нет gold ответов).\")\n",
    "else:\n",
    "    for cfg in configs:\n",
    "        em_scores, f1_scores = [], []\n",
    "        for _, row in val_df_sample.iterrows():\n",
    "            # Временно подменяем глобальные параметры\n",
    "            TOP_K_RETRIEVAL = cfg[\"TOP_K_RETRIEVAL\"]\n",
    "            ALPHA_HYBRID = cfg[\"ALPHA_HYBRID\"]\n",
    "            res = answer_one_question(\n",
    "                qid=row.get('qid'),\n",
    "                question=row.get('question'),\n",
    "                corpus=corpus,\n",
    "                bm25_index=bm25_index,\n",
    "                dense_index=dense_index,\n",
    "                emb_model=None,  # TODO: передайте emb_model\n",
    "                emb_cache=emb_cache,\n",
    "                use_hybrid=USE_HYBRID,\n",
    "                use_ce=USE_CROSS_ENCODER,\n",
    "                use_mmr=USE_MMR,\n",
    "            )\n",
    "            em_scores.append(compute_em(res['answer'], row.get('answer', '')))\n",
    "            f1_scores.append(compute_f1(res['answer'], row.get('answer', '')))\n",
    "        results_grid.append({\n",
    "            'config': cfg,\n",
    "            'em': np.mean(em_scores),\n",
    "            'f1': np.mean(f1_scores),\n",
    "        })\n",
    "    results_df = pd.DataFrame(results_grid)\n",
    "    display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5209bb2f",
   "metadata": {},
   "source": [
    "## 11. Инференс на тесте и сабмит\n",
    "\n",
    "У `test_df` обычно есть только `qid` и `question`. Формат сабмита берите из условия: этот шаблон создаёт колонку `answer` и опционально `citations_json` (строка JSON со списком ссылок). Отредактируйте под требования соревнования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc2a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission_rows = []\n",
    "if not test_df.empty:\n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "        res = answer_one_question(\n",
    "            qid=row.get('qid'),\n",
    "            question=row.get('question'),\n",
    "            corpus=corpus,\n",
    "            bm25_index=bm25_index,\n",
    "            dense_index=dense_index,\n",
    "            emb_model=None,  # TODO: передайте emb_model\n",
    "            emb_cache=emb_cache,\n",
    "            use_hybrid=USE_HYBRID,\n",
    "            use_ce=USE_CROSS_ENCODER,\n",
    "            use_mmr=USE_MMR,\n",
    "        )\n",
    "        submission_rows.append({\n",
    "            'qid': res['qid'],\n",
    "            'answer': res['answer'],  # TODO: подстройте под нужный формат сабмита\n",
    "            'citations_json': json.dumps(res['citations']),\n",
    "        })\n",
    "else:\n",
    "    print(\"[INFO] test_df пустой — проверьте путь к тестовым данным.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e185cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if submission_rows:\n",
    "    submission_df = pd.DataFrame(submission_rows)\n",
    "    submission_path = os.path.join(OUTPUT_DIR, \"submission_qa_with_citations.csv\")\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"Сабмит сохранён: {submission_path}\")\n",
    "    display(submission_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a77ad",
   "metadata": {},
   "source": [
    "## 12. Что сохранить на будущее (опционально)\n",
    "- Финальный сабмит.\n",
    "- `val_predictions.csv` для анализа ошибок.\n",
    "- Конфиг гиперпараметров (можно сохранить в JSON рядом с сабмитом).\n",
    "- Логи/заметки о том, какие настройки работали лучше всего."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
