## 0. Общая интуиция

* `chunk_size_chars` — сколько контента ты увидишь **за один заход**.
* `chunk_overlap_chars` — сколько контента будет **дублироваться** между соседними кусками.
* `min_chunk_chars` — насколько короткий хвост ты готов считать отдельным чанком, а не мусором.

Важно смотреть не на голые цифры, а на:

* **токен-бюджет** модели (8k / 16k / 32k),
* **сколько чанков** ты хочешь класть в контекст (`k_final`),
* **тип текста** (формальный/структурированный vs треп).

Очень грубо:
`300–400 символов ≈ 80–150 токенов` (зависит от языка/формата).

---

## 1. `chunk_size_chars` — максимальная длина чанка

### Когда брать МАЛЕНЬКИЕ чанки (300–600 символов)

Подходит, если:

* тебе нужен **очень точный спан**:

  * извлечение конкретного факта, даты, числа;
  * задачи типа «найди конкретное утверждение и процитируй»;
* у тебя маленький `max_tokens` для контекста, а `k_final` относительно большой:

  * напр. `max_tokens=1024`, `k_final=8` → каждый чанк должен быть компактным.

Минусы:

* модель может «не видеть» более широкий контекст (до/после абзаца),
* нужно больше чанков, чтобы покрыть документ.

Пример:

```python
chunk_size_chars = 400  # компактные куски
```

---

### Когда брать СРЕДНИЕ чанки (700–1200 символов)

Это хороший **дефолт** почти для всего RAG:

* QA по статьям, регламентам, условиям задач,
* пояснения по текстам,
* когда тебе важен **баланс**:

  * и локальный контекст (1–2 абзаца),
  * и чтобы влезло несколько чанков в окно модели.

Если у тебя, условно, `k_final=6–8` и контекст 2–4k токенов,
то `chunk_size_chars=800–1000` — очень комфортная зона.

Пример дефолта для твоего тулкита:

```python
chunk_size_chars = 800
```

---

### Когда брать БОЛЬШИЕ чанки (1200–2000+ символов)

Использовать осторожно. Полезно если:

* текст очень связный, и **смысл ломается при разрезании**:

  * длинные рассуждения, лекции, эссе;
* задача ближе к **суммаризации**, чем к точному QA;
* у тебя большой контекст (16k+), и ты готов класть 2–4 жирных куска.

Минусы:

* меньше разнообразия в окне (мало чанков, но каждый длинный),
* выше риск, что «нужное» потеряется внутри супер-длинного текста.

---

## 2. `chunk_overlap_chars` — перекрытие

Очень просто: **10–30% от `chunk_size_chars`** — нормальный диапазон.

### Когда делать МАЛЕНЬКИЙ overlap (0–10%)

* когда текста много, а вычислительные ресурсы ограничены;
* когда документы сами по себе хорошо размечены (заголовки, абзацы, пункты, и ты режешь по ним);
* задача **более грубая** — типа просто найти релевантный раздел, а не точную фразу.

Пример:

```python
chunk_size_chars = 800
chunk_overlap_chars = 50  # минимальный дублирующий хвост
```

---

### Когда делать СРЕДНИЙ overlap (10–25%)

Это снова дефолт для большинства случаев:

* фразы часто переезжают между абзацами, и логика разбросана;
* ты хочешь, чтобы модель видела “стыки” между чанками.

Пример дефолта:

```python
chunk_size_chars = 800
chunk_overlap_chars = 150  # ~20%
```

---

### Когда делать БОЛЬШОЙ overlap (25–40%)

* текст **жёстко связан по предложениям**, и многие важные вещи находятся в переходах;
* корпус небольшой, время/ресурсы позволяют сильно дублироваться;
* хочется максимально **не потерять связь** между соседними кусками.

Минусы:

* растёт количество чанков,
* растёт время индексации и ретрива.

---

## 3. `min_chunk_chars` — минимум, чтобы вообще брать чанк

Это фильтр против «мусорных хвостов»:

* если последний кусок документа совсем крошечный (типа «См. Приложение 2»),
  можно его не добавлять, чтобы не засорять retriever.

### Типичные стратегии

* **0 или совсем мало (1–50 символов)**
  → берём всё подряд, ничего не выкидываем.
  Нормально, если документы сами по себе довольно короткие.

* **≈ 30–50% от `chunk_size_chars`**
  → «хочу, чтобы каждый чанк был более-менее содержательным».

Например, при `chunk_size_chars=800`:

```python
min_chunk_chars = 300  # хвост короче 300 символов — выбрасываем
```

Это хороший вариант, если:

* у тебя много длинных документов,
* ты не хочешь пачку чанк-вставок по 1–2 предложения.

---

## 4. Готовые пресеты (чтобы не думать каждый раз)

### Пресет А: «Олимпиадный RAG по регламентам/текстам»

Баланс скорость / качество:

```python
chunk_size_chars   = 800
chunk_overlap_chars = 150
min_chunk_chars     = 300
```

Когда использовать:

* разбор условий задач, методичек, описаний соревнований;
* QA, где надо понимать 1–2 абзаца + немного контекста до/после.

---

### Пресет B: «Очень точный поиск кусочков»

```python
chunk_size_chars   = 400
chunk_overlap_chars = 80
min_chunk_chars     = 150
```

Когда:

* нужно искать конкретные маленькие спаны (формулы, строки, определения);
* важнее точность попадания в кусок, а не широкий контекст.

---

### Пресет C: «Длинные тексты, ближе к суммаризации»

```python
chunk_size_chars   = 1400
chunk_overlap_chars = 250
min_chunk_chars     = 500
```

Когда:

* длинные статьи/лекции,
* большой контекст LLM,
* хочешь, чтобы каждый чанк был реально «кусок смысла», а не один абзац.

---

Если хочешь, можем прямо под твой конкретный `max_tokens` и `k_final` для финала Всероса прикинуть конкретные цифры:
типа: «хочу класть 6–8 чанков, на контекст у меня ~1500–2000 токенов» — и тогда посчитаем целевые `chunk_size_chars` более строго.
